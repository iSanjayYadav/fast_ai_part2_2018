<!DOCTYPE html>
<html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Deep Learning 2: Part 2 Lesson 9 – Hiromi Suenaga – Medium</title><link rel="canonical" href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b"><meta name="title" content="Deep Learning 2: Part 2 Lesson 9 – Hiromi Suenaga – Medium"><meta name="referrer" content="always"><meta name="description" content="My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to Jeremy and Rachel who…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Deep Learning 2: Part 2 Lesson 9 – Hiromi Suenaga – Medium"><meta property="twitter:title" content="Deep Learning 2: Part 2 Lesson 9 – Hiromi Suenaga – Medium"><meta property="og:url" content="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*Fj9fK3G6iXBsGI_XJrxXyg.png"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really”…"><meta name="twitter:description" content="My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really”…"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*Fj9fK3G6iXBsGI_XJrxXyg.png"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://medium.com/@hiromi_suenaga"><meta property="author" content="Hiromi Suenaga"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="Hiromi Suenaga"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-04-01T02:48:16.012Z"><meta name="twitter:creator" content="@hiromi_suenaga"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="42 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/5f0cf9e4bb5b"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/5f0cf9e4bb5b"><meta property="al:android:url" content="medium://p/5f0cf9e4bb5b"><meta property="al:web:url" content="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/5f0cf9e4bb5b"><script async="" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/branch-latest.js"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":456,"height":456,"url":"https://cdn-images-1.medium.com/max/456/1*Fj9fK3G6iXBsGI_XJrxXyg.png"},"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","dateCreated":"2018-04-01T02:48:16.012Z","datePublished":"2018-04-01T02:48:16.012Z","dateModified":"2018-08-06T20:44:57.819Z","headline":"Deep Learning 2: Part 2 Lesson 9","name":"Deep Learning 2: Part 2 Lesson 9","thumbnailUrl":"https://cdn-images-1.medium.com/max/456/1*Fj9fK3G6iXBsGI_XJrxXyg.png","keywords":["Tag:Machine Learning","Tag:Deep Learning","Tag:AI","Tag:Neural Networks","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Hiromi Suenaga","url":"https://medium.com/@hiromi_suenaga"},"creator":["Hiromi Suenaga"],"publisher":{"@type":"Organization","name":"Medium","url":"https://medium.com/","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/308/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b"}</script><meta name="parsely-link" content="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/m2.css"><link rel="stylesheet" href="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/main-branding-base.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/analytics.js"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-firefox os-windows is-withMagicUnderlinesv-glyph v-glyph--m2 is-js is-withMagicUnderlines" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1540006658318" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="1349" height="613"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"><div class="paragraphControls js-paragraphControl js-paragraphControl-dfa4 u-noUserSelect is-visible" style="top: 6881px;"><span class="paragraphControls-itemText"><button class="button button--chromeless" data-action="select-anchor" data-action-value="dfa4">Top highlight</button></span></div><div class="paragraphControls js-paragraphControl js-paragraphControl-d4fd u-noUserSelect is-visible" style="top: 37291px;"><span class="paragraphControls-itemText"><button class="button button--chromeless" data-action="select-anchor" data-action-value="d4fd">You highlighted</button></span></div><div class="paragraphControls js-paragraphControl js-paragraphControl-2a9b u-noUserSelect is-visible" style="top: 42177px;"><span class="paragraphControls-itemText"><button class="button button--chromeless" data-action="select-anchor" data-action-value="2a9b">You highlighted</button></span></div></div></div><div class="metabar u-clearfix js-metabar u-boxShadow4px12pxBlackLightest is-hiddenWhenMinimized"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="u-xs-hide js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-flex0 u-flexCenter u-paddingTop0 u-fillTransparentBlackDarker"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0 u-fillTransparentBlackDarker"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-xs-show js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-flex0 u-flexCenter u-paddingTop0 u-fillTransparentBlackDarker"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0 u-fillTransparentBlackDarker"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div></div><div class="metabar-block u-flex0"><div class="buttonSet buttonSet--wide"><label class="button button--chromeless inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search Medium"><span class="svgIcon svgIcon--search svgIcon--25px u-top0 u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" placeholder="Search Medium" required="true" type="search"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-xs-top2" href="https://medium.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight10 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><a class="button button--small button--withChrome u-baseColor--buttonNormal u-xs-hide js-upgradeMembershipAction" href="https://medium.com/membership?source=upgrade_membership---nav_full" data-disable-client-nav="true" data-scroll="native">Upgrade</a><button class="button button--chromeless u-baseColor--buttonNormal is-inSiteNavBar js-userActions" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1IzvwYFGjjBE8icwhb7LdGQ.jpg" class="avatar-image avatar-image--icon" alt="Sanjay Yadav"></div></button></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full" lang="en"><header class="container u-maxWidth740"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaHeader u-paddingBottom10 row"><div class="ui-caption postMetaHeader-socialProof2 col u-size12of12 u-paddingBottom20"><div class="postMetaInline">Applause from you <span class="u-noWrap">and <button class="button button--chromeless u-baseColor--buttonNormal" data-action="show-recommends" data-action-value="5f0cf9e4bb5b">54 others</button></span></div></div><div class="col u-size12of12 js-postMetaLockup"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaLockup postMetaLockup--authorWithBio u-flexCenter js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_002.jpg" class="avatar-image avatar-image--smaller" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-lineHeightTightest"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://medium.com/@hiromi_suenaga?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><span class="followState js-followState" data-user-id="eab3a535185e"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="eab3a535185e" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="eab3a535185e" data-action-source="post_header_lockup-eab3a535185e-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="5f0cf9e4bb5b"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption postMetaInline js-testPostMetaInlineSupplemental"><time datetime="2018-04-01T02:48:16.012Z">Apr 1</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="42 min read"></span></div></div></div></div></div></header><div class="postArticle-content js-postField js-notesSource js-trackedPost" data-post-id="5f0cf9e4bb5b" data-source="post_page" data-tracking-context="postPage" data-scroll="native"><section name="5f04" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="2be5" id="2be5" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;9</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">.
 These notes will continue to be updated and improved as I continue to 
review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="929c" id="929c" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">9</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></div></div></section><section name="d0af" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b0c6" id="b0c6" class="graf graf--h3 graf--leading">Links</h3><p name="32c6" id="32c6" class="graf graf--p graf-after--h3"><a href="http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1" data-href="http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Forum</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://youtu.be/0frKXR-2PBY" data-href="https://youtu.be/0frKXR-2PBY" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Video</strong></a></p><h3 name="8cda" id="8cda" class="graf graf--h3 graf-after--p">Review</h3><h4 name="3167" id="3167" class="graf graf--h4 graf-after--h3">From Last&nbsp;week:</h4><ul class="postList"><li name="13b8" id="13b8" class="graf graf--li graf-after--h4">Pathlib; JSON</li><li name="8f79" id="8f79" class="graf graf--li graf-after--li">Dictionary comprehensions</li><li name="c475" id="c475" class="graf graf--li graf-after--li">Defaultdict</li><li name="5c7b" id="5c7b" class="graf graf--li graf-after--li">How to jump around fastai source</li><li name="2b0e" id="2b0e" class="graf graf--li graf-after--li">matplotlib OO API</li><li name="a56b" id="a56b" class="graf graf--li graf-after--li">Lambda functions</li><li name="35f7" id="35f7" class="graf graf--li graf-after--li">Bounding box coordinates</li><li name="474a" id="474a" class="graf graf--li graf-after--li">Custom head; bounding box regression</li></ul><figure name="da49" id="da49" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 388px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.400000000000006%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*2nxK3zuKRnDCu_3qVhSMnw.png" data-width="1515" data-height="839" data-action="zoom" data-action-value="1*2nxK3zuKRnDCu_3qVhSMnw.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12nxK3zuKRnDCu_3qVhSMnw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="40"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*2nxK3zuKRnDCu_3qVhSMnw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12nxK3zuKRnDCu_3qVhSMnw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*2nxK3zuKRnDCu_3qVhSMnw.png"></noscript></div></div></figure><figure name="67d3" id="67d3" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 368px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*9G88jQ42l5RdwFi2Yr_h_Q.png" data-width="1488" data-height="783" data-action="zoom" data-action-value="1*9G88jQ42l5RdwFi2Yr_h_Q.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/19G88jQ42l5RdwFi2Yr_h_Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="37"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*9G88jQ42l5RdwFi2Yr_h_Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/19G88jQ42l5RdwFi2Yr_h_Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*9G88jQ42l5RdwFi2Yr_h_Q.png"></noscript></div></div></figure><h4 name="6247" id="6247" class="graf graf--h4 graf-after--figure">From Part&nbsp;1:</h4><ul class="postList"><li name="5e8a" id="5e8a" class="graf graf--li graf-after--h4">How to view model inputs from a DataLoader</li><li name="648e" id="648e" class="graf graf--li graf-after--li">How to view model outputs</li></ul><figure name="1296" id="1296" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 259px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*E3Z5vKnp6ZkfuLR83979RA.png" data-width="1508" data-height="559" data-action="zoom" data-action-value="1*E3Z5vKnp6ZkfuLR83979RA.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1E3Z5vKnp6ZkfuLR83979RA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="27"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*E3Z5vKnp6ZkfuLR83979RA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1E3Z5vKnp6ZkfuLR83979RA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*E3Z5vKnp6ZkfuLR83979RA.png"></noscript></div></div></figure><h3 name="5492" id="5492" class="graf graf--h3 graf-after--figure">Data Augmentation and Bounding Box&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=2m58s" data-href="https://youtu.be/0frKXR-2PBY?t=2m58s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">2:58</a>]</h3><p name="e3ff" id="e3ff" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="be1e" id="be1e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Awkward rough edges of fastai:</strong><br>A <em class="markup--em markup--p-em">classifier</em> is anything with dependent variable is categorical or binomial. As opposed to <em class="markup--em markup--p-em">regression</em> which is anything with dependent variable is continuous. Naming is a little confusing but will be sorted out in future. Here, <code class="markup--code markup--p-code">continuous</code> is <code class="markup--code markup--p-code">True</code> because our dependent variable is the coordinates of bounding box — hence this is actually a regressor data.</p><pre name="67ba" id="67ba" class="graf graf--pre graf-after--p">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                       aug_tfms=augs)<br>md = Image<strong class="markup--strong markup--pre-strong">Classifier</strong>Data.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,<br>                                  <strong class="markup--strong markup--pre-strong">continuous=True</strong>, bs=4)</pre><h4 name="f768" id="f768" class="graf graf--h4 graf-after--pre">Let’s create some data augmentation [<a href="https://youtu.be/0frKXR-2PBY?t=4m40s" data-href="https://youtu.be/0frKXR-2PBY?t=4m40s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">4:40</a>]</h4><pre name="92de" id="92de" class="graf graf--pre graf-after--h4">augs = [RandomFlip(), <br>        RandomRotate(30),<br>        RandomLighting(0.1,0.1)]</pre><p name="8b2e" id="8b2e" class="graf graf--p graf-after--pre">Normally,
 we use these shortcuts Jeremy created for us, but they are simply lists
 of random augmentations. But you can easily create your own (most if 
not all of them start with “Random”).</p><figure name="2e67" id="2e67" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 524px; max-height: 52px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9.9%;"></div><img class="graf-image" data-image-id="1*lAIQHKT0GbjY0fRZKmpFaA.png" data-width="524" data-height="52" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1lAIQHKT0GbjY0fRZKmpFaA.png"></div></figure><pre name="544c" id="544c" class="graf graf--pre graf-after--figure">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,<br>                       aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,<br>                       continuous=True, bs=4)</pre><pre name="aac4" id="aac4" class="graf graf--pre graf-after--pre">idx=3<br>fig,axes = plt.subplots(3,3, figsize=(9,9))<br>for i,ax in enumerate(axes.flat):<br>    x,y=next(iter(md.aug_dl))<br>    ima=md.val_ds.denorm(to_np(x))[idx]<br>    b = bb_hw(to_np(y[idx]))<br>    print(b)<br>    show_img(ima, ax=ax)<br>    draw_rect(ax, b)</pre><pre name="97e9" id="97e9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]</em></pre><figure name="cd34" id="cd34" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 679px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.1%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*QMa_SUUVOypZHKaAuXDkSw.png" data-width="1020" data-height="990" data-action="zoom" data-action-value="1*QMa_SUUVOypZHKaAuXDkSw.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1QMa_SUUVOypZHKaAuXDkSw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="72"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*QMa_SUUVOypZHKaAuXDkSw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1QMa_SUUVOypZHKaAuXDkSw_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*QMa_SUUVOypZHKaAuXDkSw.png"></noscript></div></div></figure><p name="f3bd" id="f3bd" class="graf graf--p graf-after--figure">As you can see, the image gets rotated and lighting varies, but bounding box is <em class="markup--em markup--p-em">not moving</em> and is <em class="markup--em markup--p-em">in a wrong spot</em> [<a href="https://youtu.be/0frKXR-2PBY?t=6m17s" data-href="https://youtu.be/0frKXR-2PBY?t=6m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">6:17</a>].
 This is the problem with data augmentations when your dependent 
variable is pixel values or in some way connected to the independent 
variable — they need to be augmented together. As you can see in the 
bounding box coordinates <code class="markup--code markup--p-code">[ 115. 63. 240. 311.]</code>&nbsp;,
 our image is 224 by 224 — so it is neither scaled nor cropped. The 
dependent variable needs to go through all the geometric transformation 
as the independent variables.</p><p name="586a" id="586a" class="graf graf--p graf-after--p">To do this [<a href="https://youtu.be/0frKXR-2PBY?t=7m10s" data-href="https://youtu.be/0frKXR-2PBY?t=7m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">7:10</a>], every transformation has an optional <code class="markup--code markup--p-code">tfm_y</code> parameter:</p><pre name="8852" id="8852" class="graf graf--pre graf-after--p">augs = [RandomFlip(tfm_y=TfmType.COORD),<br>        RandomRotate(30, tfm_y=TfmType.COORD),<br>        RandomLighting(0.1,0.1, tfm_y=TfmType.COORD)]</pre><pre name="161c" id="161c" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,<br>                       tfm_y=TfmType.COORD, aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <br>                       continuous=True, bs=4)</pre><p name="e0d7" id="e0d7" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">TrmType.COORD</code> indicates that the <em class="markup--em markup--p-em">y</em> value represents coordinate. This needs to be added to all the augmentations as well as <code class="markup--code markup--p-code">tfms_from_model</code> which is responsible for cropping, zooming, resizing, padding, etc.</p><pre name="4f8e" id="4f8e" class="graf graf--pre graf-after--p">idx=3<br>fig,axes = plt.subplots(3,3, figsize=(9,9))<br>for i,ax in enumerate(axes.flat):<br>    x,y=next(iter(md.aug_dl))<br>    ima=md.val_ds.denorm(to_np(x))[idx]<br>    b = bb_hw(to_np(y[idx]))<br>    print(b)<br>    show_img(ima, ax=ax)<br>    draw_rect(ax, b)</pre><pre name="e2fc" id="e2fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[  48.   34.  112.  188.]<br>[  65.   36.  107.  185.]<br>[  49.   27.  131.  195.]<br>[  24.   18.  147.  204.]<br>[  61.   34.  113.  188.]<br>[  55.   31.  121.  191.]<br>[  52.   19.  144.  203.]<br>[   7.    0.  193.  222.]<br>[  52.   38.  105.  182.]</em></pre><figure name="f82b" id="f82b" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 507px; max-height: 495px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_ge-RyZpEIQ5fiSvo207rA.png" data-width="507" data-height="495" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1_ge-RyZpEIQ5fiSvo207rA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="72"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*_ge-RyZpEIQ5fiSvo207rA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1_ge-RyZpEIQ5fiSvo207rA_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*_ge-RyZpEIQ5fiSvo207rA.png"></noscript></div></div></figure><p name="dfa4" id="dfa4" class="graf graf--p graf-after--figure">Now,
 the bounding box moves with the image and is in the right spot. You may
 notice that sometimes it looks odd like the middle on in the bottom 
row. This is the constraint of the information we have. If the object 
occupied the corners of the original bounding box, your new bounding box
 needs to be bigger after the image rotates. So you must <strong class="markup--strong markup--p-strong">be careful of not doing too higher rotations with bounding boxes</strong> because there is not enough information for them to stay accurate. <span class="markup--quote markup--p-quote is-other" name="anon_a4e8906f8ed3" data-creator-ids="anon">If we were doing polygons or segmentations, we would not have this problem.</span></p><figure name="8d9f" id="8d9f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 400px; max-height: 220px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.00000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*4V4sjFZxn-y2cU9tCJPEUw.png" data-width="400" data-height="220" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/14V4sjFZxn-y2cU9tCJPEUw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="40"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*4V4sjFZxn-y2cU9tCJPEUw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/14V4sjFZxn-y2cU9tCJPEUw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*4V4sjFZxn-y2cU9tCJPEUw.png"></noscript></div></div><figcaption class="imageCaption">This why the box gets&nbsp;bigger</figcaption></figure><pre name="123b" id="123b" class="graf graf--pre graf-after--figure">tfm_y = TfmType.COORD<br>augs = [RandomFlip(tfm_y=tfm_y),<br>        RandomRotate(<strong class="markup--strong markup--pre-strong">3</strong>, <strong class="markup--strong markup--pre-strong">p=0.5</strong>, tfm_y=tfm_y),<br>        RandomLighting(0.05,0.05, tfm_y=tfm_y)]</pre><pre name="b051" id="b051" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                 tfm_y=tfm_y, aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <br>                 continuous=True)</pre><p name="76dc" id="76dc" class="graf graf--p graf-after--pre">So here, we do maximum of 3 degree rotation to avoid this problem [<a href="https://youtu.be/0frKXR-2PBY?t=9m14s" data-href="https://youtu.be/0frKXR-2PBY?t=9m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">9:14</a>]. It also only rotates half of the time (<code class="markup--code markup--p-code">p=0.5</code>).</p><h4 name="58b8" id="58b8" class="graf graf--h4 graf-after--p">custom_head [<a href="https://youtu.be/0frKXR-2PBY?t=9m34s" data-href="https://youtu.be/0frKXR-2PBY?t=9m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:34</a>]</h4><p name="e869" id="e869" class="graf graf--p graf-after--h4"><code class="markup--code markup--p-code">learn.summary()</code>
 will run a small batch of data through a model and prints out the size 
of tensors at every layer. As you can see, right before the <code class="markup--code markup--p-code">Flatten</code>
 layer, the tensor has the shape of 512 by 7 by 7. So if it were a rank 1
 tensor (i.e. a single vector) its length will be 25088 (512 * 7 * 7)and
 that is why our custom header’s input size is 25088. Output size is 4 
since it is the bounding box coordinates.</p><pre name="1987" id="1987" class="graf graf--pre graf-after--p">head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))<br>learn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)<br>learn.opt_fn = optim.Adam<br>learn.crit = nn.L1Loss()</pre><figure name="6c46" id="6c46" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 369px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.800000000000004%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*o9NFGVz1ua60kOpIafe5Hg.png" data-width="1440" data-height="760" data-action="zoom" data-action-value="1*o9NFGVz1ua60kOpIafe5Hg.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1o9NFGVz1ua60kOpIafe5Hg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="37"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*o9NFGVz1ua60kOpIafe5Hg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1o9NFGVz1ua60kOpIafe5Hg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*o9NFGVz1ua60kOpIafe5Hg.png"></noscript></div></div></figure><h4 name="f7f3" id="f7f3" class="graf graf--h4 graf-after--figure">Single object detection [<a href="https://youtu.be/0frKXR-2PBY?t=10m35s" data-href="https://youtu.be/0frKXR-2PBY?t=10m35s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">10:35</a>]</h4><p name="80a2" id="80a2" class="graf graf--p graf-after--h4">Let’s combine the two to create something that can classify and localize the largest object in each image.</p><p name="8a53" id="8a53" class="graf graf--p graf-after--p">There are 3 things that we need to do to train a neural network:</p><ol class="postList"><li name="bb44" id="bb44" class="graf graf--li graf-after--p">Data</li><li name="b33c" id="b33c" class="graf graf--li graf-after--li">Architecture</li><li name="62fa" id="62fa" class="graf graf--li graf-after--li">Loss Function</li></ol><h4 name="a6c0" id="a6c0" class="graf graf--h4 graf-after--li">1. Providing Data</h4><p name="e159" id="e159" class="graf graf--p graf-after--h4">We need a <code class="markup--code markup--p-code">ModelData</code>
 object whose independent variable is the images, and dependent variable
 is a tuple of bounding box coordinates and class label. There are 
several ways to do this, but here is a particularly lazy and convinient 
way Jeremy came up with is to create two <code class="markup--code markup--p-code">ModelData</code> objects representing the two different dependent variables we want (one with bounding boxes coordinates, one with classes).</p><pre name="b667" id="b667" class="graf graf--pre graf-after--p">f_model=resnet34<br>sz=224<br>bs=64</pre><pre name="65cf" id="65cf" class="graf graf--pre graf-after--pre">val_idxs = get_cv_idxs(len(trn_fns))<br>tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                       tfm_y=TfmType.COORD, aug_tfms=augs)</pre><pre name="3de6" id="3de6" class="graf graf--pre graf-after--pre">md = ImageClassifierData.from_csv(PATH, JPEGS, <strong class="markup--strong markup--pre-strong">BB_CSV</strong>, tfms=tfms, <br>                       continuous=True, val_idxs=val_idxs)</pre><pre name="e9fc" id="e9fc" class="graf graf--pre graf-after--pre">md2 = ImageClassifierData.from_csv(PATH, JPEGS, <strong class="markup--strong markup--pre-strong">CSV</strong>,<br>                       tfms=tfms_from_model(f_model, sz))</pre><p name="18c8" id="18c8" class="graf graf--p graf-after--pre">A dataset can be anything with <code class="markup--code markup--p-code">__len__</code> and <code class="markup--code markup--p-code">__getitem__</code>. Here's a dataset that adds a 2nd label to an existing dataset:</p><pre name="d395" id="d395" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConcatLblDataset</strong>(Dataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ds, y2): self.ds,self.y2 = ds,y2<br>    <strong class="markup--strong markup--pre-strong">def</strong> __len__(self): <strong class="markup--strong markup--pre-strong">return</strong> len(self.ds)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> __getitem__(self, i):<br>        x,y = self.ds[i]<br>        <strong class="markup--strong markup--pre-strong">return</strong> (x, (y,self.y2[i]))</pre><ul class="postList"><li name="646c" id="646c" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ds</code>&nbsp;: contains both independent and dependent variables</li><li name="8dc3" id="8dc3" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">y2</code>&nbsp;: contains the additional dependent variables</li><li name="c9d3" id="c9d3" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">(x, (y,self.y2[i]))</code>&nbsp;: <code class="markup--code markup--li-code">__getitem___</code> returns an independent variable and the combination of two dependent variables.</li></ul><p name="8552" id="8552" class="graf graf--p graf-after--li">We’ll use it to add the classes to the bounding boxes labels.</p><pre name="36e4" id="36e4" class="graf graf--pre graf-after--p">trn_ds2 = ConcatLblDataset(md.trn_ds, md2.trn_y)<br>val_ds2 = ConcatLblDataset(md.val_ds, md2.val_y)</pre><p name="a83d" id="a83d" class="graf graf--p graf-after--pre">Here is an example dependent variable:</p><pre name="e61c" id="e61c" class="graf graf--pre graf-after--p">val_ds2[0][1]</pre><pre name="bb40" id="bb40" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(array([   0.,   49.,  205.,  180.], dtype=float32), 14)</em></pre><p name="3916" id="3916" class="graf graf--p graf-after--pre">We can replace the dataloaders’ datasets with these new ones.</p><pre name="abd7" id="abd7" class="graf graf--pre graf-after--p">md.trn_dl.dataset = trn_ds2<br>md.val_dl.dataset = val_ds2</pre><p name="e63c" id="e63c" class="graf graf--p graf-after--pre">We have to <code class="markup--code markup--p-code">denorm</code>alize the images from the dataloader before they can be plotted.</p><pre name="bb3f" id="bb3f" class="graf graf--pre graf-after--p">x,y = next(iter(md.val_dl))<br>idx = 3<br>ima = md.val_ds.ds.denorm(to_np(x))[idx]<br>b = bb_hw(to_np(y[0][idx])); b</pre><pre name="7b26" id="7b26" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([  52.,   38.,  106.,  184.], dtype=float32)</em></pre><pre name="31ab" id="31ab" class="graf graf--pre graf-after--pre">ax = show_img(ima)<br>draw_rect(ax, b)<br>draw_text(ax, b[:2], md2.classes[y[1][idx]])</pre><figure name="442b" id="442b" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 300px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*6QqfOpqgyRogEiTCU8WZgQ.png" data-width="300" data-height="300" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16QqfOpqgyRogEiTCU8WZgQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*6QqfOpqgyRogEiTCU8WZgQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16QqfOpqgyRogEiTCU8WZgQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*6QqfOpqgyRogEiTCU8WZgQ.png"></noscript></div></div></figure><h4 name="2513" id="2513" class="graf graf--h4 graf-after--figure">2. Choosing Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=13m54s" data-href="https://youtu.be/0frKXR-2PBY?t=13m54s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">13:54</a>]</h4><p name="18d6" id="18d6" class="graf graf--p graf-after--h4">The
 architecture will be the same as the one we used for the classifier and
 bounding box regression, but we will just combine them. In other words,
 if we have <code class="markup--code markup--p-code">c</code> classes, then the number of activations we need in the final layer is 4 plus <code class="markup--code markup--p-code">c</code>. 4 for bounding box coordinates and <code class="markup--code markup--p-code">c</code> probabilities (one per class).</p><p name="f053" id="f053" class="graf graf--p graf-after--p">We’ll
 use an extra linear layer this time, plus some dropout, to help us 
train a more flexible model. In general, we want our custom head to be 
capable of solving the problem on its own if the pre-trained backbone it
 is connected to is appropriate. So in this case, we are trying to do 
quite a bit — classifier and bounding box regression, so just the single
 linear layer does not seem enough. If you were wondering why there is 
no <code class="markup--code markup--p-code">BatchNorm1d</code> after the first <code class="markup--code markup--p-code">ReLU</code>&nbsp;, ResNet backbone already has <code class="markup--code markup--p-code">BatchNorm1d</code> as its final layer.</p><pre name="eb1b" id="eb1b" class="graf graf--pre graf-after--p">head_reg4 = nn.Sequential(<br>    Flatten(),<br>    nn.ReLU(),<br>    nn.Dropout(0.5),<br>    nn.Linear(25088,256),<br>    nn.ReLU(),<br>    nn.BatchNorm1d(256),<br>    nn.Dropout(0.5),<br>    nn.Linear(256,<strong class="markup--strong markup--pre-strong">4+len(cats)</strong>),<br>)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br><br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><h4 name="f6e9" id="f6e9" class="graf graf--h4 graf-after--pre">3. Loss Function&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=15m46s" data-href="https://youtu.be/0frKXR-2PBY?t=15m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:46</a>]</h4><p name="95d7" id="95d7" class="graf graf--p graf-after--h4">The loss function needs to look at these <code class="markup--code markup--p-code">4 + len(cats)</code>
 activations and decide if they are good — whether these numbers 
accurately reflect the position and class of the largest object in the 
image. We know how to do this. For the first 4 activations, we will use 
L1Loss just like we did before (L1Loss is like a Mean Squared 
Error — instead of sum of squared errors, it uses sum of absolute 
values). For rest of the activations, we can use cross entropy loss.</p><pre name="7b18" id="7b18" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> detn_loss(input, target):<br>    bb_t,c_t = target<br>    bb_i,c_i = input[:, :4], input[:, 4:]<br>    bb_i = F.sigmoid(bb_i)*224<br><em class="markup--em markup--pre-em">    # I looked at these quantities separately first then picked a <br>    # multiplier to make them approximately equal</em><br>    <strong class="markup--strong markup--pre-strong">return</strong> F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20</pre><pre name="16e0" id="16e0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> detn_l1(input, target):<br>    bb_t,_ = target<br>    bb_i = input[:, :4]<br>    bb_i = F.sigmoid(bb_i)*224<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.l1_loss(V(bb_i),V(bb_t)).data</pre><pre name="f829" id="f829" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> detn_acc(input, target):<br>    _,c_t = target<br>    c_i = input[:, 4:]<br>    <strong class="markup--strong markup--pre-strong">return</strong> accuracy(c_i, c_t)</pre><pre name="4eaa" id="4eaa" class="graf graf--pre graf-after--pre">learn.crit = detn_loss<br>learn.metrics = [detn_acc, detn_l1]</pre><ul class="postList"><li name="f337" id="f337" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">input</code>&nbsp;: activations</li><li name="5cb2" id="5cb2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">target</code>&nbsp;: ground truth</li><li name="9b1e" id="9b1e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bb_t,c_t = target</code>&nbsp;:
 Our custom dataset returns a tuple containing bounding box coordinates 
and classes. This assignment will destructure them.</li><li name="8867" id="8867" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bb_i,c_i = input[:,&nbsp;:4], input[:, 4:]</code>&nbsp;: the first&nbsp;<code class="markup--code markup--li-code">:</code> is for the batch dimension.</li><li name="e6c0" id="e6c0" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">b_i = F.sigmoid(bb_i)*224</code>&nbsp;: we know our image is 224 by 224. <code class="markup--code markup--li-code">Sigmoid</code> will force it to be between 0 and 1, and multiply it by 224 to help our neural net to be in the range of what it has to be.</li></ul><p name="ab55" id="ab55" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question:</strong> As a general rule, is it better to put BatchNorm before or after ReLU [<a href="https://youtu.be/0frKXR-2PBY?t=18m2s" data-href="https://youtu.be/0frKXR-2PBY?t=18m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">18:02</a>]?
 Jeremy would suggest to put it after a ReLU because BathNorm is meant 
to move towards zero-mean one-standard deviation. So if you put ReLU 
right after it, you are truncating it at zero so there is no way to 
create negative numbers. But if you put ReLU then BatchNorm, it does 
have that ability and gives slightly better results. Having said that, 
it is not too big of a deal either way. You see during this part of the 
course, most of the time, Jeremy does ReLU then BatchNorm but sometimes 
does the opposite when he wants to be consistent with the paper.</p><p name="f68d" id="f68d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the intuition behind using dropout after a BatchNorm? Doesn’t BatchNorm already do a good job of regularizing [<a href="https://youtu.be/0frKXR-2PBY?t=19m12s" data-href="https://youtu.be/0frKXR-2PBY?t=19m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:12</a>]?
 BatchNorm does an okay job of regularizing but if you think back to 
part 1 when we discussed a list of things we do to avoid overfitting and
 adding BatchNorm is one of them as is data augmentation. But it’s 
perfectly possible that you’ll still be overfitting. One nice thing 
about dropout is that is it has a parameter to say how much to drop out.
 Parameters are great specifically parameters that decide how much to 
regularize because it lets you build a nice big over parameterized model
 and then decide on how much to regularize it. Jeremy tends to always 
put in a drop out starting with <code class="markup--code markup--p-code">p=0</code>
 and then as he adds regularization, he can just change the dropout 
parameter without worrying about if he saved a model he want to be able 
to load it back, but if he had dropout layers in one but no in another, 
it will not load anymore. So this way, it stays consistent.</p><p name="c531" id="c531" class="graf graf--p graf-after--p">Now we have out inputs and targets, we can calculate the L1 loss and add the cross entropy [<a href="https://youtu.be/0frKXR-2PBY?t=20m39s" data-href="https://youtu.be/0frKXR-2PBY?t=20m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:39</a>]:</p><p name="95f1" id="95f1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20</code></p><p name="d04f" id="d04f" class="graf graf--p graf-after--p">This
 is our loss function. Cross entropy and L1 loss may be of wildly 
different scales — in which case in the loss function, the larger one is
 going to dominate. In this case, Jeremy printed out the values and 
found out that if we multiply cross entropy by 20 that makes them about 
the same scale.</p><pre name="eb4a" id="eb4a" class="graf graf--pre graf-after--p">lr=1e-2<br>learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))</pre><pre name="70ea" id="70ea" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      72.036466  45.186367  0.802133   32.647586 <br>    1      51.037587  36.34964   0.828425   25.389733     <br>    2      41.4235    35.292709  0.835637   24.343577</em></pre><pre name="0564" id="0564" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[35.292709, 0.83563701808452606, 24.343576669692993]</em></pre><p name="6180" id="6180" class="graf graf--p graf-after--pre">It is nice to print out information as you train, so we grabbed L1 loss and added it as metrics.</p><pre name="06fa" id="06fa" class="graf graf--pre graf-after--p">learn.save('reg1_0')<br>learn.freeze_to(-2)<br>lrs = np.array([lr/100, lr/10, lr])<br>learn.fit(lrs/5, 1, cycle_len=5, use_clr=(32,10))</pre><pre name="11fb" id="11fb" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      34.448113  35.972973  0.801683   22.918499 <br>    1      28.889909  33.010857  0.830379   21.689888     <br>    2      24.237017  30.977512  0.81881    20.817996     <br>    3      21.132993  30.60677   0.83143    20.138552     <br>    4      18.622983  30.54178   0.825571   19.832196</pre><pre name="0865" id="0865" class="graf graf--pre graf-after--pre">[30.54178, 0.82557091116905212, 19.832195997238159]</pre><pre name="c0fb" id="c0fb" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.fit(lrs/10, 1, cycle_len=10, use_clr=(32,10))</pre><pre name="6c87" id="6c87" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      15.957164  31.111507  0.811448   19.970753 <br>    1      15.955259  32.597153  0.81235    20.111022     <br>    2      15.648723  32.231941  0.804087   19.522853     <br>    3      14.876172  30.93821   0.815805   19.226574     <br>    4      14.113872  31.03952   0.808594   19.155093     <br>    5      13.293885  29.736671  0.826022   18.761728     <br>    6      12.562566  30.000023  0.827524   18.82006      <br>    7      11.885125  30.28841   0.82512    18.904158     <br>    8      11.498326  30.070133  0.819712   18.635296     <br>    9      11.015841  30.213772  0.815805   18.551489</pre><pre name="0591" id="0591" class="graf graf--pre graf-after--pre">[30.213772, 0.81580528616905212, 18.551488876342773]</pre><p name="2c47" id="2c47" class="graf graf--p graf-after--pre">A
 detection accuracy is in the low 80’s which is the same as what it was 
before. This is not surprising because ResNet was designed to do 
classification so we wouldn’t expect to be able to improve things in 
such a simple way. It certainly wasn’t designed to do bounding box 
regression. It was explicitly actually designed in such a way to not 
care about geometry — it takes the last 7 by 7 grid of activations and 
averages them all together throwing away all the information about where
 everything came from.</p><p name="684d" id="684d" class="graf graf--p graf-after--p">Interestingly,
 when we do accuracy (classification) and bounding box at the same time,
 the L1 seems a little bit better than when we just do bounding box 
regression [<a href="https://youtu.be/0frKXR-2PBY?t=22m46s" data-href="https://youtu.be/0frKXR-2PBY?t=22m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:46</a>].
 If that is counterintuitive to you, then this would be one of the main 
things to think about after this lesson since it is a really important 
idea. The idea is this — figuring out what the main object in an image 
is, is kind of the hard part. Then figuring out exactly where the 
bounding box is and what class it is is the easy part in a way. So when 
you have a single network that’s both saying what is the object and 
where is the object, it’s going to share all the computation about 
finding the object. And all that shared computation is very efficient. 
When we back propagate the errors in the class and in the place, that’s 
all the information that is going to help the computation around finding
 the biggest object. So anytime you have multiple tasks which share some
 concept of what those tasks would need to do to complete their work, it
 is very likely they should share at least some layers of the network 
together. Later today, we will look at a model where most of the layers 
are shared except for the last one.</p><p name="acb4" id="acb4" class="graf graf--p graf-after--p">Here are the result [<a href="https://youtu.be/0frKXR-2PBY?t=24m34s" data-href="https://youtu.be/0frKXR-2PBY?t=24m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:34</a>]. As before, it does a good job when there is single major object in the image.</p><figure name="3d36" id="3d36" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 477px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.10000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*g4JAJgAcDNDikhgwtLTcwQ.png" data-width="2028" data-height="1382" data-action="zoom" data-action-value="1*g4JAJgAcDNDikhgwtLTcwQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1g4JAJgAcDNDikhgwtLTcwQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*g4JAJgAcDNDikhgwtLTcwQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1g4JAJgAcDNDikhgwtLTcwQ_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*g4JAJgAcDNDikhgwtLTcwQ.png"></noscript></div></div></figure><h3 name="8223" id="8223" class="graf graf--h3 graf-after--figure">Multi label classification [<a href="https://youtu.be/0frKXR-2PBY?t=25m29s" data-href="https://youtu.be/0frKXR-2PBY?t=25m29s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">25:29</a>]</h3><p name="cf6b" id="cf6b" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="5908" id="5908" class="graf graf--p graf-after--p">We
 want to keep building models that are slightly more complex than the 
last model so that if something stops working, we know exactly where it 
broke. Here are functions from the previous notebook:</p><pre name="52fd" id="52fd" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="9b4a" id="9b4a" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">json</strong>, <strong class="markup--strong markup--pre-strong">pdb</strong><br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">PIL</strong> <strong class="markup--strong markup--pre-strong">import</strong> ImageDraw, ImageFont<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">matplotlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> patches, patheffects<br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><h4 name="197a" id="197a" class="graf graf--h4 graf-after--pre">Setup</h4><pre name="1c51" id="1c51" class="graf graf--pre graf-after--h4">PATH = Path('data/pascal')<br>trn_j = json.load((PATH / 'pascal_train2007.json').open())<br>IMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', <br>                                 'categories']<br>FILE_NAME,ID,IMG_ID,CAT_ID,BBOX = 'file_name','id','image_id', <br>                                  'category_id','bbox'<br><br>cats = dict((o[ID], o['name']) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[CATEGORIES])<br>trn_fns = dict((o[ID], o[FILE_NAME]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[IMAGES])<br>trn_ids = [o[ID] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[IMAGES]]<br><br>JPEGS = 'VOCdevkit/VOC2007/JPEGImages'<br>IMG_PATH = PATH/JPEGS</pre><pre name="5736" id="5736" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_trn_anno():<br>    trn_anno = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:[])<br>    <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[ANNOTATIONS]:<br>        <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> o['ignore']:<br>            bb = o[BBOX]<br>            bb = np.array([bb[1], bb[0], bb[3]+bb[1]-1, <br>                           bb[2]+bb[0]-1])<br>            trn_anno[o[IMG_ID]].append((bb,o[CAT_ID]))<br>    <strong class="markup--strong markup--pre-strong">return</strong> trn_anno<br><br>trn_anno = get_trn_anno()</pre><pre name="d612" id="d612" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.set_xticks(np.linspace(0, 224, 8))<br>    ax.set_yticks(np.linspace(0, 224, 8))<br>    ax.grid()<br>    ax.set_yticklabels([])<br>    ax.set_xticklabels([])<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_outline(o, lw):<br>    o.set_path_effects([patheffects.Stroke(<br>        linewidth=lw, foreground='black'), patheffects.Normal()])<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_rect(ax, b, color='white'):<br>    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], <br>                         fill=<strong class="markup--strong markup--pre-strong">False</strong>, edgecolor=color, lw=2))<br>    draw_outline(patch, 4)<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_text(ax, xy, txt, sz=14, color='white'):<br>    text = ax.text(*xy, txt,<br>        verticalalignment='top', color=color, fontsize=sz, <br>        weight='bold')<br>    draw_outline(text, 1)</pre><pre name="9675" id="9675" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> bb_hw(a): <strong class="markup--strong markup--pre-strong">return</strong> np.array([a[1],a[0],a[3]-a[1],a[2]-a[0]])<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_im(im, ann):<br>    ax = show_img(im, figsize=(16,8))<br>    <strong class="markup--strong markup--pre-strong">for</strong> b,c <strong class="markup--strong markup--pre-strong">in</strong> ann:<br>        b = bb_hw(b)<br>        draw_rect(ax, b)<br>        draw_text(ax, b[:2], cats[c], sz=16)<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_idx(i):<br>    im_a = trn_anno[i]<br>    im = open_image(IMG_PATH/trn_fns[i])<br>    draw_im(im, im_a)</pre><h4 name="153a" id="153a" class="graf graf--h4 graf-after--pre">Multi class&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=26m12s" data-href="https://youtu.be/0frKXR-2PBY?t=26m12s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">26:12</a>]</h4><pre name="189d" id="189d" class="graf graf--pre graf-after--h4">MC_CSV = PATH/'tmp/mc.csv'</pre><pre name="9a35" id="9a35" class="graf graf--pre graf-after--pre">trn_anno[12]</pre><pre name="a704" id="a704" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[(array([ 96, 155, 269, 350]), 7)]</em></pre><pre name="2ebe" id="2ebe" class="graf graf--pre graf-after--pre">mc = [set([cats[p[1]] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> trn_anno[o]]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids]<br>mcs = [' '.join(str(p) <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> mc]</pre><pre name="8c39" id="8c39" class="graf graf--pre graf-after--pre">df = pd.DataFrame({'fn': [trn_fns[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids], <br>                   'clas': mcs}, columns=['fn','clas'])<br>df.to_csv(MC_CSV, index=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><p name="512b" id="512b" class="graf graf--p graf-after--pre">One of the students pointed out that by using Pandas, we can do things much simpler than using <code class="markup--code markup--p-code">collections.defaultdict</code> and shared <a href="https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed" data-href="https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this gist</a>. The more you get to know Pandas, the more often you realize it is a good way to solve lots of different problems.</p><p name="2dc8" id="2dc8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>:
 When you are incrementally building on top of smaller models, do you 
reuse them as pre-trained weights? or do you toss it away then retrain 
from scratch [<a href="https://youtu.be/0frKXR-2PBY?t=27m11s" data-href="https://youtu.be/0frKXR-2PBY?t=27m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">27:11</a>]?
 When Jeremy is figuring stuff out as he goes like this, he would 
generally lean towards tossing away because reusing pre-trained weights 
introduces unnecessary complexities. However, if he is trying to get to a
 point where he can train on really big images, he will generally start 
on much smaller and often re-use these weights.</p><pre name="5b68" id="5b68" class="graf graf--pre graf-after--p">f_model=resnet34<br>sz=224<br>bs=64</pre><pre name="a183" id="a183" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, MC_CSV, tfms=tfms)</pre><pre name="99ad" id="99ad" class="graf graf--pre graf-after--pre">learn = ConvLearner.pretrained(f_model, md)<br>learn.opt_fn = optim.Adam</pre><pre name="cc2f" id="cc2f" class="graf graf--pre graf-after--pre">lr = 2e-2</pre><pre name="17d6" id="17d6" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))</pre><pre name="560a" id="560a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                  <br>    0      0.104836   0.085015   0.972356  <br>    1      0.088193   0.079739   0.972461                   <br>    2      0.072346   0.077259   0.974114</em></pre><pre name="60c4" id="60c4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.077258907, 0.9741135761141777]</em></pre><pre name="a327" id="a327" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100, lr/10, lr])</pre><pre name="5faa" id="5faa" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)</pre><pre name="d590" id="d590" class="graf graf--pre graf-after--pre">learn.fit(lrs/10, 1, cycle_len=5, use_clr=(32,5))</pre><pre name="fd47" id="fd47" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                   <br>    0      0.063236   0.088847   0.970681  <br>    1      0.049675   0.079885   0.973723                   <br>    2      0.03693    0.076906   0.975601                   <br>    3      0.026645   0.075304   0.976187                   <br>    4      0.018805   0.074934   0.975165</em></pre><pre name="5ca7" id="5ca7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.074934497, 0.97516526281833649]</em></pre><pre name="e973" id="e973" class="graf graf--pre graf-after--pre">learn.save('mclas')</pre><pre name="2834" id="2834" class="graf graf--pre graf-after--pre">learn.load('mclas')</pre><pre name="8c3b" id="8c3b" class="graf graf--pre graf-after--pre">y = learn.predict()<br>x,_ = next(iter(md.val_dl))<br>x = to_np(x)</pre><pre name="5b1f" id="5b1f" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.denorm(x)[i]<br>    ya = np.nonzero(y[i]&gt;0.4)[0]<br>    b = '<strong class="markup--strong markup--pre-strong">\n</strong>'.join(md.classes[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> ya)<br>    ax = show_img(ima, ax=ax)<br>    draw_text(ax, (0,0), b)<br>plt.tight_layout()</pre><figure name="76a2" id="76a2" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 482px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.89999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*2m1Qoq3NhsqdYBd4hUTR6A.png" data-width="2038" data-height="1404" data-action="zoom" data-action-value="1*2m1Qoq3NhsqdYBd4hUTR6A.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12m1Qoq3NhsqdYBd4hUTR6A.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*2m1Qoq3NhsqdYBd4hUTR6A.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12m1Qoq3NhsqdYBd4hUTR6A_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*2m1Qoq3NhsqdYBd4hUTR6A.png"></noscript></div></div></figure><p name="4bec" id="4bec" class="graf graf--p graf-after--figure">Multi-class classification is pretty straight forward [<a href="https://youtu.be/0frKXR-2PBY?t=28m28s" data-href="https://youtu.be/0frKXR-2PBY?t=28m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:28</a>]. One minor tweak is the use of <code class="markup--code markup--p-code">set</code> in this line so that each object type appear once.:</p><pre name="de81" id="de81" class="graf graf--pre graf-after--p">mc = [<strong class="markup--strong markup--pre-strong">set</strong>([cats[p[1]] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> trn_anno[o]]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids]</pre><h4 name="f2f7" id="f2f7" class="graf graf--h4 graf-after--pre">SSD and YOLO&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=29m10s" data-href="https://youtu.be/0frKXR-2PBY?t=29m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">29:10</a>]</h4><p name="4997" id="4997" class="graf graf--p graf-after--h4">We have an input image that goes through a conv net which outputs a vector of size <code class="markup--code markup--p-code">4+c</code> where <code class="markup--code markup--p-code">c=len(cats)</code>&nbsp;.
 This gives us an object detector for a single largest object. Let’s now
 create one that finds 16 objects. The obvious way to do this would be 
to take the last linear layer and rather than having <code class="markup--code markup--p-code">4+c</code> outputs, we could have <code class="markup--code markup--p-code">16x(4+c)</code>
 outputs. This gives us 16 sets of class probabilities and 16 sets of 
bounding box coordinates. Then we would just need a loss function that 
will check whether those 16 sets of bounding boxes correctly represented
 the up to 16 objects in the image (we will go into the loss function 
later).</p><figure name="09c8" id="09c8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 423px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 84.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*fPHmCosDHcrHmtKvWFK9Mg.png" data-width="500" data-height="423" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1fPHmCosDHcrHmtKvWFK9Mg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="62"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*fPHmCosDHcrHmtKvWFK9Mg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1fPHmCosDHcrHmtKvWFK9Mg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*fPHmCosDHcrHmtKvWFK9Mg.png"></noscript></div></div></figure><p name="8cbc" id="8cbc" class="graf graf--p graf-after--figure">The second way to do this is rather than using <code class="markup--code markup--p-code">nn.linear</code>, what if instead, we took from our ResNet convolutional backbone and added an <code class="markup--code markup--p-code">nn.Conv2d</code> with stride 2 [<a href="https://youtu.be/0frKXR-2PBY?t=31m32s" data-href="https://youtu.be/0frKXR-2PBY?t=31m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:32</a>]? This will give us a <code class="markup--code markup--p-code">4x4x[# of filters]</code> tensor — here let’s make it <code class="markup--code markup--p-code">4x4x(4+c)</code>
 so that we get a tensor where the number of elements is exactly equal 
to the number of elements we wanted. Now if we created a loss function 
that took a <code class="markup--code markup--p-code">4x4x(4+c)</code> tensor and and mapped it to 16 objects in the image and checked whether each one was correctly represented by these <code class="markup--code markup--p-code">4+c</code> activations, this would work as well. It turns out, both of these approaches are actually used [<a href="https://youtu.be/0frKXR-2PBY?t=33m48s" data-href="https://youtu.be/0frKXR-2PBY?t=33m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">33:48</a>].
 The approach where the output is one big long vector from a fully 
connected linear layer is used by a class of models known as <a href="https://arxiv.org/abs/1506.02640" data-href="https://arxiv.org/abs/1506.02640" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">YOLO (You Only Look Once)</a>, where else, the approach of the convolutional activations is used by models which started with something called <a href="https://arxiv.org/abs/1512.02325" data-href="https://arxiv.org/abs/1512.02325" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">SSD (Single Shot Detector)</a>.
 Since these things came out very similar times in late 2015, things are
 very much moved towards SSD. So the point where this morning, <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" data-href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">YOLO version 3</a> came out and is now doing SSD, so that’s what we are going to do. We will also learn about why this makes more sense as well.</p><h4 name="20e9" id="20e9" class="graf graf--h4 graf-after--p">Anchor boxes&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=35m04s" data-href="https://youtu.be/0frKXR-2PBY?t=35m04s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">35:04</a>]</h4><figure name="42e6" id="42e6" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 428px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 85.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8kpDP3FZFxW99IUQE0C8Xw.png" data-width="500" data-height="428" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18kpDP3FZFxW99IUQE0C8Xw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="62"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*8kpDP3FZFxW99IUQE0C8Xw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18kpDP3FZFxW99IUQE0C8Xw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*8kpDP3FZFxW99IUQE0C8Xw.png"></noscript></div></div></figure><p name="4bfa" id="4bfa" class="graf graf--p graf-after--figure">Let’s imagine that we had another <code class="markup--code markup--p-code">Conv2d(stride=2)</code> then we would have <code class="markup--code markup--p-code">2x2x(4+c)</code> tensor. Basically, it is creating a grid that looks something like this:</p><figure name="37fd" id="37fd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 395px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.49999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*uA-oJok4-Rng6mnHOOPyNQ.png" data-width="965" data-height="545" data-action="zoom" data-action-value="1*uA-oJok4-Rng6mnHOOPyNQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1uA-oJok4-Rng6mnHOOPyNQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="40"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*uA-oJok4-Rng6mnHOOPyNQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1uA-oJok4-Rng6mnHOOPyNQ_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*uA-oJok4-Rng6mnHOOPyNQ.png"></noscript></div></div></figure><p name="c44d" id="c44d" class="graf graf--p graf-after--figure">This
 is how the geometry of the activations of the second extra 
convolutional stride 2 layer are. Remember, stride 2 convolution does 
the same thing to the geometry of the activations as a stride 1 
convolution followed by maxpooling assuming the padding is ok.</p><p name="fd03" id="fd03" class="graf graf--p graf-after--p">Let’s talk about what we might do here [<a href="https://youtu.be/0frKXR-2PBY?t=36m9s" data-href="https://youtu.be/0frKXR-2PBY?t=36m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:09</a>]. We want each of these grid cell to be responsible for finding the largest object in that part of the image.</p><h4 name="3bae" id="3bae" class="graf graf--h4 graf-after--p">Receptive Field&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=37m20s" data-href="https://youtu.be/0frKXR-2PBY?t=37m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">37:20</a>]</h4><p name="daf5" id="daf5" class="graf graf--p graf-after--h4">Why
 do we care about the idea that we would like each convolutional grid 
cell to be responsible for finding things that are in the corresponding 
part of the image? The reason is because of something called the 
receptive field of that convolutional grid cell. The basic idea is that 
throughout your convolutional layers, every piece of those tensors has a
 receptive field which means which part of the input image was 
responsible for calculating that cell. Like all things in life, the 
easiest way to see this is with Excel [<a href="https://youtu.be/0frKXR-2PBY?t=38m1s" data-href="https://youtu.be/0frKXR-2PBY?t=38m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">38:01</a>].</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="4875" id="4875" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 567px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*IgL2CMSit3Hh9N2Fq2Zlgg.png" data-width="1779" data-height="1009" data-action="zoom" data-action-value="1*IgL2CMSit3Hh9N2Fq2Zlgg.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1IgL2CMSit3Hh9N2Fq2Zlgg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="42"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*IgL2CMSit3Hh9N2Fq2Zlgg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1IgL2CMSit3Hh9N2Fq2Zlgg_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*IgL2CMSit3Hh9N2Fq2Zlgg.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="d0cd" id="d0cd" class="graf graf--p graf-after--figure">Take a single activation (in this case in the maxpool layer)and let’s see where it came from [<a href="https://youtu.be/0frKXR-2PBY?t=38m45s" data-href="https://youtu.be/0frKXR-2PBY?t=38m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">38:45</a>].
 In excel you can do Formulas → Trace Precedents. Tracing all the way 
back to the input layer, you can see that it came from this 6 x 6 
portion of the image (as well as filters). What is more, the middle 
portion has lots of weights coming out of where else cells in the 
outside only have one weight coming out. So we call this 6 x 6 cells the
 receptive field of the one activation we picked.</p><figure name="719f" id="719f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 236px; max-height: 233px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 98.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*cCBVbJ2WjiPMlqX4nA2bwA.png" data-width="236" data-height="233" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1cCBVbJ2WjiPMlqX4nA2bwA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="72"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*cCBVbJ2WjiPMlqX4nA2bwA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1cCBVbJ2WjiPMlqX4nA2bwA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*cCBVbJ2WjiPMlqX4nA2bwA.png"></noscript></div></div><figcaption class="imageCaption">3x3 convolution with opacity 15% — clearly the center of the box has more dependencies</figcaption></figure><p name="10e4" id="10e4" class="graf graf--p graf-after--figure">Note that the receptive field is not just saying it’s this box but also that the center of the box has more dependencies [<a href="https://youtu.be/0frKXR-2PBY?t=40m27s" data-href="https://youtu.be/0frKXR-2PBY?t=40m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:27</a>]
 This is a critically important concept when it comes to understanding 
architectures and understanding why conv nets work the way they do.</p><h4 name="f66c" id="f66c" class="graf graf--h4 graf-after--p">Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=41m18s" data-href="https://youtu.be/0frKXR-2PBY?t=41m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">41:18</a>]</h4><p name="19c4" id="19c4" class="graf graf--p graf-after--h4">The
 architecture is, we will have a ResNet backbone followed by one or more
 2D convolutions (one for now) which is going to give us a <code class="markup--code markup--p-code">4x4</code> grid.</p><pre name="ddb9" id="ddb9" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StdConv</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nin, nout, stride=2, drop=0.1):<br>        super().__init__()<br>        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, <br>                              padding=1)<br>        self.bn = nn.BatchNorm2d(nout)<br>        self.drop = nn.Dropout(drop)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <br>        <strong class="markup--strong markup--pre-strong">return</strong> self.drop(self.bn(F.relu(self.conv(x))))<br>        <br><strong class="markup--strong markup--pre-strong">def</strong> flatten_conv(x,k):<br>    bs,nf,gx,gy = x.size()<br>    x = x.permute(0,2,3,1).contiguous()<br>    <strong class="markup--strong markup--pre-strong">return</strong> x.view(bs,-1,nf//k)</pre><pre name="fa2e" id="fa2e" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">OutConv</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, nin, bias):<br>        super().__init__()<br>        self.k = k<br>        self.oconv1 = nn.Conv2d(nin, (len(id2cat)+1)*k, 3, <br>                                padding=1)<br>        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)<br>        self.oconv1.bias.data.zero_().add_(bias)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">return</strong> [flatten_conv(self.oconv1(x), self.k),<br>                flatten_conv(self.oconv2(x), self.k)]</pre><pre name="d08c" id="d08c" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SSD_Head</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, bias):<br>        super().__init__()<br>        self.drop = nn.Dropout(0.25)<br>        self.sconv0 = StdConv(512,256, stride=1)<br>        self.sconv2 = StdConv(256,256)<br>        self.out = OutConv(k, 256, bias)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.drop(F.relu(x))<br>        x = self.sconv0(x)<br>        x = self.sconv2(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.out(x)<br><br>head_reg4 = SSD_Head(k, -3.)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><p name="2589" id="2589" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">SSD_Head</strong></p><ol class="postList"><li name="62b3" id="62b3" class="graf graf--li graf-after--p">We start with ReLU and dropout</li><li name="dbc8" id="dbc8" class="graf graf--li graf-after--li">Then
 stride 1 convolution. The reason we start with a stride 1 convolution 
is because that does not change the geometry at all — it just lets us 
add an extra layer of calculation. It lets us create not just a linear 
layer but now we have a little mini neural network in our custom head. <code class="markup--code markup--li-code">StdConv</code>
 is defined above — it does convolution, ReLU, BatchNorm, and dropout. 
Most research code you see won’t define a class like this, instead they 
write the entire thing again and again. Don’t be like that. Duplicate 
code leads to errors and poor understanding.</li><li name="74ad" id="74ad" class="graf graf--li graf-after--li">Stride 2 convolution [<a href="https://youtu.be/0frKXR-2PBY?t=44m56s" data-href="https://youtu.be/0frKXR-2PBY?t=44m56s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">44:56</a>]</li><li name="b4de" id="b4de" class="graf graf--li graf-after--li">At the end, the output of step 3 is <code class="markup--code markup--li-code">4x4</code> which gets passed to <code class="markup--code markup--li-code">OutConv</code>. <code class="markup--code markup--li-code">OutConv</code>
 has two separate convolutional layers each of which is stride 1 so it 
is not changing the geometry of the input. One of them is of length of 
the number of classes (ignore <code class="markup--code markup--li-code">k</code> for now and <code class="markup--code markup--li-code">+1</code>
 is for “background” — i.e. no object was detected), the other’s length 
is 4. Rather than having a single conv layer that outputs <code class="markup--code markup--li-code">4+c</code>,
 let’s have two conv layers and return their outputs in a list. This 
allows these layers to specialize just a little bit. We talked about 
this idea that when you have multiple tasks, they can share layers, but 
they do not have to share all the layers. In this case, our two tasks of
 creating a classifier and creating and creating bounding box regression
 share every single layers except the very last one.</li><li name="7bcd" id="7bcd" class="graf graf--li graf-after--li">At
 the end, we flatten out the convolution because Jeremy wrote the loss 
function to expect flattened out tensor, but we could totally rewrite it
 to not do that.</li></ol><h4 name="d6b4" id="d6b4" class="graf graf--h4 graf-after--li"><a href="https://github.com/fastai/fastai/blob/master/docs/style.md" data-href="https://github.com/fastai/fastai/blob/master/docs/style.md" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">Fastai Coding Style</a>&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=42m58s" data-href="https://youtu.be/0frKXR-2PBY?t=42m58s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:58</a>]</h4><p name="6d61" id="6d61" class="graf graf--p graf-after--h4">The
 first draft was released this week. It is very heavily orient towards 
the idea of expository programming which is the idea that programming 
code should be something that you can use to explain an idea, ideally as
 readily as mathematical notation, to somebody that understands your 
coding method. The idea goes back a very long way, but it was best 
described in the Turing Award lecture of 1979 by probably Jeremy’s 
greatest computer science hero Ken Iverson. He had been working on it 
since well before 1964 but 1964 was the first example of this approach 
of programming he released which is called APL and 25 years later, he 
won the Turing Award. He then passed on the baton to his son Eric 
Iverson. Fastai style guide is an attempt at taking some of these ideas.</p><h4 name="2d44" id="2d44" class="graf graf--h4 graf-after--p">Loss Function&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=47m44s" data-href="https://youtu.be/0frKXR-2PBY?t=47m44s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">47:44</a>]</h4><p name="aad3" id="aad3" class="graf graf--p graf-after--h4">The loss function needs to look at each of these 16 sets of activations, each of which has four bounding box coordinates and <code class="markup--code markup--p-code">c+1</code>
 class probabilities and decide if those activations are close or far 
away from the object which is the closest to this grid cell in the 
image. If nothing is there, then whether it is predicting background 
correctly. That turns out to be very hard to do.</p><h4 name="339d" id="339d" class="graf graf--h4 graf-after--p">Matching Problem&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=48m43s" data-href="https://youtu.be/0frKXR-2PBY?t=48m43s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">48:43</a>]</h4><figure name="b566" id="b566" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 540px; max-height: 541px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*2dqj3hivcOF6ThoL-nhMyA.png" data-width="540" data-height="541" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12dqj3hivcOF6ThoL-nhMyA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*2dqj3hivcOF6ThoL-nhMyA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/12dqj3hivcOF6ThoL-nhMyA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*2dqj3hivcOF6ThoL-nhMyA.png"></noscript></div></div></figure><p name="6b0b" id="6b0b" class="graf graf--p graf-after--figure">The
 loss function needs to take each of the objects in the image and match 
them to one of these convolutional grid cells to say “this grid cell is 
responsible for this particular object” so then it can go ahead and say 
“okay, how close are the 4 coordinates and how close are the class 
probabilities.</p><p name="8e9f" id="8e9f" class="graf graf--p graf-after--p">Here is our goal [<a href="https://youtu.be/0frKXR-2PBY?t=49m56s" data-href="https://youtu.be/0frKXR-2PBY?t=49m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:56</a>]:</p><figure name="f181" id="f181" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 201px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8M9x-WgHNasmuLSJNbKoaQ.png" data-width="1168" data-height="335" data-action="zoom" data-action-value="1*8M9x-WgHNasmuLSJNbKoaQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18M9x-WgHNasmuLSJNbKoaQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="20"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*8M9x-WgHNasmuLSJNbKoaQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18M9x-WgHNasmuLSJNbKoaQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*8M9x-WgHNasmuLSJNbKoaQ.png"></noscript></div></div></figure><p name="3bda" id="3bda" class="graf graf--p graf-after--figure">Our dependent variable looks like the one on the left, and our final convolutional layer is going to be <code class="markup--code markup--p-code">4x4x(c+1)</code> in this case <code class="markup--code markup--p-code">c=20</code>.
 We then flatten that out into a vector. Our goal is to come up with a 
function which takes in a dependent variable and also some particular 
set of activations that ended up coming out of the model and returns a 
higher number if these activations are not a good reflection of the 
ground truth bounding boxes; or a lower number if it is a good 
reflection.</p><h4 name="308c" id="308c" class="graf graf--h4 graf-after--p">Testing [<a href="https://youtu.be/0frKXR-2PBY?t=51m58s" data-href="https://youtu.be/0frKXR-2PBY?t=51m58s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">51:58</a>]</h4><pre name="986e" id="986e" class="graf graf--pre graf-after--h4">x,y = next(iter(md.val_dl))<br>x,y = V(x),V(y)<br>learn.model.eval()<br>batch = learn.model(x)<br>b_clas,b_bb = batch<br>b_clas.size(),b_bb.size()</pre><pre name="209c" id="209c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(torch.Size([64, 16, 21]), torch.Size([64, 16, 4]))</em></pre><p name="992c" id="992c" class="graf graf--p graf-after--pre">Make sure these shapes make sense. Let’s now look at the ground truth <code class="markup--code markup--p-code">y</code> [<a href="https://youtu.be/0frKXR-2PBY?t=53m24s" data-href="https://youtu.be/0frKXR-2PBY?t=53m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:24</a>]:</p><pre name="5514" id="5514" class="graf graf--pre graf-after--p">idx=7<br>b_clasi = b_clas[idx]<br>b_bboxi = b_bb[idx]<br>ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>bbox,clas = get_y(y[0][idx], y[1][idx])<br>bbox,clas</pre><pre name="b746" id="b746" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(Variable containing:<br>  0.6786  0.4866  0.9911  0.6250<br>  0.7098  0.0848  0.9911  0.5491<br>  0.5134  0.8304  0.6696  0.9063<br> [torch.cuda.FloatTensor of size 3x4 (GPU 0)], Variable containing:<br>   8<br>  10<br>  17<br> [torch.cuda.LongTensor of size 3 (GPU 0)])</em></pre><p name="a6a5" id="a6a5" class="graf graf--p graf-after--pre">Note
 that bounding box coordinates have been scaled to between 0 and 
1 — basically we are treating the image as being 1x1, so they are 
relative to the size of the image.</p><p name="b788" id="b788" class="graf graf--p graf-after--p">We already have <code class="markup--code markup--p-code">show_ground_truth</code> function. This <code class="markup--code markup--p-code">torch_gt</code> (gt: ground truth) function simply converts tensors into numpy array.</p><pre name="d582" id="d582" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> torch_gt(ax, ima, bbox, clas, prs=<strong class="markup--strong markup--pre-strong">None</strong>, thresh=0.4):<br>    <strong class="markup--strong markup--pre-strong">return</strong> show_ground_truth(ax, ima, to_np((bbox*224).long()),<br>         to_np(clas), <br>         to_np(prs) <strong class="markup--strong markup--pre-strong">if</strong> prs <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong> <strong class="markup--strong markup--pre-strong">else</strong> <strong class="markup--strong markup--pre-strong">None</strong>, thresh)</pre><pre name="4392" id="4392" class="graf graf--pre graf-after--pre">fig, ax = plt.subplots(figsize=(7,7))<br>torch_gt(ax, ima, bbox, clas)</pre><figure name="d862" id="d862" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 401px; max-height: 401px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*Q3ZtSRtk-a2OwKfE1wa5zw.png" data-width="401" data-height="401" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Q3ZtSRtk-a2OwKfE1wa5zw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*Q3ZtSRtk-a2OwKfE1wa5zw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Q3ZtSRtk-a2OwKfE1wa5zw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*Q3ZtSRtk-a2OwKfE1wa5zw.png"></noscript></div></div></figure><p name="68a9" id="68a9" class="graf graf--p graf-after--figure">The above is a ground truth. Here is our <code class="markup--code markup--p-code">4x4</code> grid cells from our final convolutional layer [<a href="https://youtu.be/0frKXR-2PBY?t=54m44s" data-href="https://youtu.be/0frKXR-2PBY?t=54m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">54:44</a>]:</p><pre name="1592" id="1592" class="graf graf--pre graf-after--p">fig, ax = plt.subplots(figsize=(7,7))<br>torch_gt(ax, ima, anchor_cnr, b_clasi.max(1)[1])</pre><figure name="9b0b" id="9b0b" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 401px; max-height: 401px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*xjKmShqdLnD_JX4Aj7U80g.png" data-width="401" data-height="401" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1xjKmShqdLnD_JX4Aj7U80g_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*xjKmShqdLnD_JX4Aj7U80g.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1xjKmShqdLnD_JX4Aj7U80g.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*xjKmShqdLnD_JX4Aj7U80g.png"></noscript></div></div></figure><p name="930c" id="930c" class="graf graf--p graf-after--figure">Each
 of these square boxes, different papers call them different things. The
 three terms you’ll hear are: anchor boxes, prior boxes, or default 
boxes. We will stick with the term anchor boxes.</p><p name="e77b" id="e77b" class="graf graf--p graf-after--p">What
 we are going to do for this loss function is we are going to go through
 a matching problem where we are going to take every one of these 16 
boxes and see which one of these three ground truth objects has the 
highest amount of overlap with a given square [<a href="https://youtu.be/0frKXR-2PBY?t=55m21s" data-href="https://youtu.be/0frKXR-2PBY?t=55m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:21</a>]. To do this, we have to have some way of measuring amount of overlap and a standard function for this is called <a href="https://en.wikipedia.org/wiki/Jaccard_index" data-href="https://en.wikipedia.org/wiki/Jaccard_index" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Jaccard index</a> (IoU).</p><figure name="7378" id="7378" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 300px; max-height: 234px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*10ORjq4HuOc0umcnojiDPA.png" data-width="300" data-height="234" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/110ORjq4HuOc0umcnojiDPA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="57"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*10ORjq4HuOc0umcnojiDPA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/110ORjq4HuOc0umcnojiDPA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*10ORjq4HuOc0umcnojiDPA.png"></noscript></div></div></figure><p name="3347" id="3347" class="graf graf--p graf-after--figure">We are going to go through and find the Jaccard overlap for each one of the three objects versus each of the 16 anchor boxes [<a href="https://youtu.be/0frKXR-2PBY?t=57m11s" data-href="https://youtu.be/0frKXR-2PBY?t=57m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:11</a>]. That is going to give us a <code class="markup--code markup--p-code">3x16</code> matrix.</p><p name="3634" id="3634" class="graf graf--p graf-after--p">Here are the <em class="markup--em markup--p-em">coordinates</em> of all of our anchor boxes (centers, height, width):</p><pre name="4953" id="4953" class="graf graf--pre graf-after--p">anchors</pre><pre name="2863" id="2863" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Variable containing:<br> 0.1250  0.1250  0.2500  0.2500<br> 0.1250  0.3750  0.2500  0.2500<br> 0.1250  0.6250  0.2500  0.2500<br> 0.1250  0.8750  0.2500  0.2500<br> 0.3750  0.1250  0.2500  0.2500<br> 0.3750  0.3750  0.2500  0.2500<br> 0.3750  0.6250  0.2500  0.2500<br> 0.3750  0.8750  0.2500  0.2500<br> 0.6250  0.1250  0.2500  0.2500<br> 0.6250  0.3750  0.2500  0.2500<br> 0.6250  0.6250  0.2500  0.2500<br> 0.6250  0.8750  0.2500  0.2500<br> 0.8750  0.1250  0.2500  0.2500<br> 0.8750  0.3750  0.2500  0.2500<br> 0.8750  0.6250  0.2500  0.2500<br> 0.8750  0.8750  0.2500  0.2500<br>[torch.cuda.FloatTensor of size 16x4 (GPU 0)]</em></pre><p name="9f3a" id="9f3a" class="graf graf--p graf-after--pre">Here are the amount of overlap between 3 ground truth objects and 16 anchor boxes:</p><pre name="f523" id="f523" class="graf graf--pre graf-after--p">overlaps = jaccard(bbox.data, anchor_cnr.data)<br>overlaps</pre><pre name="09bc" id="09bc" class="graf graf--pre graf-after--pre">Columns 0 to 7   <br>0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000      </pre><pre name="888b" id="888b" class="graf graf--pre graf-after--pre">Columns 8 to 15   <br>0.0000  0.0091 0.0922  0.0000  0.0000  0.0315  0.3985  0.0000  0.0356  0.0549 0.0103  0.0000  0.2598  0.4538  0.0653  0.0000  0.0000  0.0000 0.0000  0.1897  0.0000  0.0000  0.0000  0.0000 [torch.cuda.FloatTensor of size 3x16 (GPU 0)]</pre><p name="8142" id="8142" class="graf graf--p graf-after--pre">What
 we could do now is we could take the max of dimension 1 (row-wise) 
which will tell us for each ground truth object, what the maximum amount
 that overlaps with some grid cell as well as the index:</p><pre name="197d" id="197d" class="graf graf--pre graf-after--p">overlaps.max(1)</pre><pre name="5e49" id="5e49" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.3985<br>  0.4538<br>  0.1897<br> [torch.cuda.FloatTensor of size 3 (GPU 0)], <br>  14<br>  13<br>  11<br> [torch.cuda.LongTensor of size 3 (GPU 0)])</em></pre><p name="fdce" id="fdce" class="graf graf--p graf-after--pre">We
 will also going to look at max over a dimension 0 (column-wise) which 
will tell us what is the maximum amount of overlap for each grid cell 
across all of the ground truth objects [<a href="https://youtu.be/0frKXR-2PBY?t=59m8s" data-href="https://youtu.be/0frKXR-2PBY?t=59m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:08</a>]:</p><pre name="df2e" id="df2e" class="graf graf--pre graf-after--p">overlaps.max(0)</pre><pre name="757d" id="757d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0356<br>  0.0549<br>  0.0922<br>  0.1897<br>  0.2598<br>  0.4538<br>  0.3985<br>  0.0000<br> [torch.cuda.FloatTensor of size 16 (GPU 0)], <br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  1<br>  1<br>  0<br>  2<br>  1<br>  1<br>  0<br>  0<br> [torch.cuda.LongTensor of size 16 (GPU 0)])</em></pre><p name="5f19" id="5f19" class="graf graf--p graf-after--pre">What
 is particularly interesting here is that it tells us for every grid 
cell what is the index of the ground truth object which overlaps with it
 the most. Zero is a bit overloaded here — zero could either mean the 
amount of overlap was zero or its largest overlap is with object index 
zero. It is going to turn out not to matter but just FYI.</p><p name="d4fd" id="d4fd" class="graf graf--p graf-after--p">There is a function called <code class="markup--code markup--p-code">map_to_ground_truth</code> which we will not worry about for now [<a href="https://youtu.be/0frKXR-2PBY?t=59m57s" data-href="https://youtu.be/0frKXR-2PBY?t=59m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">59:57</a>].
 It is super simple code but it is slightly awkward to think about. 
Basically what it does is it combines these two sets of overlaps in a 
way described in the SSD paper t<span class="markup--quote markup--p-quote is-me" name="87b4c707aca" data-creator-ids="b43c07dd10f6">o assign every anchor box to a ground truth object</span>.
 The way it assign that is each of the three (row-wise max) gets 
assigned as is. For the rest of the anchor boxes, they get assigned to 
anything which they have an overlap of at least 0.5 with (column-wise). 
If neither applies, it is considered to be a cell which contains 
background.</p><pre name="367d" id="367d" class="graf graf--pre graf-after--p">gt_overlap,gt_idx = map_to_ground_truth(overlaps)<br>gt_overlap,gt_idx</pre><pre name="31ef" id="31ef" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0356<br>  0.0549<br>  0.0922<br>  1.9900<br>  0.2598<br>  1.9900<br>  1.9900<br>  0.0000<br> [torch.cuda.FloatTensor of size 16 (GPU 0)], <br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  1<br>  1<br>  0<br>  2<br>  1<br>  1<br>  0<br>  0<br> [torch.cuda.LongTensor of size 16 (GPU 0)])</em></pre><p name="66c7" id="66c7" class="graf graf--p graf-after--pre">Now you can see a list of all the assignments [<a href="https://youtu.be/0frKXR-2PBY?t=1h1m5s" data-href="https://youtu.be/0frKXR-2PBY?t=1h1m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:05</a>]. Anywhere that has <code class="markup--code markup--p-code">gt_overlap &lt; 0.5</code>
 gets assigned background. The three row-wise max anchor box has high 
number to force the assignments. Now we can combine these values to 
classes:</p><pre name="313f" id="313f" class="graf graf--pre graf-after--p">gt_clas = clas[gt_idx]; gt_clas</pre><pre name="cfab" id="cfab" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Variable containing:<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br> 10<br> 10<br>  8<br> 17<br> 10<br> 10<br>  8<br>  8<br>[torch.cuda.LongTensor of size 16 (GPU 0)]</em></pre><p name="73df" id="73df" class="graf graf--p graf-after--pre">Then add a threshold and finally comes up with the three classes that are being predicted:</p><pre name="9817" id="9817" class="graf graf--pre graf-after--p">thresh = 0.5<br>pos = gt_overlap &gt; thresh<br>pos_idx = torch.nonzero(pos)[:,0]<br>neg_idx = torch.nonzero(1-pos)[:,0]<br>pos_idx</pre><pre name="0b8b" id="0b8b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em"> 11<br> 13<br> 14<br>[torch.cuda.LongTensor of size 3 (GPU 0)]</em></pre><p name="3aa2" id="3aa2" class="graf graf--p graf-after--pre">And here are what each of these anchor boxes is meant to be predicting:</p><pre name="3a09" id="3a09" class="graf graf--pre graf-after--p">gt_clas[1-pos] = len(id2cat)<br>[id2cat[o] <strong class="markup--strong markup--pre-strong">if</strong> o&lt;len(id2cat) <strong class="markup--strong markup--pre-strong">else</strong> 'bg' <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> gt_clas.data]</pre><pre name="6a65" id="6a65" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'sofa',<br> 'bg',<br> 'diningtable',<br> 'chair',<br> 'bg']</em></pre><p name="c233" id="c233" class="graf graf--p graf-after--pre">So that was the matching stage [<a href="https://youtu.be/0frKXR-2PBY?t=1h2m29s" data-href="https://youtu.be/0frKXR-2PBY?t=1h2m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:02:29</a>]. For L1 loss, we can:</p><ol class="postList"><li name="1dc4" id="1dc4" class="graf graf--li graf-after--p">take the activations which matched (<code class="markup--code markup--li-code">pos_idx = [11, 13, 14]</code>)</li><li name="9131" id="9131" class="graf graf--li graf-after--li">subtract from those the ground truth bounding boxes</li><li name="9713" id="9713" class="graf graf--li graf-after--li">take the absolute value of the difference</li><li name="7f64" id="7f64" class="graf graf--li graf-after--li">take the mean of that.</li></ol><p name="483c" id="483c" class="graf graf--p graf-after--li">For classifications, we can just do a cross entropy</p><pre name="4c9b" id="4c9b" class="graf graf--pre graf-after--p">gt_bbox = bbox[gt_idx]<br>loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()<br>clas_loss  = F.cross_entropy(b_clasi, gt_clas)<br>loc_loss,clas_loss</pre><pre name="0d14" id="0d14" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(Variable containing:<br> 1.00000e-02 *<br>   6.5887<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  1.0331<br> [torch.cuda.FloatTensor of size 1 (GPU 0)])</em></pre><p name="8150" id="8150" class="graf graf--p graf-after--pre">We
 will end up with 16 predicted bounding boxes, most of them will be 
background. If you are wondering what it predicts in terms of bounding 
box of background, the answer is it totally ignores it.</p><pre name="7536" id="7536" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(3, 4, figsize=(16, 12))<br><strong class="markup--strong markup--pre-strong">for</strong> idx,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>    bbox,clas = get_y(bbox,clas); bbox,clas<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], <br>             b_clas[idx].max(1)[0].sigmoid(), 0.01)<br>plt.tight_layout()</pre><figure name="bcf4" id="bcf4" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 513px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8azTUd1Ujf3FQSMBwIXgAw.png" data-width="1181" data-height="866" data-action="zoom" data-action-value="1*8azTUd1Ujf3FQSMBwIXgAw.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18azTUd1Ujf3FQSMBwIXgAw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="52"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*8azTUd1Ujf3FQSMBwIXgAw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18azTUd1Ujf3FQSMBwIXgAw_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*8azTUd1Ujf3FQSMBwIXgAw.png"></noscript></div></div></figure><h4 name="558e" id="558e" class="graf graf--h4 graf-after--figure">Tweak 1. How do we interpret the activations [<a href="https://youtu.be/0frKXR-2PBY?t=1h4m16s" data-href="https://youtu.be/0frKXR-2PBY?t=1h4m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:04:16</a>]?</h4><p name="cc2a" id="cc2a" class="graf graf--p graf-after--h4">The way we interpret the activation is defined here:</p><pre name="b02b" id="b02b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> actn_to_bb(actn, anchors):<br>    actn_bbs = torch.tanh(actn)<br>    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]<br>    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]<br>    <strong class="markup--strong markup--pre-strong">return</strong> hw2corners(actn_centers, actn_hw)</pre><p name="2a9b" id="2a9b" class="graf graf--p graf-after--pre">We grab the activations, we stick them through <code class="markup--code markup--p-code">tanh</code> (remember <code class="markup--code markup--p-code">tanh</code> is the same shape as sigmoid except it is scaled to be between -1 and 1) which forces it to be within that range. <span class="markup--quote markup--p-quote is-me" name="e30237ec8b2f" data-creator-ids="b43c07dd10f6">We
 then grab the actual position of the anchor boxes, and we will move 
them around according to the value of the activations divided by two</span> (<code class="markup--code markup--p-code">actn_bbs[:,:2]/2</code>).
 In other words, each predicted bounding box can be moved by up to 50% 
of a grid size from where its default position is. Ditto for its height 
and width — it can be up to twice as big or half as big as its default 
size.</p><h4 name="d59f" id="d59f" class="graf graf--h4 graf-after--p">Tweak 2. We actually use binary cross entropy loss instead of cross entropy [<a href="https://youtu.be/0frKXR-2PBY?t=1h5m36s" data-href="https://youtu.be/0frKXR-2PBY?t=1h5m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:05:36</a>]</h4><pre name="960e" id="960e" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BCE_Loss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, num_classes):<br>        super().__init__()<br>        self.num_classes = num_classes<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, pred, targ):<br>        t = one_hot_embedding(targ, self.num_classes+1)<br>        t = V(t[:,:-1].contiguous())<em class="markup--em markup--pre-em">#.cpu()</em><br>        x = pred[:,:-1]<br>        w = self.get_weight(x,t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.binary_cross_entropy_with_logits(x, t, w, <br>                            size_average=<strong class="markup--strong markup--pre-strong">False</strong>)/self.num_classes<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">None</strong></pre><p name="820e" id="820e" class="graf graf--p graf-after--pre">Binary
 cross entropy is what we normally use for multi-label classification. 
Like in the planet satellite competition, each satellite image could 
have multiple things. If it has multiple things in it, you cannot use 
softmax because softmax really encourages just one thing to have the 
high number. In our case, each anchor box can only have one object 
associated with it, so it is not for that reason that we are avoiding 
softmax. It is something else — which is it is possible for an anchor 
box to have nothing associated with it. There are two ways to handle 
this idea of “background”; one would be to say background is just a 
class, so let’s use softmax and just treat background as one of the 
classes that the softmax could predict. A lot of people have done it 
this way. But that is a really hard thing to ask neural network to do [<a href="https://youtu.be/0frKXR-2PBY?t=1h5m52s" data-href="https://youtu.be/0frKXR-2PBY?t=1h5m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:52</a>]
 — it is basically asking whether this grid cell does not have any of 
the 20 objects that I am interested with Jaccard overlap of more than 
0.5. It is a really hard to thing to put into a single computation. On 
the other hand, what if we just asked for each class; “is it a 
motorbike?” “is it a bus?”, “ is it a person?” etc and if all the answer
 is no, consider that background. That is the way we do it here. It is 
not that we can have multiple true labels, but we can have zero.</p><p name="29b4" id="29b4" class="graf graf--p graf-after--p">In <code class="markup--code markup--p-code">forward</code>&nbsp;:</p><ol class="postList"><li name="482a" id="482a" class="graf graf--li graf-after--p">First we take the one hot embedding of the target (at this stage, we do have the idea of background)</li><li name="0e7c" id="0e7c" class="graf graf--li graf-after--li">Then we remove the background column (the last one) which results in a vector either of all zeros or one one.</li><li name="7ba1" id="7ba1" class="graf graf--li graf-after--li">Use binary cross-entropy predictions.</li></ol><p name="6a88" id="6a88" class="graf graf--p graf-after--li">This
 is a minor tweak, but it is the kind of minor tweak that Jeremy wants 
you to think about and understand because it makes a really big 
difference to your training and when there is some increment over a 
previous paper, it would be something like this [<a href="https://youtu.be/0frKXR-2PBY?t=1h8m25s" data-href="https://youtu.be/0frKXR-2PBY?t=1h8m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:25</a>]. It is important to understand what this is doing and more importantly why.</p><p name="9ad6" id="9ad6" class="graf graf--p graf-after--p">So now we have [<a href="https://youtu.be/0frKXR-2PBY?t=1h9m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h9m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:39</a>]:</p><ul class="postList"><li name="3ec1" id="3ec1" class="graf graf--li graf-after--p">A custom loss function</li><li name="d4bf" id="d4bf" class="graf graf--li graf-after--li">A way to calculate Jaccard index</li><li name="882e" id="882e" class="graf graf--li graf-after--li">A way to convert activations to bounding box</li><li name="71d6" id="71d6" class="graf graf--li graf-after--li">A way to map anchor boxes to ground truth</li></ul><p name="9d33" id="9d33" class="graf graf--p graf-after--li">Now all it’s left is SSD loss function.</p><h4 name="5fe0" id="5fe0" class="graf graf--h4 graf-after--p">SSD Loss Function [<a href="https://youtu.be/0frKXR-2PBY?t=1h9m55s" data-href="https://youtu.be/0frKXR-2PBY?t=1h9m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:09:55</a>]</h4><pre name="96a6" id="96a6" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">def</strong> ssd_1_loss(b_c,b_bb,bbox,clas,print_it=<strong class="markup--strong markup--pre-strong">False</strong>):<br>    bbox,clas = get_y(bbox,clas)<br>    a_ic = actn_to_bb(b_bb, anchors)<br>    overlaps = jaccard(bbox.data, anchor_cnr.data)<br>    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)<br>    gt_clas = clas[gt_idx]<br>    pos = gt_overlap &gt; 0.4<br>    pos_idx = torch.nonzero(pos)[:,0]<br>    gt_clas[1-pos] = len(id2cat)<br>    gt_bbox = bbox[gt_idx]<br>    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()<br>    clas_loss  = loss_f(b_c, gt_clas)<br>    <strong class="markup--strong markup--pre-strong">return</strong> loc_loss, clas_loss<br><br><strong class="markup--strong markup--pre-strong">def</strong> ssd_loss(pred,targ,print_it=<strong class="markup--strong markup--pre-strong">False</strong>):<br>    lcs,lls = 0.,0.<br>    <strong class="markup--strong markup--pre-strong">for</strong> b_c,b_bb,bbox,clas <strong class="markup--strong markup--pre-strong">in</strong> zip(*pred,*targ):<br>        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,print_it)<br>        lls += loc_loss<br>        lcs += clas_loss<br>    <strong class="markup--strong markup--pre-strong">if</strong> print_it: print(f'loc: <strong class="markup--strong markup--pre-strong">{lls.data[0]}</strong>, clas: <strong class="markup--strong markup--pre-strong">{lcs.data[0]}</strong>')<br>    <strong class="markup--strong markup--pre-strong">return</strong> lls+lcs</pre><p name="8c47" id="8c47" class="graf graf--p graf-after--pre">The <code class="markup--code markup--p-code">ssd_loss</code> function which is what we set as the criteria, it loops through each image in the mini-batch and call <code class="markup--code markup--p-code">ssd_1_loss</code> function (i.e. SSD loss for one image).</p><p name="98d0" id="98d0" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ssd_1_loss</code> is where it is all happening. It begins by de-structuring <code class="markup--code markup--p-code">bbox</code> and <code class="markup--code markup--p-code">clas</code>. Let’s take a closer look at <code class="markup--code markup--p-code">get_y</code> [<a href="https://youtu.be/0frKXR-2PBY?t=1h10m38s" data-href="https://youtu.be/0frKXR-2PBY?t=1h10m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:10:38</a>]:</p><pre name="b8d2" id="b8d2" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_y(bbox,clas):<br>    bbox = bbox.view(-1,4)/sz<br>    bb_keep = ((bbox[:,2]-bbox[:,0])&gt;0).nonzero()[:,0]<br>    <strong class="markup--strong markup--pre-strong">return</strong> bbox[bb_keep],clas[bb_keep]</pre><p name="d4b5" id="d4b5" class="graf graf--p graf-after--pre">A
 lot of code you find on the internet does not work with mini-batches. 
It only does one thing at a time which we don’t want. In this case, all 
these functions (<code class="markup--code markup--p-code">get_y</code>, <code class="markup--code markup--p-code">actn_to_bb</code>, <code class="markup--code markup--p-code">map_to_ground_truth</code>)
 is working on, not exactly a mini-batch at a time, but a whole bunch of
 ground truth objects at a time. The data loader is being fed a 
mini-batch at a time to do the convolutional layers. Because we can have
 <em class="markup--em markup--p-em">different numbers of ground truth objects in each image</em>
 but a tensor has to be the strict rectangular shape, fastai 
automatically pads it with zeros (any target values that are shorter) [<a href="https://youtu.be/0frKXR-2PBY?t=1h11m8s" data-href="https://youtu.be/0frKXR-2PBY?t=1h11m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:08</a>].
 This was something that was added recently and super handy, but that 
does mean that you then have to make sure that you get rid of those 
zeros. So <code class="markup--code markup--p-code">get_y</code> gets rid of any of the bounding boxes that are just padding.</p><ol class="postList"><li name="f229" id="f229" class="graf graf--li graf-after--p">Get rid of the padding</li><li name="0e2d" id="0e2d" class="graf graf--li graf-after--li">Turn the activations to bounding boxes</li><li name="953a" id="953a" class="graf graf--li graf-after--li">Do the Jaccard</li><li name="fa65" id="fa65" class="graf graf--li graf-after--li">Do map_to_ground_truth</li><li name="26b9" id="26b9" class="graf graf--li graf-after--li">Check that there is an overlap greater than something around 0.4~0.5 (different papers use different values for this)</li><li name="aa5a" id="aa5a" class="graf graf--li graf-after--li">Find the indices of things that matched</li><li name="bad6" id="bad6" class="graf graf--li graf-after--li">Assign background class for the ones that did not match</li><li name="a55d" id="a55d" class="graf graf--li graf-after--li">Then
 finally get L1 loss for the localization, binary cross entropy loss for
 the classification, and return them which gets added in <code class="markup--code markup--li-code">ssd_loss</code></li></ol><h4 name="b35e" id="b35e" class="graf graf--h4 graf-after--li">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h12m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h12m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:12:47</a>]</h4><pre name="0f37" id="0f37" class="graf graf--pre graf-after--h4">learn.crit = ssd_loss<br>lr = 3e-3<br>lrs = np.array([lr/100,lr/10,lr])</pre><pre name="8eb3" id="8eb3" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(1)</pre><pre name="5ef8" id="5ef8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      44.232681  21476.816406</em></pre><figure name="9862" id="9862" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 402px; max-height: 270px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*V8J7FkreIVG7tKxGQQRV2Q.png" data-width="402" data-height="270" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1V8J7FkreIVG7tKxGQQRV2Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*V8J7FkreIVG7tKxGQQRV2Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1V8J7FkreIVG7tKxGQQRV2Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*V8J7FkreIVG7tKxGQQRV2Q.png"></noscript></div></div></figure><pre name="c8ad" id="c8ad" class="graf graf--pre graf-after--figure">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(1)</pre><pre name="6db9" id="6db9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      86.852668  32587.789062</em></pre><figure name="939e" id="939e" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 396px; max-height: 270px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*-q583mkIy-e3k6dz5HmkYw.png" data-width="396" data-height="270" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1-q583mkIy-e3k6dz5HmkYw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*-q583mkIy-e3k6dz5HmkYw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1-q583mkIy-e3k6dz5HmkYw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*-q583mkIy-e3k6dz5HmkYw.png"></noscript></div></div></figure><pre name="b198" id="b198" class="graf graf--pre graf-after--figure">learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))</pre><pre name="e101" id="e101" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      45.570843  37.099854 <br>    1      37.165911  32.165031                           <br>    2      33.27844   30.990122                           <br>    3      31.12054   29.804482                           <br>    4      29.305789  28.943184</em></pre><pre name="3fb3" id="3fb3" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[28.943184]</em></pre><pre name="f2a9" id="f2a9" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))</pre><pre name="e4ee" id="e4ee" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      43.726979  33.803085 <br>    1      34.771754  29.012939                           <br>    2      30.591864  27.132868                           <br>    3      27.896905  26.151638                           <br>    4      25.907382  25.739273</em></pre><pre name="e44e" id="e44e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[25.739273]</em></pre><pre name="b505" id="b505" class="graf graf--pre graf-after--pre">learn.save('0')</pre><pre name="e878" id="e878" class="graf graf--pre graf-after--pre">learn.load('0')</pre><h4 name="6949" id="6949" class="graf graf--h4 graf-after--pre">Result [<a href="https://youtu.be/0frKXR-2PBY?t=1h13m16s" data-href="https://youtu.be/0frKXR-2PBY?t=1h13m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:13:16</a>]</h4><figure name="3657" id="3657" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 513px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*8azTUd1Ujf3FQSMBwIXgAw.png" data-width="1181" data-height="866" data-action="zoom" data-action-value="1*8azTUd1Ujf3FQSMBwIXgAw.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18azTUd1Ujf3FQSMBwIXgAw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="52"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*8azTUd1Ujf3FQSMBwIXgAw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/18azTUd1Ujf3FQSMBwIXgAw_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*8azTUd1Ujf3FQSMBwIXgAw.png"></noscript></div></div></figure><p name="efec" id="efec" class="graf graf--p graf-after--figure">In
 practice, we want to remove the background and also add some threshold 
for probabilities, but it is on the right track. The potted plant image,
 the result is not surprising as all of our anchor boxes were small (4x4
 grid). To go from here to something that is going to be more accurate, 
all we are going to do is to create way more anchor boxes.</p><p name="c496" id="c496" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: For the multi-label classification, why aren’t we multiplying the categorical loss by a constant like we did before [<a href="https://youtu.be/0frKXR-2PBY?t=1h15m20s" data-href="https://youtu.be/0frKXR-2PBY?t=1h15m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:15:20</a>]? Great question. It is because later on it will turn out we do not need to.</p><h4 name="5ece" id="5ece" class="graf graf--h4 graf-after--p">More anchors! [<a href="https://youtu.be/0frKXR-2PBY?t=1h14m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h14m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:14:47</a>]</h4><p name="3906" id="3906" class="graf graf--p graf-after--h4">There are 3 ways to do this:</p><ol class="postList"><li name="9e13" id="9e13" class="graf graf--li graf-after--p">Create anchor boxes of different sizes (zoom):</li></ol></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="3"><figure name="ce4b" id="ce4b" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--li" style="width: 33.668%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 99.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*OtrTSJqBXyjeypKehik1CQ.png" data-width="457" data-height="454" data-action="zoom" data-action-value="1*OtrTSJqBXyjeypKehik1CQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OtrTSJqBXyjeypKehik1CQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="72"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*OtrTSJqBXyjeypKehik1CQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OtrTSJqBXyjeypKehik1CQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*OtrTSJqBXyjeypKehik1CQ.png"></noscript></div></div></figure><figure name="b8d9" id="b8d9" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.133%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.89999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*YG5bCP3O-jVhaQX_wuiSSg.png" data-width="428" data-height="432" data-action="zoom" data-action-value="1*YG5bCP3O-jVhaQX_wuiSSg.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1YG5bCP3O-jVhaQX_wuiSSg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*YG5bCP3O-jVhaQX_wuiSSg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1YG5bCP3O-jVhaQX_wuiSSg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*YG5bCP3O-jVhaQX_wuiSSg.png"></noscript></div></div></figure><figure name="02ff" id="02ff" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.2%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.69999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*QCo0wOgJKXDBYNlmE7zUmA.png" data-width="414" data-height="417" data-action="zoom" data-action-value="1*QCo0wOgJKXDBYNlmE7zUmA.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1QCo0wOgJKXDBYNlmE7zUmA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*QCo0wOgJKXDBYNlmE7zUmA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1QCo0wOgJKXDBYNlmE7zUmA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*QCo0wOgJKXDBYNlmE7zUmA.png"></noscript></div></div><figcaption class="imageCaption" style="width: 301.205%; left: -201.205%;">From left (1x1, 2x2, 4x4 grids of anchor boxes). Notice that some of the anchor box is bigger than the original&nbsp;image.</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="225d" id="225d" class="graf graf--p graf-after--figure">2. Create anchor boxes of different aspect ratios:</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="3"><figure name="fdae" id="fdae" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 33.333%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*ko8vZK4RD8H2l4u1hXCQZQ.png" data-width="401" data-height="401" data-action="zoom" data-action-value="1*ko8vZK4RD8H2l4u1hXCQZQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1ko8vZK4RD8H2l4u1hXCQZQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*ko8vZK4RD8H2l4u1hXCQZQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1ko8vZK4RD8H2l4u1hXCQZQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*ko8vZK4RD8H2l4u1hXCQZQ.png"></noscript></div></div></figure><figure name="0b2d" id="0b2d" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.1%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.69999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*3rvuvY6Fu2S6eoN3nK1QWg.png" data-width="401" data-height="404" data-action="zoom" data-action-value="1*3rvuvY6Fu2S6eoN3nK1QWg.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/13rvuvY6Fu2S6eoN3nK1QWg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*3rvuvY6Fu2S6eoN3nK1QWg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/13rvuvY6Fu2S6eoN3nK1QWg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*3rvuvY6Fu2S6eoN3nK1QWg.png"></noscript></div></div></figure><figure name="508b" id="508b" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.567%;" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 99.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*bWZwFqf2Bv-ZbW-KedNO0Q.png" data-width="404" data-height="401" data-action="zoom" data-action-value="1*bWZwFqf2Bv-ZbW-KedNO0Q.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1bWZwFqf2Bv-ZbW-KedNO0Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="72"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/400/1*bWZwFqf2Bv-ZbW-KedNO0Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1bWZwFqf2Bv-ZbW-KedNO0Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/400/1*bWZwFqf2Bv-ZbW-KedNO0Q.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="8881" id="8881" class="graf graf--p graf-after--figure">3.
 Use more convolutional layers as sources of anchor boxes (the boxes are
 randomly jittered so that we can see ones that are overlapping [<a href="https://youtu.be/0frKXR-2PBY?t=1h16m28s" data-href="https://youtu.be/0frKXR-2PBY?t=1h16m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:28</a>]):</p><figure name="f455" id="f455" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 402px; max-height: 403px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*LwFOFtmawmpqp6VDc56RmA.png" data-width="402" data-height="403" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1LwFOFtmawmpqp6VDc56RmA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*LwFOFtmawmpqp6VDc56RmA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1LwFOFtmawmpqp6VDc56RmA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*LwFOFtmawmpqp6VDc56RmA.png"></noscript></div></div></figure><p name="305a" id="305a" class="graf graf--p graf-after--figure">Combining these approaches, you can create lots of anchor boxes (Jeremy said he wouldn’t print it, but here it is):</p><figure name="fd17" id="fd17" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 449px; max-height: 459px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 102.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*ymt8L0CCKMd9SG82SemdIA.png" data-width="449" data-height="459" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1ymt8L0CCKMd9SG82SemdIA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*ymt8L0CCKMd9SG82SemdIA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1ymt8L0CCKMd9SG82SemdIA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*ymt8L0CCKMd9SG82SemdIA.png"></noscript></div></div></figure><pre name="14d2" id="14d2" class="graf graf--pre graf-after--figure">anc_grids = [4, 2, 1]<br>anc_zooms = [0.75, 1., 1.3]<br>anc_ratios = [(1., 1.), (1., 0.5), (0.5, 1.)]<br><br>anchor_scales = [(anz*i,anz*j) <strong class="markup--strong markup--pre-strong">for</strong> anz <strong class="markup--strong markup--pre-strong">in</strong> anc_zooms <br>                                    <strong class="markup--strong markup--pre-strong">for</strong> (i,j) <strong class="markup--strong markup--pre-strong">in</strong> anc_ratios]<br>k = len(anchor_scales)<br>anc_offsets = [1/(o*2) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> anc_grids]</pre><pre name="2e8f" id="2e8f" class="graf graf--pre graf-after--pre">anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)<br>                        <strong class="markup--strong markup--pre-strong">for</strong> ao,ag <strong class="markup--strong markup--pre-strong">in</strong> zip(anc_offsets,anc_grids)])<br>anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)<br>                        <strong class="markup--strong markup--pre-strong">for</strong> ao,ag <strong class="markup--strong markup--pre-strong">in</strong> zip(anc_offsets,anc_grids)])<br>anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)</pre><pre name="908b" id="908b" class="graf graf--pre graf-after--pre">anc_sizes = np.concatenate([np.array([[o/ag,p/ag] <br>              <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(ag*ag) <strong class="markup--strong markup--pre-strong">for</strong> o,p <strong class="markup--strong markup--pre-strong">in</strong> anchor_scales])<br>                 <strong class="markup--strong markup--pre-strong">for</strong> ag <strong class="markup--strong markup--pre-strong">in</strong> anc_grids])<br>grid_sizes = V(np.concatenate([np.array([ 1/ag <br>              <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(ag*ag) <strong class="markup--strong markup--pre-strong">for</strong> o,p <strong class="markup--strong markup--pre-strong">in</strong> anchor_scales])<br>                  <strong class="markup--strong markup--pre-strong">for</strong> ag <strong class="markup--strong markup--pre-strong">in</strong> anc_grids]), <br>                      requires_grad=<strong class="markup--strong markup--pre-strong">False</strong>).unsqueeze(1)<br>anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), <br>              requires_grad=<strong class="markup--strong markup--pre-strong">False</strong>).float()<br>anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])</pre><p name="43cc" id="43cc" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">anchors</code>&nbsp;: middle and height, width</p><p name="1b94" id="1b94" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">anchor_cnr</code>&nbsp;: top left and bottom right corners</p><h4 name="ce3e" id="ce3e" class="graf graf--h4 graf-after--p">Review of key concept [<a href="https://youtu.be/0frKXR-2PBY?t=1h18m" data-href="https://youtu.be/0frKXR-2PBY?t=1h18m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:18:00</a>]</h4><figure name="a3db" id="a3db" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 204px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*C67J9RhTAiz9MCD-ebpp_w.png" data-width="1746" data-height="509" data-action="zoom" data-action-value="1*C67J9RhTAiz9MCD-ebpp_w.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1C67J9RhTAiz9MCD-ebpp_w_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="20"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*C67J9RhTAiz9MCD-ebpp_w.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1C67J9RhTAiz9MCD-ebpp_w.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*C67J9RhTAiz9MCD-ebpp_w.png"></noscript></div></div></figure><ul class="postList"><li name="cdc9" id="cdc9" class="graf graf--li graf-after--figure">We have a vector of ground truth (sets of 4 bounding box coordinates and a class)</li><li name="eb81" id="eb81" class="graf graf--li graf-after--li">We have a neural net that takes some input and spits out some output activations</li><li name="1fb5" id="1fb5" class="graf graf--li graf-after--li">Compare
 the activations and the ground truth, calculate a loss, find the 
derivative of that, and adjust weights according to the derivative times
 a learning rate.</li><li name="90af" id="90af" class="graf graf--li graf-after--li">We
 need a loss function that can take ground truth and activation and spit
 out a number that says how good these activations are. To do this, we 
need to take each one of <code class="markup--code markup--li-code">m</code> ground truth objects and decide which set of <code class="markup--code markup--li-code">(4+c)</code> activations is responsible for that object [<a href="https://youtu.be/0frKXR-2PBY?t=1h21m58s" data-href="https://youtu.be/0frKXR-2PBY?t=1h21m58s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:21:58</a>] — which one we should be comparing to decide whether the class is correct and bounding box is close or not (matching problem).</li><li name="48c8" id="48c8" class="graf graf--li graf-after--li">Since we are using SSD approach, so it is not arbitrary which ones we match up [<a href="https://youtu.be/0frKXR-2PBY?t=1h23m18s" data-href="https://youtu.be/0frKXR-2PBY?t=1h23m18s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:23:18</a>]. We want to match up the set of activations whose receptive field has the maximum density from where the real object is.</li><li name="c1a1" id="c1a1" class="graf graf--li graf-after--li">The
 loss function needs to be some consistent task. If in the first image, 
the top left object corresponds with the first 4+c activations, and in 
the second image, we threw things around and suddenly it’s now going 
with the last 4+c activations, the neural net doesn’t know what to 
learn.</li><li name="3723" id="3723" class="graf graf--li graf-after--li">Once matching problem is resolved, the rest is just the same as the single object detection.</li></ul><p name="7654" id="7654" class="graf graf--p graf-after--li">Architectures:</p><ul class="postList"><li name="2c45" id="2c45" class="graf graf--li graf-after--p">YOLO — the last layer is fully connected (no concept of geometry)</li><li name="639f" id="639f" class="graf graf--li graf-after--li">SSD — the last layer is convolutional</li></ul><h4 name="db54" id="db54" class="graf graf--h4 graf-after--li">k (zooms x ratios)[<a href="https://youtu.be/0frKXR-2PBY?t=1h29m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h29m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:29:39</a>]</h4><p name="c68a" id="c68a" class="graf graf--p graf-after--h4">For
 every grid cell which can be different sizes, we can have different 
orientations and zooms representing different anchor boxes which are 
just like conceptual ideas that every one of anchor boxes is associated 
with one set of <code class="markup--code markup--p-code">4+c</code> activations in our model. So however many anchor boxes we have, we need to have that times <code class="markup--code markup--p-code">(4+c)</code>
 activations. That does not mean that each convolutional layer needs 
that many activations. Because 4x4 convolutional layer already has 16 
sets of activations, the 2x2 layer has 4 sets of activations, and 
finally 1x1 has one set. So we basically get 1 + 4 + 16 for free. So we 
only needs to know <code class="markup--code markup--p-code">k</code> where <code class="markup--code markup--p-code">k</code> is the number of zooms by the number of aspect ratios. Where else, the grids, we will get for free through our architecture.</p><h4 name="ca16" id="ca16" class="graf graf--h4 graf-after--p">Model Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=1h31m10s" data-href="https://youtu.be/0frKXR-2PBY?t=1h31m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:10</a>]</h4><pre name="4a50" id="4a50" class="graf graf--pre graf-after--h4">drop=0.4<br><br><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SSD_MultiHead</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, bias):<br>        super().__init__()<br>        self.drop = nn.Dropout(drop)<br>        self.sconv0 = StdConv(512,256, stride=1, drop=drop)<br>        self.sconv1 = StdConv(256,256, drop=drop)<br>        self.sconv2 = StdConv(256,256, drop=drop)<br>        self.sconv3 = StdConv(256,256, drop=drop)<br>        self.out1 = OutConv(k, 256, bias)<br>        self.out2 = OutConv(k, 256, bias)<br>        self.out3 = OutConv(k, 256, bias)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.drop(F.relu(x))<br>        x = self.sconv0(x)<br>        x = self.sconv1(x)<br>        o1c,o1l = self.out1(x)<br>        x = self.sconv2(x)<br>        o2c,o2l = self.out2(x)<br>        x = self.sconv3(x)<br>        o3c,o3l = self.out3(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> [torch.cat([o1c,o2c,o3c], dim=1),<br>                torch.cat([o1l,o2l,o3l], dim=1)]<br><br>head_reg4 = SSD_MultiHead(k, -4.)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><p name="5fd9" id="5fd9" class="graf graf--p graf-after--pre">The
 model is nearly identical to what we had before. But we have a number 
of stride 2 convolutions which is going to take us through to 4x4, 2x2, 
and 1x1 (each stride 2 convolution halves our grid size in both 
directions).</p><ul class="postList"><li name="5dc6" id="5dc6" class="graf graf--li graf-after--p">After
 we do our first convolution to get to 4x4, we will grab a set of 
outputs from that because we want to save away the 4x4 anchors.</li><li name="16d7" id="16d7" class="graf graf--li graf-after--li">Once we get to 2x2, we grab another set of now 2x2 anchors</li><li name="f368" id="f368" class="graf graf--li graf-after--li">Then finally we get to 1x1</li><li name="afc1" id="afc1" class="graf graf--li graf-after--li">We then concatenate them all together, which gives us the correct number of activations (one activation for every anchor box).</li></ul><h4 name="5984" id="5984" class="graf graf--h4 graf-after--li">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h32m50s" data-href="https://youtu.be/0frKXR-2PBY?t=1h32m50s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:32:50</a>]</h4><pre name="676d" id="676d" class="graf graf--pre graf-after--h4">learn.crit = ssd_loss<br>lr = 1e-2<br>lrs = np.array([lr/100,lr/10,lr])</pre><pre name="1dca" id="1dca" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(n_skip_end=2)</pre><figure name="1150" id="1150" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 402px; max-height: 270px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*jB_OxbaTmMXHbkeXE4G0SQ.png" data-width="402" data-height="270" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1jB_OxbaTmMXHbkeXE4G0SQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*jB_OxbaTmMXHbkeXE4G0SQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1jB_OxbaTmMXHbkeXE4G0SQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*jB_OxbaTmMXHbkeXE4G0SQ.png"></noscript></div></div></figure><pre name="a580" id="a580" class="graf graf--pre graf-after--figure">learn.fit(lrs, 1, cycle_len=4, use_clr=(20,8))</pre><pre name="0d46" id="0d46" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      15.124349  15.015433 <br>    1      13.091956  10.39855                            <br>    2      11.643629  9.4289                              <br>    3      10.532467  8.822998</em></pre><pre name="296f" id="296f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[8.822998]</em></pre><pre name="7b9f" id="7b9f" class="graf graf--pre graf-after--pre">learn.save('tmp')</pre><pre name="7493" id="7493" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)<br>learn.fit(lrs/2, 1, cycle_len=4, use_clr=(20,8))</pre><pre name="98c2" id="98c2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      9.821056   10.335152 <br>    1      9.419633   11.834093                           <br>    2      8.78818    7.907762                            <br>    3      8.219976   7.456364</em></pre><pre name="727f" id="727f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[7.4563637]</em></pre><pre name="2c78" id="2c78" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>y = V(y)<br>batch = learn.model(V(x))<br>b_clas,b_bb = batch<br>x = to_np(x)<br><br>fig, axes = plt.subplots(3, 4, figsize=(16, 12))<br><strong class="markup--strong markup--pre-strong">for</strong> idx,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.ds.denorm(x)[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], <br>             b_clas[idx].max(1)[0].sigmoid(), <strong class="markup--strong markup--pre-strong">0.2</strong>)<br>plt.tight_layout()</pre><p name="bac0" id="bac0" class="graf graf--p graf-after--pre">Here, we printed out those detections with at least probability of <code class="markup--code markup--p-code">0.2</code>&nbsp;. Some of them look pretty hopeful but others not so much.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="bc4f" id="bc4f" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 728px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 72.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*l168j5d3fWBZLST3XLPD6A.png" data-width="1178" data-height="858" data-action="zoom" data-action-value="1*l168j5d3fWBZLST3XLPD6A.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1l168j5d3fWBZLST3XLPD6A.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="52"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*l168j5d3fWBZLST3XLPD6A.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1l168j5d3fWBZLST3XLPD6A_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*l168j5d3fWBZLST3XLPD6A.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><h3 name="acc2" id="acc2" class="graf graf--h3 graf-after--figure">History of object detection [<a href="https://youtu.be/0frKXR-2PBY?t=1h33m43s" data-href="https://youtu.be/0frKXR-2PBY?t=1h33m43s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:33:43</a>]</h3><figure name="16b5" id="16b5" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 371px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 53%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*bQPvoI0soxtlBt1cEZlzcQ.png" data-width="1706" data-height="904" data-action="zoom" data-action-value="1*bQPvoI0soxtlBt1cEZlzcQ.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1bQPvoI0soxtlBt1cEZlzcQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="37"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*bQPvoI0soxtlBt1cEZlzcQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1bQPvoI0soxtlBt1cEZlzcQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*bQPvoI0soxtlBt1cEZlzcQ.png"></noscript></div></div></figure><p name="8ece" id="8ece" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/1312.2249" data-href="https://arxiv.org/abs/1312.2249" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Scalable Object Detection using Deep Neural Networks</a></p><ul class="postList"><li name="c5f6" id="c5f6" class="graf graf--li graf-after--p">When people refer to the multi-box method, they are talking about this paper.</li><li name="1caf" id="1caf" class="graf graf--li graf-after--li">This
 was the paper that came up with the idea that we can have a loss 
function that has this matching process and then you can use that to do 
object detection. So everything since that time has been trying to 
figure out how to make this better.</li></ul><p name="29bb" id="29bb" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1506.01497" data-href="https://arxiv.org/abs/1506.01497" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><ul class="postList"><li name="3858" id="3858" class="graf graf--li graf-after--p">In
 parallel, Ross Girshick was going down a totally different direction. 
He had these two-stage process where the first stage used the classical 
computer vision approaches to find edges and changes of gradients to 
guess which parts of the image may represent distinct objects. Then fit 
each of those into a convolutional neural network which was basically 
designed to figure out if that is the kind of object we are interested 
in.</li><li name="9e50" id="9e50" class="graf graf--li graf-after--li">R-CNN and Fast R-CNN are hybrid of traditional computer vision and deep learning.</li><li name="3b77" id="3b77" class="graf graf--li graf-after--li">What
 Ross and his team then did was they took the multibox idea and replaced
 the traditional non-deep learning computer vision part of their two 
stage process with the conv net. So now they have two conv nets: one for
 region proposals (all of the things that might be objects) and the 
second part was the same as his earlier work.</li></ul><p name="9a15" id="9a15" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1506.02640" data-href="https://arxiv.org/abs/1506.02640" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">You Only Look Once: Unified, Real-Time Object Detection</a></p><p name="cb78" id="cb78" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1512.02325" data-href="https://arxiv.org/abs/1512.02325" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SSD: Single Shot MultiBox Detector</a></p><ul class="postList"><li name="9e7d" id="9e7d" class="graf graf--li graf-after--p">At
 similar time these paper came out. Both of these did something pretty 
cool which is they achieved similar performance as the Faster R-CNN but 
with 1 stage.</li><li name="ea38" id="ea38" class="graf graf--li graf-after--li">They
 took the multibox idea and they tried to figure out how to deal with 
messy outputs. The basic ideas were to use, for example, hard negative 
mining where they would go through and find all of the matches that did 
not look that good and throw them away, use very tricky and complex data
 augmentation methods, and all kind of hackery. But they got them to 
work pretty well.</li></ul><p name="a3f4" id="a3f4" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1708.02002" data-href="https://arxiv.org/abs/1708.02002" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Focal Loss for Dense Object Detection</a> (RetinaNet)</p><ul class="postList"><li name="e6d1" id="e6d1" class="graf graf--li graf-after--p">Then something really cool happened late last year which is this thing called focal loss.</li><li name="883d" id="883d" class="graf graf--li graf-after--li">They
 actually realized why this messy thing wasn’t working. When we look at 
an image, there are 3 different granularities of convolutional grid 
(4x4, 2x2, 1x1) [<a href="https://youtu.be/0frKXR-2PBY?t=1h37m28s" data-href="https://youtu.be/0frKXR-2PBY?t=1h37m28s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:37:28</a>].
 The 1x1 is quite likely to have a reasonable overlap with some object 
because most photos have some kind of main subject. On the other hand, 
in the 4x4 grid cells, the most of 16 anchor boxes are not going to have
 a much of an overlap with anything. So if somebody was to say to you 
“$20 bet, what do you reckon this little clip is?” and you are not sure,
 you will say “background” because most of the time, it is the 
background.</li></ul><p name="05da" id="05da" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>:
 I understand why we have a 4x4 grid of receptive fields with 1 anchor 
box each to coarsely localize objects in the image. But what I think I’m
 missing is why we need multiple receptive fields at different sizes. 
The first version already included 16 receptive fields, each with a 
single anchor box associated. With the additions, there are now many 
more anchor boxes to consider. Is this because you constrained how much a
 receptive field could move or scale from its original size? Or is there
 another reason? [<a href="https://youtu.be/0frKXR-2PBY?t=1h38m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h38m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:38:47</a>]
 It is kind of backwards. The reason Jeremy did the constraining was 
because he knew he was going to be adding more boxes later. But really, 
the reason is that the Jaccard overlap between one of those 4x4 grid 
cells and a picture where a single object that takes up most of the 
image is never going to be 0.5. The intersection is much smaller than 
the union because the object is too big. So for this general idea to 
work where we are saying you are responsible for something that you have
 better than 50% overlap with, we need anchor boxes which will on a 
regular basis have a 50% or higher overlap which means we need to have a
 variety of sizes, shapes, and scales. This all happens in the loss 
function. The vast majority of the interesting stuff in all of the 
object detection is the loss function.</p><h4 name="bfb0" id="bfb0" class="graf graf--h4 graf-after--p">Focal Loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h40m38s" data-href="https://youtu.be/0frKXR-2PBY?t=1h40m38s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:40:38</a>]</h4><figure name="aa98" id="aa98" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 598px; max-height: 341px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.99999999999999%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*6Bood7G6dUuhigy9cxkZ-Q.png" data-width="598" data-height="341" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16Bood7G6dUuhigy9cxkZ-Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="42"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*6Bood7G6dUuhigy9cxkZ-Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16Bood7G6dUuhigy9cxkZ-Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*6Bood7G6dUuhigy9cxkZ-Q.png"></noscript></div></div></figure><p name="7caf" id="7caf" class="graf graf--p graf-after--figure">The key thing is this very first picture. The blue line is the binary cross entropy loss. If the answer is not a motorbike [<a href="https://youtu.be/0frKXR-2PBY?t=1h41m46s" data-href="https://youtu.be/0frKXR-2PBY?t=1h41m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:46</a>],
 and I said “I think it’s not a motorbike and I am 60% sure” with the 
blue line, the loss is still about 0.5 which is pretty bad. So if we 
want to get our loss down, then for all these things which are actually 
back ground, we have to be saying “I am sure that is background”, “I am 
sure it’s not a motorbike, or a bus, or a person” — because if I don’t 
say we are sure it is not any of these things, then we still get loss.</p><p name="75ba" id="75ba" class="graf graf--p graf-after--p">That is why the motorbike example did not work [<a href="https://youtu.be/0frKXR-2PBY?t=1h42m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h42m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:39</a>].
 Because even when it gets to lower right corner and it wants to say “I 
think it’s a motorbike”, there is no payoff for it to say so. If it is 
wrong, it gets killed. And the vast majority of the time, it is 
background. Even if it is not background, it is not enough just to say 
“it’s not background” — you have to say which of the 20 things it is.</p><p name="0405" id="0405" class="graf graf--p graf-after--p">So the trick is to trying to find a different loss function [<a href="https://youtu.be/0frKXR-2PBY?t=1h44m" data-href="https://youtu.be/0frKXR-2PBY?t=1h44m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:00</a>]
 that looks more like the purple line. Focal loss is literally just a 
scaled cross entropy loss. Now if we say “I’m&nbsp;.6 sure it’s not a 
motorbike” then the loss function will say “good for you! no worries” [<a href="https://youtu.be/0frKXR-2PBY?t=1h44m42s" data-href="https://youtu.be/0frKXR-2PBY?t=1h44m42s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:42</a>].</p><p name="e20f" id="e20f" class="graf graf--p graf-after--p">The actual contribution of this paper is to add <code class="markup--code markup--p-code">(1 − pt)^γ</code> to the start of the equation [<a href="https://youtu.be/0frKXR-2PBY?t=1h45m6s" data-href="https://youtu.be/0frKXR-2PBY?t=1h45m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:06</a>]
 which sounds like nothing but actually people have been trying to 
figure out this problem for years. When you come across a paper like 
this which is game-changing, you shouldn’t assume you are going to have 
to write thousands of lines of code. Very often it is one line of code, 
or the change of a single constant, or adding log to a single place.</p><p name="3d32" id="3d32" class="graf graf--p graf-after--p">A couple of terrific things about this paper [<a href="https://youtu.be/0frKXR-2PBY?t=1h46m8s" data-href="https://youtu.be/0frKXR-2PBY?t=1h46m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:46:08</a>]:</p><ul class="postList"><li name="4dce" id="4dce" class="graf graf--li graf-after--p">Equations are written in a simple manner</li><li name="b4f3" id="b4f3" class="graf graf--li graf-after--li">They “refactor”</li></ul><h4 name="021e" id="021e" class="graf graf--h4 graf-after--li">Implementing Focal Loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h49m27s" data-href="https://youtu.be/0frKXR-2PBY?t=1h49m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:49:27</a>]:</h4><figure name="a6b9" id="a6b9" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 429px; max-height: 36px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 8.4%;"></div><img class="graf-image" data-image-id="1*wIp0HYEWPnkiuxLeCfEiAg.png" data-width="429" data-height="36" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1wIp0HYEWPnkiuxLeCfEiAg.png"></div></figure><p name="0a90" id="0a90" class="graf graf--p graf-after--figure">Remember,
 -log(pt) is the cross entropy loss and focal loss is just a scaled 
version. When we defined the binomial cross entropy loss, you may have 
noticed that there was a weight which by default was none:</p><pre name="2c64" id="2c64" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BCE_Loss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, num_classes):<br>        super().__init__()<br>        self.num_classes = num_classes<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, pred, targ):<br>        t = one_hot_embedding(targ, self.num_classes+1)<br>        t = V(t[:,:-1].contiguous())<em class="markup--em markup--pre-em">#.cpu()</em><br>        x = pred[:,:-1]<br>        w = self.get_weight(x,t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.binary_cross_entropy_with_logits(x, t, w, <br>                          size_average=<strong class="markup--strong markup--pre-strong">False</strong>)/self.num_classes<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">None</strong></pre><p name="0e89" id="0e89" class="graf graf--p graf-after--pre">When you call <code class="markup--code markup--p-code">F.binary_cross_entropy_with_logits</code>, you can pass in the weight. Since we just wanted to multiply a cross entropy by something, we can just define <code class="markup--code markup--p-code">get_weight</code>. Here is the entirety of focal loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h50m23s" data-href="https://youtu.be/0frKXR-2PBY?t=1h50m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:23</a>]:</p><pre name="1cbf" id="1cbf" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">FocalLoss</strong>(BCE_Loss):<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t):<br>        alpha,gamma = 0.25,2.<br>        p = x.sigmoid()<br>        pt = p*t + (1-p)*(1-t)<br>        w = alpha*t + (1-alpha)*(1-t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> w * (1-pt).pow(gamma)</pre><p name="9651" id="9651" class="graf graf--p graf-after--pre">If
 you were wondering why alpha and gamma are 0.25 and 2, here is another 
excellent thing about this paper, because they tried lots of different 
values and found that these work well:</p><figure name="9bed" id="9bed" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 319px; max-height: 232px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 72.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*qFPRvFHQMQplSJGp3QLiNA.png" data-width="319" data-height="232" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1qFPRvFHQMQplSJGp3QLiNA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="52"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*qFPRvFHQMQplSJGp3QLiNA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1qFPRvFHQMQplSJGp3QLiNA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*qFPRvFHQMQplSJGp3QLiNA.png"></noscript></div></div></figure><h4 name="6ed2" id="6ed2" class="graf graf--h4 graf-after--figure">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h51m25s" data-href="https://youtu.be/0frKXR-2PBY?t=1h51m25s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:51:25</a>]</h4><pre name="ebda" id="ebda" class="graf graf--pre graf-after--h4">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(n_skip_end=2)</pre><figure name="5e64" id="5e64" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 396px; max-height: 270px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*lQPSR3V2IXbxOpcgNE-U-Q.png" data-width="396" data-height="270" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1lQPSR3V2IXbxOpcgNE-U-Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*lQPSR3V2IXbxOpcgNE-U-Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1lQPSR3V2IXbxOpcgNE-U-Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*lQPSR3V2IXbxOpcgNE-U-Q.png"></noscript></div></div></figure><pre name="cf7e" id="cf7e" class="graf graf--pre graf-after--figure">learn.fit(lrs, 1, cycle_len=10, use_clr=(20,10))</pre><pre name="a909" id="a909" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      24.263046  28.975235 <br>    1      20.459562  16.362392                           <br>    2      17.880827  14.884829                           <br>    3      15.956896  13.676485                           <br>    4      14.521345  13.134197                           <br>    5      13.460941  12.594139                           <br>    6      12.651842  12.069849                           <br>    7      11.944972  11.956457                           <br>    8      11.385798  11.561226                           <br>    9      10.988802  11.362164</em></pre><pre name="f3db" id="f3db" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[11.362164]</em></pre><pre name="08b0" id="08b0" class="graf graf--pre graf-after--pre">learn.save('fl0')<br>learn.load('fl0')</pre><pre name="5836" id="5836" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)<br>learn.fit(lrs/4, 1, cycle_len=10, use_clr=(20,10))</pre><pre name="911e" id="911e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      10.871668  11.615532 <br>    1      10.908461  11.604334                           <br>    2      10.549796  11.486127                           <br>    3      10.130961  11.088478                           <br>    4      9.70691    10.72144                            <br>    5      9.319202   10.600481                           <br>    6      8.916653   10.358334                           <br>    7      8.579452   10.624706                           <br>    8      8.274838   10.163422                           <br>    9      7.994316   10.108068</em></pre><pre name="44c8" id="44c8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[10.108068]</em></pre><pre name="268f" id="268f" class="graf graf--pre graf-after--pre">learn.save('drop4')<br>learn.load('drop4')</pre><pre name="eed6" id="eed6" class="graf graf--pre graf-after--pre">plot_results(0.75)</pre><figure name="3481" id="3481" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 524px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*G4HCc1mpkvHFqbhrb5Uwpw.png" data-width="1143" data-height="856" data-action="zoom" data-action-value="1*G4HCc1mpkvHFqbhrb5Uwpw.png" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1G4HCc1mpkvHFqbhrb5Uwpw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="55"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*G4HCc1mpkvHFqbhrb5Uwpw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1G4HCc1mpkvHFqbhrb5Uwpw_002.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*G4HCc1mpkvHFqbhrb5Uwpw.png"></noscript></div></div></figure><p name="bc8c" id="bc8c" class="graf graf--p graf-after--figure">This
 time things are looking quite a bit better. So our last step, for now, 
is to basically figure out how to pull out just the interesting ones.</p><h4 name="dc8a" id="dc8a" class="graf graf--h4 graf-after--p">Non Maximum Suppression [<a href="https://youtu.be/0frKXR-2PBY?t=1h52m15s" data-href="https://youtu.be/0frKXR-2PBY?t=1h52m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:52:15</a>]</h4><p name="f1db" id="f1db" class="graf graf--p graf-after--h4">All
 we are going to do is we are going to go through every pair of these 
bounding boxes and if they overlap by more than some amount, say 0.5, 
using Jaccard and they are both predicting the same class, we are going 
to assume they are the same thing and we are going to pick the one with 
higher <code class="markup--code markup--p-code">p</code> value.</p><p name="6313" id="6313" class="graf graf--p graf-after--p">It is really boring code, Jeremy didn’t write it himself and copied somebody else’s. No reason particularly to go through it.</p><pre name="1f12" id="1f12" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> nms(boxes, scores, overlap=0.5, top_k=100):<br>    keep = scores.new(scores.size(0)).zero_().long()<br>    <strong class="markup--strong markup--pre-strong">if</strong> boxes.numel() == 0: <strong class="markup--strong markup--pre-strong">return</strong> keep<br>    x1 = boxes[:, 0]<br>    y1 = boxes[:, 1]<br>    x2 = boxes[:, 2]<br>    y2 = boxes[:, 3]<br>    area = torch.mul(x2 - x1, y2 - y1)<br>    v, idx = scores.sort(0)  <em class="markup--em markup--pre-em"># sort in ascending order</em><br>    idx = idx[-top_k:]  <em class="markup--em markup--pre-em"># indices of the top-k largest vals</em><br>    xx1 = boxes.new()<br>    yy1 = boxes.new()<br>    xx2 = boxes.new()<br>    yy2 = boxes.new()<br>    w = boxes.new()<br>    h = boxes.new()<br><br>    count = 0<br>    <strong class="markup--strong markup--pre-strong">while</strong> idx.numel() &gt; 0:<br>        i = idx[-1]  <em class="markup--em markup--pre-em"># index of current largest val</em><br>        keep[count] = i<br>        count += 1<br>        <strong class="markup--strong markup--pre-strong">if</strong> idx.size(0) == 1: <strong class="markup--strong markup--pre-strong">break</strong><br>        idx = idx[:-1]  <em class="markup--em markup--pre-em"># remove kept element from view</em><br>        <em class="markup--em markup--pre-em"># load bboxes of next highest vals</em><br>        torch.index_select(x1, 0, idx, out=xx1)<br>        torch.index_select(y1, 0, idx, out=yy1)<br>        torch.index_select(x2, 0, idx, out=xx2)<br>        torch.index_select(y2, 0, idx, out=yy2)<br>        <em class="markup--em markup--pre-em"># store element-wise max with next highest score</em><br>        xx1 = torch.clamp(xx1, min=x1[i])<br>        yy1 = torch.clamp(yy1, min=y1[i])<br>        xx2 = torch.clamp(xx2, max=x2[i])<br>        yy2 = torch.clamp(yy2, max=y2[i])<br>        w.resize_as_(xx2)<br>        h.resize_as_(yy2)<br>        w = xx2 - xx1<br>        h = yy2 - yy1<br>        <em class="markup--em markup--pre-em"># check sizes of xx1 and xx2.. after each iteration</em><br>        w = torch.clamp(w, min=0.0)<br>        h = torch.clamp(h, min=0.0)<br>        inter = w*h<br>        <em class="markup--em markup--pre-em"># IoU = i / (area(a) + area(b) - i)</em><br>        rem_areas = torch.index_select(area, 0, idx)  <br>        <em class="markup--em markup--pre-em"># load remaining areas)</em><br>        union = (rem_areas - inter) + area[i]<br>        IoU = inter/union  <em class="markup--em markup--pre-em"># store result in iou</em><br>        <em class="markup--em markup--pre-em"># keep only elements with an IoU &lt;= overlap</em><br>        idx = idx[IoU.le(overlap)]<br>    <strong class="markup--strong markup--pre-strong">return</strong> keep, count</pre><pre name="6e66" id="6e66" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_nmf(idx):<br>    ima=md.val_ds.ds.denorm(x)[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    clas_pr, clas_ids = b_clas[idx].max(1)<br>    clas_pr = clas_pr.sigmoid()<br><br>    conf_scores = b_clas[idx].sigmoid().t().data<br><br>    out1,out2,cc = [],[],[]<br>    <strong class="markup--strong markup--pre-strong">for</strong> cl <strong class="markup--strong markup--pre-strong">in</strong> range(0, len(conf_scores)-1):<br>        c_mask = conf_scores[cl] &gt; 0.25<br>        <strong class="markup--strong markup--pre-strong">if</strong> c_mask.sum() == 0: <strong class="markup--strong markup--pre-strong">continue</strong><br>        scores = conf_scores[cl][c_mask]<br>        l_mask = c_mask.unsqueeze(1).expand_as(a_ic)<br>        boxes = a_ic[l_mask].view(-1, 4)<br>        ids, count = nms(boxes.data, scores, 0.4, 50)<br>        ids = ids[:count]<br>        out1.append(scores[ids])<br>        out2.append(boxes.data[ids])<br>        cc.append([cl]*count)<br>    cc = T(np.concatenate(cc))<br>    out1 = torch.cat(out1)<br>    out2 = torch.cat(out2)<br><br>    fig, ax = plt.subplots(figsize=(8,8))<br>    torch_gt(ax, ima, out2, cc, out1, 0.1)</pre><pre name="26af" id="26af" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(12): show_nmf(i)</pre></div><div class="section-inner sectionLayout--outsetColumn"><figure name="fc05" id="fc05" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*MXk2chJJEcjOz8hMn1ZsOQ.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1MXk2chJJEcjOz8hMn1ZsOQ_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*MXk2chJJEcjOz8hMn1ZsOQ.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1MXk2chJJEcjOz8hMn1ZsOQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*MXk2chJJEcjOz8hMn1ZsOQ.png"></noscript></div></div></figure><figure name="d53d" id="d53d" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*Fj9fK3G6iXBsGI_XJrxXyg.png" data-width="456" data-height="456" data-is-featured="true" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Fj9fK3G6iXBsGI_XJrxXyg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*Fj9fK3G6iXBsGI_XJrxXyg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Fj9fK3G6iXBsGI_XJrxXyg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*Fj9fK3G6iXBsGI_XJrxXyg.png"></noscript></div></div></figure><figure name="2958" id="2958" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*6p3dm-i-YxC9QkxouHJdoA.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16p3dm-i-YxC9QkxouHJdoA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*6p3dm-i-YxC9QkxouHJdoA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/16p3dm-i-YxC9QkxouHJdoA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*6p3dm-i-YxC9QkxouHJdoA.png"></noscript></div></div></figure><figure name="75fa" id="75fa" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*nkEpAd2_H4lG1vQfnCJn4Q.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1nkEpAd2_H4lG1vQfnCJn4Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*nkEpAd2_H4lG1vQfnCJn4Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1nkEpAd2_H4lG1vQfnCJn4Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*nkEpAd2_H4lG1vQfnCJn4Q.png"></noscript></div></div></figure><figure name="cbef" id="cbef" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*THGq5C21NaP92vw5E_QNdA.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1THGq5C21NaP92vw5E_QNdA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*THGq5C21NaP92vw5E_QNdA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1THGq5C21NaP92vw5E_QNdA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*THGq5C21NaP92vw5E_QNdA.png"></noscript></div></div></figure><figure name="b2b2" id="b2b2" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*0wckbiUSax2JpBlgJxJ05g.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/10wckbiUSax2JpBlgJxJ05g_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*0wckbiUSax2JpBlgJxJ05g.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/10wckbiUSax2JpBlgJxJ05g.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*0wckbiUSax2JpBlgJxJ05g.png"></noscript></div></div></figure><figure name="a1f1" id="a1f1" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*EWbNGEQFvYMgC4PSaLe8Ww.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1EWbNGEQFvYMgC4PSaLe8Ww_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*EWbNGEQFvYMgC4PSaLe8Ww.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1EWbNGEQFvYMgC4PSaLe8Ww.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*EWbNGEQFvYMgC4PSaLe8Ww.png"></noscript></div></div></figure><figure name="5cba" id="5cba" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*vTRCVjln4vkma1R6eBeSwA.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1vTRCVjln4vkma1R6eBeSwA_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*vTRCVjln4vkma1R6eBeSwA.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1vTRCVjln4vkma1R6eBeSwA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*vTRCVjln4vkma1R6eBeSwA.png"></noscript></div></div></figure><figure name="4351" id="4351" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*3Q01FZuzfptkYrekJiGm1g.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/13Q01FZuzfptkYrekJiGm1g_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*3Q01FZuzfptkYrekJiGm1g.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/13Q01FZuzfptkYrekJiGm1g.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*3Q01FZuzfptkYrekJiGm1g.png"></noscript></div></div></figure><figure name="b40b" id="b40b" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*-cD3LQIG9FnyJbt0cnpbNg.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1-cD3LQIG9FnyJbt0cnpbNg_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*-cD3LQIG9FnyJbt0cnpbNg.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1-cD3LQIG9FnyJbt0cnpbNg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*-cD3LQIG9FnyJbt0cnpbNg.png"></noscript></div></div></figure><figure name="ead3" id="ead3" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*Hkgs1u9PFH9ZrTKL8YBW2Q.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Hkgs1u9PFH9ZrTKL8YBW2Q_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*Hkgs1u9PFH9ZrTKL8YBW2Q.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Hkgs1u9PFH9ZrTKL8YBW2Q.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*Hkgs1u9PFH9ZrTKL8YBW2Q.png"></noscript></div></div></figure><figure name="bb83" id="bb83" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 456px; max-height: 456px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*uyTNlp61jcyaW9knbnNSEw.png" data-width="456" data-height="456" data-scroll="native"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1uyTNlp61jcyaW9knbnNSEw_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1000/1*uyTNlp61jcyaW9knbnNSEw.png" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1uyTNlp61jcyaW9knbnNSEw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1000/1*uyTNlp61jcyaW9knbnNSEw.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="55b5" id="55b5" class="graf graf--p graf-after--figure">There are some things still to fix here [<a href="https://youtu.be/0frKXR-2PBY?t=1h53m43s" data-href="https://youtu.be/0frKXR-2PBY?t=1h53m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:43</a>]. The trick will be to use something called feature pyramid. That is what we are going to do in lesson 14.</p><h4 name="5eed" id="5eed" class="graf graf--h4 graf-after--p">Talking a little more about SSD paper [<a href="https://youtu.be/0frKXR-2PBY?t=1h54m3s" data-href="https://youtu.be/0frKXR-2PBY?t=1h54m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:54:03</a>]</h4><p name="b19e" id="b19e" class="graf graf--p graf-after--h4">When
 this paper came out, Jeremy was excited because this and YOLO were the 
first kind of single-pass good quality object detection method that come
 along. There has been this continuous repetition of history in the deep
 learning world which is things that involve multiple passes of multiple
 different pieces, over time, particularly where they involve some 
non-deep learning pieces (like R-CNN did), over time, they always get 
turned into a single end-to-end deep learning model. So I tend to ignore
 them until that happens because that’s the point where people have 
figured out how to show this as a deep learning model, as soon as they 
do that they generally end up something much faster and much more 
accurate. So SSD and YOLO were really important.</p><p name="0629" id="0629" class="graf graf--p graf-after--p">The
 model is 4 paragraphs. Papers are really concise which means you need 
to read them pretty carefully. Partly, though, you need to know which 
bits to read carefully. The bits where they say “here we are going to 
prove the error bounds on this model,” you could ignore that because you
 don’t care about proving error bounds. But the bit which says here is 
what the model is, you need to read real carefully.</p><p name="18c6" id="18c6" class="graf graf--p graf-after--p">Jeremy reads a section <strong class="markup--strong markup--p-strong">2.1 Model</strong> [<a href="https://youtu.be/0frKXR-2PBY?t=1h56m37s" data-href="https://youtu.be/0frKXR-2PBY?t=1h56m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:56:37</a>]</p><p name="1d2d" id="1d2d" class="graf graf--p graf-after--p">If
 you jump straight in and read a paper like this, these 4 paragraphs 
would probably make no sense. But now that we’ve gone through it, you 
read those and hopefully thinking “oh that’s just what Jeremy said, only
 they sad it better than Jeremy and less words [<a href="https://youtu.be/0frKXR-2PBY?t=2h37s" data-href="https://youtu.be/0frKXR-2PBY?t=2h37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:00:37</a>]. If you start to read a paper and go “what the heck”, the trick is to then start reading back over the citations.</p><p name="9414" id="9414" class="graf graf--p graf-after--p">Jeremy reads <strong class="markup--strong markup--p-strong">Matching strategy</strong> and <strong class="markup--strong markup--p-strong">Training objective</strong> (a.k.a. Loss function)[<a href="https://youtu.be/0frKXR-2PBY?t=2h1m44s" data-href="https://youtu.be/0frKXR-2PBY?t=2h1m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:44</a>]</p><h4 name="11d8" id="11d8" class="graf graf--h4 graf-after--p">Some paper tips [<a href="https://youtu.be/0frKXR-2PBY?t=2h2m34s" data-href="https://youtu.be/0frKXR-2PBY?t=2h2m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:02:34</a>]</h4><p name="6731" id="6731" class="graf graf--p graf-after--h4"><a href="https://arxiv.org/pdf/1312.2249.pdf" data-href="https://arxiv.org/pdf/1312.2249.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Scalable Object Detection using Deep Neural Networks</a></p><ul class="postList"><li name="20f5" id="20f5" class="graf graf--li graf--startsWithDoubleQuote graf-after--p">“Training objective” is loss function</li><li name="1d70" id="1d70" class="graf graf--li graf-after--li">Double bars and two 2’s like this means Mean Squared Error</li></ul><figure name="4909" id="4909" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 446px; max-height: 73px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.400000000000002%;"></div><img class="graf-image" data-image-id="1*LubBtX9ODFMBgI34bFHtdw.png" data-width="446" data-height="73" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1LubBtX9ODFMBgI34bFHtdw.png"></div></figure><ul class="postList"><li name="72d3" id="72d3" class="graf graf--li graf-after--figure">log(c) and log(1-c), and x and (1-x) they are all the pieces for binary cross entropy:</li></ul><figure name="ba87" id="ba87" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 562px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.299999999999999%;"></div><img class="graf-image" data-image-id="1*3Xq3HB72jsVKI7uHOHzRDQ.png" data-width="562" data-height="86" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/13Xq3HB72jsVKI7uHOHzRDQ.png"></div></figure><p name="2118" id="2118" class="graf graf--p graf-after--figure graf--trailing">This
 week, go through the code and go through the paper and see what is 
going on. Remember what Jeremy did to make it easier for you was he took
 that loss function, he copied it into a cell and split it up so that 
each bit was in a separate cell. Then after every sell, he printed or 
plotted that value. Hopefully this is a good starting point.</p></div></div></section><section name="8519" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="27d5" id="27d5" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">9</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/deep-learning?source=post" data-action-source="post">Deep Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/ai?source=post" data-action-source="post">AI</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/neural-networks?source=post" data-action-source="post">Neural Networks</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter u-width60" data-post-id="5f0cf9e4bb5b" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----5f0cf9e4bb5b---------------------clap_footer"><div class="u-relative u-foreground"><button class="button button--large button--circle is-active button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal" data-action="multivote" data-action-value="5f0cf9e4bb5b" data-action-type="long-press" data-action-source="post_actions_footer-----5f0cf9e4bb5b---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="5f0cf9e4bb5b"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft10"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="5f0cf9e4bb5b">321</button></span></div></div><div class="buttonSet u-flex0"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="scroll-to-responses">6</button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse is-active u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="remove-from-bookmarks" data-action-value="5f0cf9e4bb5b" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="eab3a535185e"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="eab3a535185e" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="eab3a535185e" data-action-source="footer_card-eab3a535185e-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="5f0cf9e4bb5b"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga?source=footer_card" title="Go to the profile of Hiromi Suenaga" aria-label="Go to the profile of Hiromi Suenaga" data-action-source="footer_card" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg.jpg" class="avatar-image avatar-image--small" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/@hiromi_suenaga" property="cc:attributionName" title="Go to the profile of Hiromi Suenaga" aria-label="Go to the profile of Hiromi Suenaga" rel="author cc:attributionUrl" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a></h3></div></li></div></div></div><div class="js-postFooterPlacements"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1000 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="5854f9d9eb0c" data-source="placement_card_footer_grid---------0-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*ZyjecDAw7VhPLmb6c31A5g.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Also tagged Machine Learning</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Stop Installing Tensorflow using pip for performance sake!</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@learnedvector" data-action="show-user-card" data-action-value="1bdc81ea939d" data-action-type="hover" data-user-id="1bdc81ea939d" data-collection-slug="towards-data-science" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1Fz_1DK1W2BYGTNJQ5XGQHg.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Michael Nguyen"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@learnedvector?source=placement_card_footer_grid---------0-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-43" data-action-value="1bdc81ea939d" data-action-type="hover" data-user-id="1bdc81ea939d" data-collection-slug="towards-data-science" dir="auto">Michael Nguyen</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="2 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5854f9d9eb0c" data-is-label-padded="true" data-source="placement_card_footer_grid-----5854f9d9eb0c----0-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="5854f9d9eb0c" data-action-type="long-press" data-action-source="placement_card_footer_grid-----5854f9d9eb0c----0-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="5854f9d9eb0c">4K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5854f9d9eb0c" data-action-source="placement_card_footer_grid-----5854f9d9eb0c----0-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="4a4a8db896db" data-source="placement_card_footer_grid---------1-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*jnZT4gFAzScOJ_VnYsni0g.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Also tagged Machine Learning</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">The Most in Demand Skills for Data Scientists</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@jeffhale" data-action="show-user-card" data-action-value="451599b1142a" data-action-type="hover" data-user-id="451599b1142a" data-collection-slug="towards-data-science" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1FjId1jWOkaOu1lrpZOyI8A.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jeff Hale"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@jeffhale?source=placement_card_footer_grid---------1-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-43" data-action-value="451599b1142a" data-action-type="hover" data-user-id="451599b1142a" data-collection-slug="towards-data-science" dir="auto">Jeff Hale</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="4a4a8db896db" data-is-label-padded="true" data-source="placement_card_footer_grid-----4a4a8db896db----1-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="4a4a8db896db" data-action-type="long-press" data-action-source="placement_card_footer_grid-----4a4a8db896db----1-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="4a4a8db896db">3.9K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="4a4a8db896db" data-action-source="placement_card_footer_grid-----4a4a8db896db----1-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="4ebd4711d0ec" data-source="placement_card_footer_grid---------2-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/0*K8eg3bUVu4AG-4FB&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Also tagged Deep Learning</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Beyond Word Embeddings Part 2- Word Vectors &amp; NLP Modeling from BoW to BERT</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@aribornstein" data-action="show-user-card" data-action-value="b3c7769e3e2f" data-action-type="hover" data-user-id="b3c7769e3e2f" data-collection-slug="towards-data-science" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/19Y0zWLwh1nYuBZMetDnC7w.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Aaron (Ari) Bornstein"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@aribornstein?source=placement_card_footer_grid---------2-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-43" data-action-value="b3c7769e3e2f" data-action-type="hover" data-user-id="b3c7769e3e2f" data-collection-slug="towards-data-science" dir="auto">Aaron (Ari) Bornstein</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="4ebd4711d0ec" data-is-label-padded="true" data-source="placement_card_footer_grid-----4ebd4711d0ec----2-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="4ebd4711d0ec" data-action-type="long-press" data-action-source="placement_card_footer_grid-----4ebd4711d0ec----2-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="4ebd4711d0ec">241</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="4ebd4711d0ec" data-action-source="placement_card_footer_grid-----4ebd4711d0ec----2-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_23"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1IzvwYFGjjBE8icwhb7LdGQ_002.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Sanjay Yadav"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">Sanjay Yadav</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_6"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="f02bd50ffdd3" data-source="responses---------0---------------------" data-action-scope="_actionscope_7" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kahthong1993" data-action="show-user-card" data-action-value="438535c75e3c" data-action-type="hover" data-user-id="438535c75e3c" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0-Hv0oFRZlgXHvgGp.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Kah Thong Loh"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@kahthong1993?source=responses---------0---------------------" data-action="show-user-card" data-action-source="responses---------0---------------------" data-action-value="438535c75e3c" data-action-type="hover" data-user-id="438535c75e3c" dir="auto">Kah Thong Loh</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@kahthong1993/hi-hiromo-f02bd50ffdd3?source=responses---------0---------------------" data-action="open-post" data-action-value="https://medium.com/@kahthong1993/hi-hiromo-f02bd50ffdd3?source=responses---------0---------------------" data-action-source="preview-listing"><time datetime="2018-05-01T04:06:04.523Z">May 1</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@kahthong1993/hi-hiromo-f02bd50ffdd3?source=responses---------0---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a213" id="a213" class="graf graf--p graf--leading">Hi Hiromo,</p><p name="3517" id="3517" class="graf graf--p graf-after--p">Appreciate for the excellent notes!!</p><p name="ea1c" id="ea1c" class="graf graf--p graf-after--p">Not sure why I have no access to the part 2 topic in fast ai forum. (Already registered account)</p><p name="a22f" id="a22f" class="graf graf--p graf-after--p">Do you mind sharing the links to Jeremy video’s from lesson 8 onwards?</p><p name="40da" id="40da" class="graf graf--p graf-after--p">I can’t find the link to lesson 10, 12, 13 etc…</p><p name="4825" id="4825" class="graf graf--p graf-after--p graf--trailing">Thanks!!!</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="f02bd50ffdd3" data-is-flush-left="true" data-source="listing-----f02bd50ffdd3---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="f02bd50ffdd3" data-action-type="long-press" data-action-source="listing-----f02bd50ffdd3---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@kahthong1993/hi-hiromo-f02bd50ffdd3?source=responses---------0---------------------#--responses" data-action-source="responses---------0---------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="f02bd50ffdd3" data-action-source="listing-----f02bd50ffdd3---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="f02bd50ffdd3"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="f2c1a6acd87" data-source="responses---------0---------------------" data-action-scope="_actionscope_8" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------0---------------------" data-action="show-user-card" data-action-source="responses---------0---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/here-they-are-http-course-fast-ai-lessons-lessons2-html-f2c1a6acd87?source=responses---------0---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/here-they-are-http-course-fast-ai-lessons-lessons2-html-f2c1a6acd87?source=responses---------0---------------------" data-action-source="preview-listing"><time datetime="2018-05-23T22:04:25.190Z">May 24</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@hiromi_suenaga/here-they-are-http-course-fast-ai-lessons-lessons2-html-f2c1a6acd87?source=responses---------0---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4635" id="4635" class="graf graf--p graf--leading graf--trailing">Here they are! <span class="markup--anchor markup--p-anchor" data-action="open-inner-link" data-action-value="http://course.fast.ai/lessons/lessons2.html">http://course.fast.ai/lessons/lessons2.html</span></p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="f2c1a6acd87" data-is-flush-left="true" data-source="listing-----f2c1a6acd87---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="f2c1a6acd87" data-action-type="long-press" data-action-source="listing-----f2c1a6acd87---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="f2c1a6acd87" data-action-source="listing-----f2c1a6acd87---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="f2c1a6acd87"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_9"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation between <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a> and <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto">Kelin Christi</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="787e7054b512" data-source="responses---------1---------------------" data-action-scope="_actionscope_10" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0yBC-qRRQwHpUBwbA.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Kelin Christi"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@kelinC11?source=responses---------1---------------------" data-action="show-user-card" data-action-source="responses---------1---------------------" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto">Kelin Christi</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@kelinC11/hi-hiromi-787e7054b512?source=responses---------1---------------------" data-action="open-post" data-action-value="https://medium.com/@kelinC11/hi-hiromi-787e7054b512?source=responses---------1---------------------" data-action-source="preview-listing"><time datetime="2018-04-22T23:25:54.796Z">Apr 23</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="1 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@kelinC11/hi-hiromi-787e7054b512?source=responses---------1---------------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e537" id="e537" class="graf graf--p graf--leading">Hi Hiromi,</p><p name="35b7" id="35b7" class="graf graf--p graf-after--p">Thanks a lot for these awesome notes. Very detailed, and thorough!</p><p name="54fe" id="54fe" class="graf graf--p graf-after--p graf--trailing">I
 was running the code, and ran into an error in the “Loss Function” 
Section. After defining learn.crit &amp; learn.metrics, when I tried to 
run learn.fit(lr,1,cycle_len=3, use_clr=(32,5)&nbsp;, I get an error “ 
’list’ object has no…</p></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="787e7054b512" data-is-flush-left="true" data-source="listing-----787e7054b512---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="787e7054b512" data-action-type="long-press" data-action-source="listing-----787e7054b512---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@kelinC11/hi-hiromi-787e7054b512?source=responses---------1---------------------#--responses" data-action-source="responses---------1---------------------">3 responses</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="787e7054b512" data-action-source="listing-----787e7054b512---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="787e7054b512"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="eb41832e965b" data-source="responses---------1---------------------" data-action-scope="_actionscope_11" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------1---------------------" data-action="show-user-card" data-action-source="responses---------1---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/it-looks-like-the-recent-changes-to-model-py-eb41832e965b?source=responses---------1---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/it-looks-like-the-recent-changes-to-model-py-eb41832e965b?source=responses---------1---------------------" data-action-source="preview-listing"><time datetime="2018-04-25T01:19:18.981Z">Apr 25</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="1 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@hiromi_suenaga/it-looks-like-the-recent-changes-to-model-py-eb41832e965b?source=responses---------1---------------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="previewTitle" id="previewTitle" class="graf graf--h3 graf--leading graf--title">It looks like the recent changes to&nbsp;model.py</h3><h4 name="previewSubtitle" id="previewSubtitle" class="graf graf--h4 graf-after--h3 graf--trailing graf--subtitle">Hope that&nbsp;helps!</h4></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="eb41832e965b" data-is-flush-left="true" data-source="listing-----eb41832e965b---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="eb41832e965b" data-action-type="long-press" data-action-source="listing-----eb41832e965b---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@hiromi_suenaga/it-looks-like-the-recent-changes-to-model-py-eb41832e965b?source=responses---------1---------------------#--responses" data-action-source="responses---------1---------------------">2 responses</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="eb41832e965b" data-action-source="listing-----eb41832e965b---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="eb41832e965b"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="eb08b923614b" data-source="responses---------1---------------------" data-action-scope="_actionscope_12" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0yBC-qRRQwHpUBwbA.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Kelin Christi"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@kelinC11?source=responses---------1---------------------" data-action="show-user-card" data-action-source="responses---------1---------------------" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto">Kelin Christi</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@kelinC11/update-worked-and-the-code-makes-sense-eb08b923614b?source=responses---------1---------------------" data-action="open-post" data-action-value="https://medium.com/@kelinC11/update-worked-and-the-code-makes-sense-eb08b923614b?source=responses---------1---------------------" data-action-source="preview-listing"><time datetime="2018-04-25T21:32:41.567Z">Apr 26</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@kelinC11/update-worked-and-the-code-makes-sense-eb08b923614b?source=responses---------1---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="993c" id="993c" class="graf graf--p graf--leading graf--trailing">Update: Worked, and the code makes sense&nbsp;:)</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="eb08b923614b" data-is-flush-left="true" data-source="listing-----eb08b923614b---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="eb08b923614b" data-action-type="long-press" data-action-source="listing-----eb08b923614b---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="eb08b923614b">1</button></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="eb08b923614b" data-action-source="listing-----eb08b923614b---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="eb08b923614b"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_13"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--summary"><span class="postMetaInline"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0yBC-qRRQwHpUBwbA.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Kelin Christi"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto">Kelin Christi</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental">Hi Hiromi, Thanks a lot for these awesome notes. Very detailed, and thorough!</div></div></div></span></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="cff2407499cb" data-source="responses---------2---------------------" data-action-scope="_actionscope_14" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------2---------------------" data-action="show-user-card" data-action-source="responses---------2---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/yes-i-see-it-too-cff2407499cb?source=responses---------2---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/yes-i-see-it-too-cff2407499cb?source=responses---------2---------------------" data-action-source="preview-listing"><time datetime="2018-04-24T04:24:49.718Z">Apr 24</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@hiromi_suenaga/yes-i-see-it-too-cff2407499cb?source=responses---------2---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="6539" id="6539" class="graf graf--p graf--leading graf--trailing">Yes!
 I see it too. I will poke around to see if I can figure it out 
tomorrow, but in the meantime, I asked on the forum to see if it’s 
something obvious for somebody.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="cff2407499cb" data-is-flush-left="true" data-source="listing-----cff2407499cb---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="cff2407499cb" data-action-type="long-press" data-action-source="listing-----cff2407499cb---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="cff2407499cb" data-action-source="listing-----cff2407499cb---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="cff2407499cb"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_15"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="e154f74f1f3b" data-source="responses---------3---------------------" data-action-scope="_actionscope_16" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@ang_shine" data-action="show-user-card" data-action-value="d17b79560d8c" data-action-type="hover" data-user-id="d17b79560d8c" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1dmbNkD5D-u45r44go_cf0g.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of ang shine"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@ang_shine?source=responses---------3---------------------" data-action="show-user-card" data-action-source="responses---------3---------------------" data-action-value="d17b79560d8c" data-action-type="hover" data-user-id="d17b79560d8c" dir="auto">ang shine</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@ang_shine/why-can-the-overlap-greater-than-1-is-it-the-iou-e154f74f1f3b?source=responses---------3---------------------" data-action="open-post" data-action-value="https://medium.com/@ang_shine/why-can-the-overlap-greater-than-1-is-it-the-iou-e154f74f1f3b?source=responses---------3---------------------" data-action-source="preview-listing"><time datetime="2018-05-23T05:11:03.416Z">May 23</time></a></div></div></div></div></div><a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b#31ef"><div class="u-fontSize14 u-marginTop10 u-marginBottom20 u-padding14 u-xs-padding12 u-borderRadius3 u-borderCardBackground u-borderLighterHover u-boxShadow1px4pxCardBorder"><div class="label label--quote u-accentColor--highlightFaint">1.9900</div></div></a><div><a class="" href="https://medium.com/@ang_shine/why-can-the-overlap-greater-than-1-is-it-the-iou-e154f74f1f3b?source=responses---------3---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="a8cf" id="a8cf" class="graf graf--p graf--leading graf--trailing">why can the overlap greater than 1? Is it the IoU?</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="e154f74f1f3b" data-is-flush-left="true" data-source="listing-----e154f74f1f3b---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="e154f74f1f3b" data-action-type="long-press" data-action-source="listing-----e154f74f1f3b---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@ang_shine/why-can-the-overlap-greater-than-1-is-it-the-iou-e154f74f1f3b?source=responses---------3---------------------#--responses" data-action-source="responses---------3---------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="e154f74f1f3b" data-action-source="listing-----e154f74f1f3b---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="e154f74f1f3b"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="3ce34dff4481" data-source="responses---------3---------------------" data-action-scope="_actionscope_17" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------3---------------------" data-action="show-user-card" data-action-source="responses---------3---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/actually-1-99-is-not-a-calculated-value-jeremy-uses-it-to-force-assignment-3ce34dff4481?source=responses---------3---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/actually-1-99-is-not-a-calculated-value-jeremy-uses-it-to-force-assignment-3ce34dff4481?source=responses---------3---------------------" data-action-source="preview-listing"><time datetime="2018-05-23T22:07:20.544Z">May 24</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@hiromi_suenaga/actually-1-99-is-not-a-calculated-value-jeremy-uses-it-to-force-assignment-3ce34dff4481?source=responses---------3---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="0b32" id="0b32" class="graf graf--p graf--leading">Actually, 1.99 is not a calculated value. Jeremy uses it to force assignment:</p><pre name="ffee" id="ffee" class="graf graf--pre graf-after--p">gt_overlap[prior_idx] = 1.99</pre><p name="4508" id="4508" class="graf graf--p graf-after--pre">Here is the video timestamp he mentions it:<br><span class="markup--anchor markup--p-anchor" data-action="open-inner-link" data-action-value="https://youtu.be/0frKXR-2PBY?t=1h1m19s">https://youtu.be/0frKXR-2PBY?t=1h1m19s</span></p><p name="b2b8" id="b2b8" class="graf graf--p graf-after--p graf--trailing">Hope that helps!</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="3ce34dff4481" data-is-flush-left="true" data-source="listing-----3ce34dff4481---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="3ce34dff4481" data-action-type="long-press" data-action-source="listing-----3ce34dff4481---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="3ce34dff4481" data-action-source="listing-----3ce34dff4481---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="3ce34dff4481"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_18"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--summary"><span class="postMetaInline"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0yBC-qRRQwHpUBwbA.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Kelin Christi"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@kelinC11" data-action="show-user-card" data-action-value="442a1f871e72" data-action-type="hover" data-user-id="442a1f871e72" dir="auto">Kelin Christi</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental">Hi Hiromi, Thanks a lot for these awesome notes. Very detailed, and thorough!</div></div></div></span></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="25ab3ccfb4fc" data-source="responses---------4---------------------" data-action-scope="_actionscope_19" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------4---------------------" data-action="show-user-card" data-action-source="responses---------4---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/hi-kelin-25ab3ccfb4fc?source=responses---------4---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/hi-kelin-25ab3ccfb4fc?source=responses---------4---------------------" data-action-source="preview-listing"><time datetime="2018-04-23T13:02:05.821Z">Apr 23</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@hiromi_suenaga/hi-kelin-25ab3ccfb4fc?source=responses---------4---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="d8a1" id="d8a1" class="graf graf--p graf--leading">Hi Kelin!</p><p name="950e" id="950e" class="graf graf--p graf-after--p graf--trailing">Thank you for the kind words&nbsp;:) I will take a look this evening and let you know!</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="25ab3ccfb4fc" data-is-flush-left="true" data-source="listing-----25ab3ccfb4fc---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="25ab3ccfb4fc" data-action-type="long-press" data-action-source="listing-----25ab3ccfb4fc---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="25ab3ccfb4fc">1</button></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="25ab3ccfb4fc" data-action-source="listing-----25ab3ccfb4fc---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="25ab3ccfb4fc"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_20"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation with <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="63f377534c51" data-source="responses---------5---------------------" data-action-scope="_actionscope_21" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@js.mayer.min" data-action="show-user-card" data-action-value="d3510ee70c98" data-action-type="hover" data-user-id="d3510ee70c98" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/0pjW7-Zw9Nn2M5Fk9.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Terry JS Min"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@js.mayer.min?source=responses---------5---------------------" data-action="show-user-card" data-action-source="responses---------5---------------------" data-action-value="d3510ee70c98" data-action-type="hover" data-user-id="d3510ee70c98" dir="auto">Terry JS Min</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@js.mayer.min/hi-hiromi-63f377534c51?source=responses---------5---------------------" data-action="open-post" data-action-value="https://medium.com/@js.mayer.min/hi-hiromi-63f377534c51?source=responses---------5---------------------" data-action-source="preview-listing"><time datetime="2018-06-09T08:01:46.490Z">Jun 9</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@js.mayer.min/hi-hiromi-63f377534c51?source=responses---------5---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5987" id="5987" class="graf graf--p graf--leading">Hi Hiromi,</p><p name="bc3d" id="bc3d" class="graf graf--p graf-after--p">Your course note is so great!</p><p name="dc4b" id="dc4b" class="graf graf--p graf-after--p">Can I translate into Korean and re-post to my medium blog?</p><p name="9923" id="9923" class="graf graf--p graf-after--p">If you give me a chance to do it, it was really helpful to understand course much more</p><p name="b31c" id="b31c" class="graf graf--p graf-after--p graf--trailing">Thanks!</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="63f377534c51" data-is-flush-left="true" data-source="listing-----63f377534c51---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="63f377534c51" data-action-type="long-press" data-action-source="listing-----63f377534c51---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@js.mayer.min/hi-hiromi-63f377534c51?source=responses---------5---------------------#--responses" data-action-source="responses---------5---------------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="63f377534c51" data-action-source="listing-----63f377534c51---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="63f377534c51"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="22382e0c689" data-source="responses---------5---------------------" data-action-scope="_actionscope_22" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@hiromi_suenaga" data-action="show-user-card" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto"><img src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/1OUE6UCNR-GpaCAVQno08Wg_003.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Hiromi Suenaga"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@hiromi_suenaga?source=responses---------5---------------------" data-action="show-user-card" data-action-source="responses---------5---------------------" data-action-value="eab3a535185e" data-action-type="hover" data-user-id="eab3a535185e" dir="auto">Hiromi Suenaga</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@hiromi_suenaga/of-course-please-give-all-the-credits-to-jeremy-and-rachel-at-fast-ai-22382e0c689?source=responses---------5---------------------" data-action="open-post" data-action-value="https://medium.com/@hiromi_suenaga/of-course-please-give-all-the-credits-to-jeremy-and-rachel-at-fast-ai-22382e0c689?source=responses---------5---------------------" data-action-source="preview-listing"><time datetime="2018-06-09T19:39:42.710Z">Jun 10</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@hiromi_suenaga/of-course-please-give-all-the-credits-to-jeremy-and-rachel-at-fast-ai-22382e0c689?source=responses---------5---------------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="8fb9" id="8fb9" class="graf graf--p graf--leading graf--trailing">Of
 course&nbsp;:) Please give all the credits to Jeremy and Rachel at 
Fast.ai. I’m merely taking notes of the great course they’ve created.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="22382e0c689" data-is-flush-left="true" data-source="listing-----22382e0c689---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="22382e0c689" data-action-type="long-press" data-action-source="listing-----22382e0c689---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="22382e0c689" data-action-source="listing-----22382e0c689---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="22382e0c689"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div></div><div class="container js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1000 js-postLeftSidebar"><div class="u-foreground u-top0 u-sm-hide u-marginLeftNegative12 js-postShareWidget u-fixed u-transition--fadeOut300" data-scroll="fixed" style="transform: translateY(150px);"><ul><li class="u-textAlignCenter u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexColumn u-marginBottom10 u-width60" data-post-id="5f0cf9e4bb5b" data-is-icon-29px="true" data-is-vertical="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_share_widget-----5f0cf9e4bb5b---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--large button--circle is-active button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal" data-action="multivote" data-action-value="5f0cf9e4bb5b" data-action-type="long-press" data-action-source="post_share_widget-----5f0cf9e4bb5b---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="5f0cf9e4bb5b"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-flexOrderNegative1 u-height20 u-marginBottom7"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-block u-marginAuto" data-action="show-recommends" data-action-value="5f0cf9e4bb5b">321</button></span></div></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_share_widget"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_share_widget"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse is-active u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="remove-from-bookmarks" data-action-value="5f0cf9e4bb5b" data-action-source="post_share_widget-----5f0cf9e4bb5b---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li></ul></div></aside><div class="highlightMenu" data-action-scope="_actionscope_3" style="left: 550px; top: 32885px;"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="twitter" data-action-source="quote_menu" data-skip-onboarding="true"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></button><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"35411-e9ff56c","currentUser":{"userId":"b43c07dd10f6","username":"sanjay.yd131","name":"Sanjay Yadav","email":"sanjay.yd131@gmail.com","imageId":"1*IzvwYFGjjBE8icwhb7LdGQ.jpeg","createdAt":1502935739158,"lastPostCreatedAt":1526282329934,"isVerified":true,"subscriberEmail":"","onboardingStatus":3,"googleAccountId":"113835258251004167102","googleEmail":"sanjay.yd131@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":false,"isWriterProgramInvited":false,"isWriterProgramOptedOut":false,"writerProgramVersion":0,"writerProgramEnrolledAt":0},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"en-us","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.anz4F_X386Mr2Tdpy5STEg.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.Qa55l1D5ePln79NOugNE8w.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.5s69Mp-Cz26-QrsaVmOXBg.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.bwtJUwCarv7WWkBZzf_utg.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.bH46UNHkcsTcjuDtX7hMnQ.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.aC0aGmFx-iCihVwkFpFURw.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.-vfQ1WjMCIK9YriocSc6lg.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.s4EnkkjeBgT1l6Ykb5Tc0A.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.eJgbqw7Cb9VJrqGd7k82ZA.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.HW5rA9xPSvdVJ6yqNQzLbw.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.cf3ZbZZIsOODWUEgJLhxOA.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1540006647672:35baae994490","useragent":{"browser":"firefox","family":"firefox","os":"windows","version":62,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"enable_paypal_delinquency_handling":true,"enable_post_monger_v2":true,"enable_post_monger_v3":true,"enable_star_meter_ui":true,"max_premium_content_per_user_under_metering":3,"enable_metered_out_email":true,"enable_automated_mission_control_triggers":true,"enable_top_stories_for_you":true,"enable_ios_member_post_labeling":true,"enable_lite_profile":true,"enable_signin_wall_custom_domain":true,"app_download_email_template":"control","enable_topic_lifecycle_email":true,"enable_marketing_emails":true,"enable_truncated_rss_for_tags_and_topics":true,"enable_ios_payment_page_button_experimental_copy":true,"enable_post_diversity_on_rex_homepage":true,"enable_parsely":true,"enable_native_ai":true,"enable_branch_io":true,"enable_branch_io_deeplinks":true,"enable_synchronous_parsely":true,"enable_ios_post_stats":true,"enable_reader_interests":true,"enable_post_stats_page_new_milestones":true,"enable_lite_topics":true,"enable_lite_stories":true,"enable_lite_billing_history":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"annual_renewal_reminder_email_variant":"control","enable_annual_renewal_reminder_email":true,"enable_sift_integration":true,"enable_sift_science_webhook":true,"enable_quality_assurance_queue":true,"enable_writer_controls":true,"enable_partner_dashboard_tax_link":true,"enable_new_collaborative_filtering_data":true,"enable_left_aligned_logo_and_topic_metabar":true,"enable_rtr_control_model":true,"android_rating_prompt_stories_read_threshold":3},"xsrfToken":"pvZCiqtVfhUH","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"},{"id":10,"url":"https://glyph.medium.com/css/elv8.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"6c1f4c6c47aa1d4e\",\"ot-tracer-traceid\":\"276971ed7c6865b4\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"goldfinchUrl":"https://goldfinch.medium.com","buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","paypalClientMode":"production","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"IN","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm"}
// ]]></script><script charset="UTF-8" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/main-base.js" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"5f0cf9e4bb5b","versionId":"de36f820714b","creatorId":"eab3a535185e","creator":{"userId":"eab3a535185e","name":"Hiromi Suenaga","username":"hiromi_suenaga","createdAt":1505308665801,"imageId":"1*OUE6UCNR-GpaCAVQno08Wg.jpeg","backgroundImageId":"","bio":"","twitterScreenName":"hiromi_suenaga","socialStats":{"userId":"eab3a535185e","usersFollowedCount":10,"usersFollowedByCount":1348,"type":"SocialStats"},"social":{"userId":"b43c07dd10f6","targetUserId":"eab3a535185e","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"type":"User"},"homeCollectionId":"","title":"Deep Learning 2: Part 2 Lesson 9","detectedLanguage":"en","latestVersion":"de36f820714b","latestPublishedVersion":"de36f820714b","hasUnpublishedEdits":false,"latestRev":6207,"createdAt":1522109274395,"updatedAt":1533588297819,"acceptedAt":0,"firstPublishedAt":1522550896012,"latestPublishedAt":1533588297819,"vote":true,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really”…","bodyModel":{"paragraphs":[{"name":"2be5","type":3,"text":"Deep Learning 2: Part 2 Lesson 9","markups":[]},{"name":"2560","type":1,"text":"My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to Jeremy and Rachel who gave me this opportunity to learn.","markups":[{"type":3,"start":23,"end":37,"href":"http://www.fast.ai/","title":"","rel":"noopener nofollow nofollow noopener noopener noopener nofollow noopener","anchorType":0},{"type":3,"start":175,"end":181,"href":"https://twitter.com/jeremyphoward","title":"","rel":"noopener nofollow nofollow noopener noopener noopener nofollow noopener","anchorType":0},{"type":3,"start":186,"end":192,"href":"https://twitter.com/math_rachel","title":"","rel":"noopener nofollow nofollow noopener noopener noopener nofollow noopener","anchorType":0},{"type":2,"start":0,"end":231}]},{"name":"929c","type":1,"text":"Lessons: 1 ・ 2 ・ 3 ・ 4 ・ 5 ・ 6 ・ 7 ・ 8 ・ 9 ・ 10 ・ 11 ・ 12 ・ 13 ・ 14","markups":[{"type":3,"start":9,"end":10,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197","title":"","rel":"","anchorType":0},{"type":3,"start":13,"end":14,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4","title":"","rel":"","anchorType":0},{"type":3,"start":17,"end":18,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56","title":"","rel":"","anchorType":0},{"type":3,"start":21,"end":22,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa","title":"","rel":"","anchorType":0},{"type":3,"start":25,"end":26,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8","title":"","rel":"","anchorType":0},{"type":3,"start":29,"end":30,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c","title":"","rel":"","anchorType":0},{"type":3,"start":33,"end":34,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c","title":"","rel":"","anchorType":0},{"type":3,"start":37,"end":38,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493","title":"","rel":"","anchorType":0},{"type":3,"start":41,"end":42,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","title":"","rel":"","anchorType":0},{"type":3,"start":45,"end":47,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c","title":"","rel":"","anchorType":0},{"type":3,"start":50,"end":52,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34","title":"","rel":"","anchorType":0},{"type":3,"start":55,"end":57,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94","title":"","rel":"","anchorType":0},{"type":3,"start":60,"end":62,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0","title":"","rel":"","anchorType":0},{"type":3,"start":65,"end":67,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add","title":"","rel":"","anchorType":0},{"type":1,"start":41,"end":42}]},{"name":"b0c6","type":3,"text":"Links","markups":[]},{"name":"32c6","type":1,"text":"Forum / Video","markups":[{"type":3,"start":0,"end":5,"href":"http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1","title":"","rel":"noopener","anchorType":0},{"type":3,"start":8,"end":13,"href":"https://youtu.be/0frKXR-2PBY","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":13}]},{"name":"8cda","type":3,"text":"Review","markups":[]},{"name":"3167","type":13,"text":"From Last week:","markups":[]},{"name":"13b8","type":9,"text":"Pathlib; JSON","markups":[]},{"name":"8f79","type":9,"text":"Dictionary comprehensions","markups":[]},{"name":"c475","type":9,"text":"Defaultdict","markups":[]},{"name":"5c7b","type":9,"text":"How to jump around fastai source","markups":[]},{"name":"2b0e","type":9,"text":"matplotlib OO API","markups":[]},{"name":"a56b","type":9,"text":"Lambda functions","markups":[]},{"name":"35f7","type":9,"text":"Bounding box coordinates","markups":[]},{"name":"474a","type":9,"text":"Custom head; bounding box regression","markups":[]},{"name":"da49","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2nxK3zuKRnDCu_3qVhSMnw.png","originalWidth":1515,"originalHeight":839}},{"name":"67d3","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*9G88jQ42l5RdwFi2Yr_h_Q.png","originalWidth":1488,"originalHeight":783}},{"name":"6247","type":13,"text":"From Part 1:","markups":[]},{"name":"5e8a","type":9,"text":"How to view model inputs from a DataLoader","markups":[]},{"name":"648e","type":9,"text":"How to view model outputs","markups":[]},{"name":"1296","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*E3Z5vKnp6ZkfuLR83979RA.png","originalWidth":1508,"originalHeight":559}},{"name":"5492","type":3,"text":"Data Augmentation and Bounding Box [2:58]","markups":[{"type":3,"start":36,"end":40,"href":"https://youtu.be/0frKXR-2PBY?t=2m58s","title":"","rel":"","anchorType":0}]},{"name":"e3ff","type":1,"text":"Notebook","markups":[{"type":3,"start":0,"end":8,"href":"https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb","title":"","rel":"","anchorType":0}]},{"name":"be1e","type":1,"text":"Awkward rough edges of fastai:\nA classifier is anything with dependent variable is categorical or binomial. As opposed to regression which is anything with dependent variable is continuous. Naming is a little confusing but will be sorted out in future. Here, continuous is True because our dependent variable is the coordinates of bounding box — hence this is actually a regressor data.","markups":[{"type":10,"start":259,"end":269},{"type":10,"start":273,"end":277},{"type":1,"start":0,"end":30},{"type":2,"start":33,"end":43},{"type":2,"start":122,"end":132}]},{"name":"67ba","type":8,"text":"tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, \n                       aug_tfms=augs)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,\n                                  continuous=True, bs=4)","markups":[{"type":1,"start":108,"end":118},{"type":1,"start":198,"end":213}]},{"name":"f768","type":13,"text":"Let’s create some data augmentation [4:40]","markups":[{"type":3,"start":37,"end":41,"href":"https://youtu.be/0frKXR-2PBY?t=4m40s","title":"","rel":"","anchorType":0}]},{"name":"92de","type":8,"text":"augs = [RandomFlip(), \n        RandomRotate(30),\n        RandomLighting(0.1,0.1)]","markups":[]},{"name":"8b2e","type":1,"text":"Normally, we use these shortcuts Jeremy created for us, but they are simply lists of random augmentations. But you can easily create your own (most if not all of them start with “Random”).","markups":[]},{"name":"2e67","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*lAIQHKT0GbjY0fRZKmpFaA.png","originalWidth":524,"originalHeight":52}},{"name":"544c","type":8,"text":"tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,\n                       aug_tfms=augs)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,\n                       continuous=True, bs=4)","markups":[]},{"name":"aac4","type":8,"text":"idx=3\nfig,axes = plt.subplots(3,3, figsize=(9,9))\nfor i,ax in enumerate(axes.flat):\n    x,y=next(iter(md.aug_dl))\n    ima=md.val_ds.denorm(to_np(x))[idx]\n    b = bb_hw(to_np(y[idx]))\n    print(b)\n    show_img(ima, ax=ax)\n    draw_rect(ax, b)","markups":[]},{"name":"97e9","type":8,"text":"[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]\n[ 115.   63.  240.  311.]","markups":[{"type":2,"start":0,"end":233}]},{"name":"cd34","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*QMa_SUUVOypZHKaAuXDkSw.png","originalWidth":1020,"originalHeight":990}},{"name":"f3bd","type":1,"text":"As you can see, the image gets rotated and lighting varies, but bounding box is not moving and is in a wrong spot [6:17]. This is the problem with data augmentations when your dependent variable is pixel values or in some way connected to the independent variable — they need to be augmented together. As you can see in the bounding box coordinates [ 115. 63. 240. 311.] , our image is 224 by 224 — so it is neither scaled nor cropped. The dependent variable needs to go through all the geometric transformation as the independent variables.","markups":[{"type":10,"start":349,"end":370},{"type":3,"start":115,"end":119,"href":"https://youtu.be/0frKXR-2PBY?t=6m17s","title":"","rel":"","anchorType":0},{"type":2,"start":80,"end":90},{"type":2,"start":98,"end":113}]},{"name":"586a","type":1,"text":"To do this [7:10], every transformation has an optional tfm_y parameter:","markups":[{"type":10,"start":56,"end":61},{"type":3,"start":12,"end":16,"href":"https://youtu.be/0frKXR-2PBY?t=7m10s","title":"","rel":"","anchorType":0}]},{"name":"8852","type":8,"text":"augs = [RandomFlip(tfm_y=TfmType.COORD),\n        RandomRotate(30, tfm_y=TfmType.COORD),\n        RandomLighting(0.1,0.1, tfm_y=TfmType.COORD)]","markups":[]},{"name":"161c","type":8,"text":"tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,\n                       tfm_y=TfmType.COORD, aug_tfms=augs)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, \n                       continuous=True, bs=4)","markups":[]},{"name":"e0d7","type":1,"text":"TrmType.COORD indicates that the y value represents coordinate. This needs to be added to all the augmentations as well as tfms_from_model which is responsible for cropping, zooming, resizing, padding, etc.","markups":[{"type":10,"start":0,"end":13},{"type":10,"start":123,"end":138},{"type":2,"start":33,"end":34}]},{"name":"4f8e","type":8,"text":"idx=3\nfig,axes = plt.subplots(3,3, figsize=(9,9))\nfor i,ax in enumerate(axes.flat):\n    x,y=next(iter(md.aug_dl))\n    ima=md.val_ds.denorm(to_np(x))[idx]\n    b = bb_hw(to_np(y[idx]))\n    print(b)\n    show_img(ima, ax=ax)\n    draw_rect(ax, b)","markups":[]},{"name":"e2fc","type":8,"text":"[  48.   34.  112.  188.]\n[  65.   36.  107.  185.]\n[  49.   27.  131.  195.]\n[  24.   18.  147.  204.]\n[  61.   34.  113.  188.]\n[  55.   31.  121.  191.]\n[  52.   19.  144.  203.]\n[   7.    0.  193.  222.]\n[  52.   38.  105.  182.]","markups":[{"type":2,"start":0,"end":233}]},{"name":"f82b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_ge-RyZpEIQ5fiSvo207rA.png","originalWidth":507,"originalHeight":495}},{"name":"dfa4","type":1,"text":"Now, the bounding box moves with the image and is in the right spot. You may notice that sometimes it looks odd like the middle on in the bottom row. This is the constraint of the information we have. If the object occupied the corners of the original bounding box, your new bounding box needs to be bigger after the image rotates. So you must be careful of not doing too higher rotations with bounding boxes because there is not enough information for them to stay accurate. If we were doing polygons or segmentations, we would not have this problem.","markups":[{"type":1,"start":344,"end":408}]},{"name":"8d9f","type":4,"text":"This why the box gets bigger","markups":[],"layout":1,"metadata":{"id":"1*4V4sjFZxn-y2cU9tCJPEUw.png","originalWidth":400,"originalHeight":220}},{"name":"123b","type":8,"text":"tfm_y = TfmType.COORD\naugs = [RandomFlip(tfm_y=tfm_y),\n        RandomRotate(3, p=0.5, tfm_y=tfm_y),\n        RandomLighting(0.05,0.05, tfm_y=tfm_y)]","markups":[{"type":1,"start":76,"end":77},{"type":1,"start":79,"end":84}]},{"name":"b051","type":8,"text":"tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, \n                 tfm_y=tfm_y, aug_tfms=augs)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, \n                 continuous=True)","markups":[]},{"name":"76dc","type":1,"text":"So here, we do maximum of 3 degree rotation to avoid this problem [9:14]. It also only rotates half of the time (p=0.5).","markups":[{"type":10,"start":113,"end":118},{"type":3,"start":67,"end":71,"href":"https://youtu.be/0frKXR-2PBY?t=9m14s","title":"","rel":"","anchorType":0}]},{"name":"58b8","type":13,"text":"custom_head [9:34]","markups":[{"type":3,"start":13,"end":17,"href":"https://youtu.be/0frKXR-2PBY?t=9m34s","title":"","rel":"","anchorType":0}]},{"name":"e869","type":1,"text":"learn.summary() will run a small batch of data through a model and prints out the size of tensors at every layer. As you can see, right before the Flatten layer, the tensor has the shape of 512 by 7 by 7. So if it were a rank 1 tensor (i.e. a single vector) its length will be 25088 (512 * 7 * 7)and that is why our custom header’s input size is 25088. Output size is 4 since it is the bounding box coordinates.","markups":[{"type":10,"start":0,"end":15},{"type":10,"start":147,"end":154}]},{"name":"1987","type":8,"text":"head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))\nlearn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)\nlearn.opt_fn = optim.Adam\nlearn.crit = nn.L1Loss()","markups":[]},{"name":"6c46","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*o9NFGVz1ua60kOpIafe5Hg.png","originalWidth":1440,"originalHeight":760}},{"name":"f7f3","type":13,"text":"Single object detection [10:35]","markups":[{"type":3,"start":25,"end":30,"href":"https://youtu.be/0frKXR-2PBY?t=10m35s","title":"","rel":"","anchorType":0}]},{"name":"80a2","type":1,"text":"Let’s combine the two to create something that can classify and localize the largest object in each image.","markups":[]},{"name":"8a53","type":1,"text":"There are 3 things that we need to do to train a neural network:","markups":[]},{"name":"bb44","type":10,"text":"Data","markups":[]},{"name":"b33c","type":10,"text":"Architecture","markups":[]},{"name":"62fa","type":10,"text":"Loss Function","markups":[]},{"name":"a6c0","type":13,"text":"1. Providing Data","markups":[]},{"name":"e159","type":1,"text":"We need a ModelData object whose independent variable is the images, and dependent variable is a tuple of bounding box coordinates and class label. There are several ways to do this, but here is a particularly lazy and convinient way Jeremy came up with is to create two ModelData objects representing the two different dependent variables we want (one with bounding boxes coordinates, one with classes).","markups":[{"type":10,"start":10,"end":19},{"type":10,"start":271,"end":280}]},{"name":"b667","type":8,"text":"f_model=resnet34\nsz=224\nbs=64","markups":[]},{"name":"65cf","type":8,"text":"val_idxs = get_cv_idxs(len(trn_fns))\ntfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, \n                       tfm_y=TfmType.COORD, aug_tfms=augs)","markups":[]},{"name":"3de6","type":8,"text":"md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, \n                       continuous=True, val_idxs=val_idxs)","markups":[{"type":1,"start":47,"end":53}]},{"name":"e9fc","type":8,"text":"md2 = ImageClassifierData.from_csv(PATH, JPEGS, CSV,\n                       tfms=tfms_from_model(f_model, sz))","markups":[{"type":1,"start":48,"end":51}]},{"name":"18c8","type":1,"text":"A dataset can be anything with __len__ and __getitem__. Here's a dataset that adds a 2nd label to an existing dataset:","markups":[{"type":10,"start":31,"end":38},{"type":10,"start":43,"end":54}]},{"name":"d395","type":8,"text":"class ConcatLblDataset(Dataset):\n    def __init__(self, ds, y2): self.ds,self.y2 = ds,y2\n    def __len__(self): return len(self.ds)\n    \n    def __getitem__(self, i):\n        x,y = self.ds[i]\n        return (x, (y,self.y2[i]))","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":22},{"type":1,"start":37,"end":40},{"type":1,"start":93,"end":96},{"type":1,"start":112,"end":118},{"type":1,"start":141,"end":144},{"type":1,"start":200,"end":206}]},{"name":"646c","type":9,"text":"ds : contains both independent and dependent variables","markups":[{"type":10,"start":0,"end":2}]},{"name":"8dc3","type":9,"text":"y2 : contains the additional dependent variables","markups":[{"type":10,"start":0,"end":2}]},{"name":"c9d3","type":9,"text":"(x, (y,self.y2[i])) : __getitem___ returns an independent variable and the combination of two dependent variables.","markups":[{"type":10,"start":0,"end":19},{"type":10,"start":22,"end":34}]},{"name":"8552","type":1,"text":"We’ll use it to add the classes to the bounding boxes labels.","markups":[]},{"name":"36e4","type":8,"text":"trn_ds2 = ConcatLblDataset(md.trn_ds, md2.trn_y)\nval_ds2 = ConcatLblDataset(md.val_ds, md2.val_y)","markups":[]},{"name":"a83d","type":1,"text":"Here is an example dependent variable:","markups":[]},{"name":"e61c","type":8,"text":"val_ds2[0][1]","markups":[]},{"name":"bb40","type":8,"text":"(array([   0.,   49.,  205.,  180.], dtype=float32), 14)","markups":[{"type":2,"start":0,"end":56}]},{"name":"3916","type":1,"text":"We can replace the dataloaders’ datasets with these new ones.","markups":[]},{"name":"abd7","type":8,"text":"md.trn_dl.dataset = trn_ds2\nmd.val_dl.dataset = val_ds2","markups":[]},{"name":"e63c","type":1,"text":"We have to denormalize the images from the dataloader before they can be plotted.","markups":[{"type":10,"start":11,"end":17}]},{"name":"bb3f","type":8,"text":"x,y = next(iter(md.val_dl))\nidx = 3\nima = md.val_ds.ds.denorm(to_np(x))[idx]\nb = bb_hw(to_np(y[0][idx])); b","markups":[]},{"name":"7b26","type":8,"text":"array([  52.,   38.,  106.,  184.], dtype=float32)","markups":[{"type":2,"start":0,"end":50}]},{"name":"31ab","type":8,"text":"ax = show_img(ima)\ndraw_rect(ax, b)\ndraw_text(ax, b[:2], md2.classes[y[1][idx]])","markups":[]},{"name":"442b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6QqfOpqgyRogEiTCU8WZgQ.png","originalWidth":300,"originalHeight":300}},{"name":"2513","type":13,"text":"2. Choosing Architecture [13:54]","markups":[{"type":3,"start":26,"end":31,"href":"https://youtu.be/0frKXR-2PBY?t=13m54s","title":"","rel":"","anchorType":0}]},{"name":"18d6","type":1,"text":"The architecture will be the same as the one we used for the classifier and bounding box regression, but we will just combine them. In other words, if we have c classes, then the number of activations we need in the final layer is 4 plus c. 4 for bounding box coordinates and c probabilities (one per class).","markups":[{"type":10,"start":159,"end":160},{"type":10,"start":238,"end":239},{"type":10,"start":276,"end":277}]},{"name":"f053","type":1,"text":"We’ll use an extra linear layer this time, plus some dropout, to help us train a more flexible model. In general, we want our custom head to be capable of solving the problem on its own if the pre-trained backbone it is connected to is appropriate. So in this case, we are trying to do quite a bit — classifier and bounding box regression, so just the single linear layer does not seem enough. If you were wondering why there is no BatchNorm1d after the first ReLU , ResNet backbone already has BatchNorm1d as its final layer.","markups":[{"type":10,"start":432,"end":443},{"type":10,"start":460,"end":464},{"type":10,"start":495,"end":506}]},{"name":"eb1b","type":8,"text":"head_reg4 = nn.Sequential(\n    Flatten(),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(25088,256),\n    nn.ReLU(),\n    nn.BatchNorm1d(256),\n    nn.Dropout(0.5),\n    nn.Linear(256,4+len(cats)),\n)\nmodels = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)\n\nlearn = ConvLearner(md, models)\nlearn.opt_fn = optim.Adam","markups":[{"type":1,"start":183,"end":194}]},{"name":"f6e9","type":13,"text":"3. Loss Function [15:46]","markups":[{"type":3,"start":18,"end":23,"href":"https://youtu.be/0frKXR-2PBY?t=15m46s","title":"","rel":"","anchorType":0}]},{"name":"95d7","type":1,"text":"The loss function needs to look at these 4 + len(cats) activations and decide if they are good — whether these numbers accurately reflect the position and class of the largest object in the image. We know how to do this. For the first 4 activations, we will use L1Loss just like we did before (L1Loss is like a Mean Squared Error — instead of sum of squared errors, it uses sum of absolute values). For rest of the activations, we can use cross entropy loss.","markups":[{"type":10,"start":41,"end":54}]},{"name":"7b18","type":8,"text":"def detn_loss(input, target):\n    bb_t,c_t = target\n    bb_i,c_i = input[:, :4], input[:, 4:]\n    bb_i = F.sigmoid(bb_i)*224\n    # I looked at these quantities separately first then picked a \n    # multiplier to make them approximately equal\n    return F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":246,"end":252},{"type":2,"start":125,"end":241}]},{"name":"16e0","type":8,"text":"def detn_l1(input, target):\n    bb_t,_ = target\n    bb_i = input[:, :4]\n    bb_i = F.sigmoid(bb_i)*224\n    return F.l1_loss(V(bb_i),V(bb_t)).data","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":107,"end":113}]},{"name":"f829","type":8,"text":"def detn_acc(input, target):\n    _,c_t = target\n    c_i = input[:, 4:]\n    return accuracy(c_i, c_t)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":75,"end":81}]},{"name":"4eaa","type":8,"text":"learn.crit = detn_loss\nlearn.metrics = [detn_acc, detn_l1]","markups":[]},{"name":"f337","type":9,"text":"input : activations","markups":[{"type":10,"start":0,"end":5}]},{"name":"5cb2","type":9,"text":"target : ground truth","markups":[{"type":10,"start":0,"end":6}]},{"name":"9b1e","type":9,"text":"bb_t,c_t = target : Our custom dataset returns a tuple containing bounding box coordinates and classes. This assignment will destructure them.","markups":[{"type":10,"start":0,"end":17}]},{"name":"8867","type":9,"text":"bb_i,c_i = input[:, :4], input[:, 4:] : the first : is for the batch dimension.","markups":[{"type":10,"start":0,"end":37},{"type":10,"start":50,"end":51}]},{"name":"e6c0","type":9,"text":"b_i = F.sigmoid(bb_i)*224 : we know our image is 224 by 224. Sigmoid will force it to be between 0 and 1, and multiply it by 224 to help our neural net to be in the range of what it has to be.","markups":[{"type":10,"start":0,"end":25},{"type":10,"start":61,"end":68}]},{"name":"ab55","type":1,"text":"Question: As a general rule, is it better to put BatchNorm before or after ReLU [18:02]? Jeremy would suggest to put it after a ReLU because BathNorm is meant to move towards zero-mean one-standard deviation. So if you put ReLU right after it, you are truncating it at zero so there is no way to create negative numbers. But if you put ReLU then BatchNorm, it does have that ability and gives slightly better results. Having said that, it is not too big of a deal either way. You see during this part of the course, most of the time, Jeremy does ReLU then BatchNorm but sometimes does the opposite when he wants to be consistent with the paper.","markups":[{"type":3,"start":81,"end":86,"href":"https://youtu.be/0frKXR-2PBY?t=18m2s","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":9}]},{"name":"f68d","type":1,"text":"Question: What is the intuition behind using dropout after a BatchNorm? Doesn’t BatchNorm already do a good job of regularizing [19:12]? BatchNorm does an okay job of regularizing but if you think back to part 1 when we discussed a list of things we do to avoid overfitting and adding BatchNorm is one of them as is data augmentation. But it’s perfectly possible that you’ll still be overfitting. One nice thing about dropout is that is it has a parameter to say how much to drop out. Parameters are great specifically parameters that decide how much to regularize because it lets you build a nice big over parameterized model and then decide on how much to regularize it. Jeremy tends to always put in a drop out starting with p=0 and then as he adds regularization, he can just change the dropout parameter without worrying about if he saved a model he want to be able to load it back, but if he had dropout layers in one but no in another, it will not load anymore. So this way, it stays consistent.","markups":[{"type":10,"start":728,"end":731},{"type":3,"start":129,"end":134,"href":"https://youtu.be/0frKXR-2PBY?t=19m12s","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":8}]},{"name":"c531","type":1,"text":"Now we have out inputs and targets, we can calculate the L1 loss and add the cross entropy [20:39]:","markups":[{"type":3,"start":92,"end":97,"href":"https://youtu.be/0frKXR-2PBY?t=20m39s","title":"","rel":"","anchorType":0}]},{"name":"95f1","type":1,"text":"F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20","markups":[{"type":10,"start":0,"end":52}]},{"name":"d04f","type":1,"text":"This is our loss function. Cross entropy and L1 loss may be of wildly different scales — in which case in the loss function, the larger one is going to dominate. In this case, Jeremy printed out the values and found out that if we multiply cross entropy by 20 that makes them about the same scale.","markups":[]},{"name":"eb4a","type":8,"text":"lr=1e-2\nlearn.fit(lr, 1, cycle_len=3, use_clr=(32,5))","markups":[]},{"name":"70ea","type":8,"text":"epoch      trn_loss   val_loss   detn_acc   detn_l1       \n    0      72.036466  45.186367  0.802133   32.647586 \n    1      51.037587  36.34964   0.828425   25.389733     \n    2      41.4235    35.292709  0.835637   24.343577","markups":[{"type":2,"start":0,"end":226}]},{"name":"0564","type":8,"text":"[35.292709, 0.83563701808452606, 24.343576669692993]","markups":[{"type":2,"start":0,"end":52}]},{"name":"6180","type":1,"text":"It is nice to print out information as you train, so we grabbed L1 loss and added it as metrics.","markups":[]},{"name":"06fa","type":8,"text":"learn.save('reg1_0')\nlearn.freeze_to(-2)\nlrs = np.array([lr/100, lr/10, lr])\nlearn.fit(lrs/5, 1, cycle_len=5, use_clr=(32,10))","markups":[]},{"name":"11fb","type":8,"text":"epoch      trn_loss   val_loss   detn_acc   detn_l1       \n    0      34.448113  35.972973  0.801683   22.918499 \n    1      28.889909  33.010857  0.830379   21.689888     \n    2      24.237017  30.977512  0.81881    20.817996     \n    3      21.132993  30.60677   0.83143    20.138552     \n    4      18.622983  30.54178   0.825571   19.832196","markups":[]},{"name":"0865","type":8,"text":"[30.54178, 0.82557091116905212, 19.832195997238159]","markups":[]},{"name":"c0fb","type":8,"text":"learn.unfreeze()\nlearn.fit(lrs/10, 1, cycle_len=10, use_clr=(32,10))","markups":[]},{"name":"6c87","type":8,"text":"epoch      trn_loss   val_loss   detn_acc   detn_l1       \n    0      15.957164  31.111507  0.811448   19.970753 \n    1      15.955259  32.597153  0.81235    20.111022     \n    2      15.648723  32.231941  0.804087   19.522853     \n    3      14.876172  30.93821   0.815805   19.226574     \n    4      14.113872  31.03952   0.808594   19.155093     \n    5      13.293885  29.736671  0.826022   18.761728     \n    6      12.562566  30.000023  0.827524   18.82006      \n    7      11.885125  30.28841   0.82512    18.904158     \n    8      11.498326  30.070133  0.819712   18.635296     \n    9      11.015841  30.213772  0.815805   18.551489","markups":[]},{"name":"0591","type":8,"text":"[30.213772, 0.81580528616905212, 18.551488876342773]","markups":[]},{"name":"2c47","type":1,"text":"A detection accuracy is in the low 80’s which is the same as what it was before. This is not surprising because ResNet was designed to do classification so we wouldn’t expect to be able to improve things in such a simple way. It certainly wasn’t designed to do bounding box regression. It was explicitly actually designed in such a way to not care about geometry — it takes the last 7 by 7 grid of activations and averages them all together throwing away all the information about where everything came from.","markups":[]},{"name":"684d","type":1,"text":"Interestingly, when we do accuracy (classification) and bounding box at the same time, the L1 seems a little bit better than when we just do bounding box regression [22:46]. If that is counterintuitive to you, then this would be one of the main things to think about after this lesson since it is a really important idea. The idea is this — figuring out what the main object in an image is, is kind of the hard part. Then figuring out exactly where the bounding box is and what class it is is the easy part in a way. So when you have a single network that’s both saying what is the object and where is the object, it’s going to share all the computation about finding the object. And all that shared computation is very efficient. When we back propagate the errors in the class and in the place, that’s all the information that is going to help the computation around finding the biggest object. So anytime you have multiple tasks which share some concept of what those tasks would need to do to complete their work, it is very likely they should share at least some layers of the network together. Later today, we will look at a model where most of the layers are shared except for the last one.","markups":[{"type":3,"start":166,"end":171,"href":"https://youtu.be/0frKXR-2PBY?t=22m46s","title":"","rel":"","anchorType":0}]},{"name":"acb4","type":1,"text":"Here are the result [24:34]. As before, it does a good job when there is single major object in the image.","markups":[{"type":3,"start":21,"end":26,"href":"https://youtu.be/0frKXR-2PBY?t=24m34s","title":"","rel":"","anchorType":0}]},{"name":"3d36","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*g4JAJgAcDNDikhgwtLTcwQ.png","originalWidth":2028,"originalHeight":1382}},{"name":"8223","type":3,"text":"Multi label classification [25:29]","markups":[{"type":3,"start":28,"end":33,"href":"https://youtu.be/0frKXR-2PBY?t=25m29s","title":"","rel":"","anchorType":0}]},{"name":"cf6b","type":1,"text":"Notebook","markups":[{"type":3,"start":0,"end":8,"href":"https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb","title":"","rel":"","anchorType":0}]},{"name":"5908","type":1,"text":"We want to keep building models that are slightly more complex than the last model so that if something stops working, we know exactly where it broke. Here are functions from the previous notebook:","markups":[]},{"name":"52fd","type":8,"text":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","markups":[]},{"name":"9b4a","type":8,"text":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport json, pdb\nfrom PIL import ImageDraw, ImageFont\nfrom matplotlib import patches, patheffects\ntorch.backends.cudnn.benchmark=True","markups":[{"type":1,"start":0,"end":4},{"type":1,"start":5,"end":24},{"type":1,"start":25,"end":31},{"type":1,"start":34,"end":38},{"type":1,"start":39,"end":53},{"type":1,"start":54,"end":60},{"type":1,"start":64,"end":70},{"type":1,"start":71,"end":75},{"type":1,"start":77,"end":80},{"type":1,"start":81,"end":85},{"type":1,"start":86,"end":89},{"type":1,"start":90,"end":96},{"type":1,"start":118,"end":122},{"type":1,"start":123,"end":133},{"type":1,"start":134,"end":140},{"type":1,"start":193,"end":197}]},{"name":"197a","type":13,"text":"Setup","markups":[]},{"name":"1c51","type":8,"text":"PATH = Path('data/pascal')\ntrn_j = json.load((PATH / 'pascal_train2007.json').open())\nIMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', \n                                 'categories']\nFILE_NAME,ID,IMG_ID,CAT_ID,BBOX = 'file_name','id','image_id', \n                                  'category_id','bbox'\n\ncats = dict((o[ID], o['name']) for o in trn_j[CATEGORIES])\ntrn_fns = dict((o[ID], o[FILE_NAME]) for o in trn_j[IMAGES])\ntrn_ids = [o[ID] for o in trn_j[IMAGES]]\n\nJPEGS = 'VOCdevkit/VOC2007/JPEGImages'\nIMG_PATH = PATH/JPEGS","markups":[{"type":1,"start":343,"end":346},{"type":1,"start":349,"end":351},{"type":1,"start":408,"end":411},{"type":1,"start":414,"end":416},{"type":1,"start":449,"end":452},{"type":1,"start":455,"end":457}]},{"name":"5736","type":8,"text":"def get_trn_anno():\n    trn_anno = collections.defaultdict(lambda:[])\n    for o in trn_j[ANNOTATIONS]:\n        if not o['ignore']:\n            bb = o[BBOX]\n            bb = np.array([bb[1], bb[0], bb[3]+bb[1]-1, \n                           bb[2]+bb[0]-1])\n            trn_anno[o[IMG_ID]].append((bb,o[CAT_ID]))\n    return trn_anno\n\ntrn_anno = get_trn_anno()","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":59,"end":65},{"type":1,"start":74,"end":77},{"type":1,"start":80,"end":82},{"type":1,"start":111,"end":113},{"type":1,"start":114,"end":117},{"type":1,"start":315,"end":321}]},{"name":"d612","type":8,"text":"def show_img(im, figsize=None, ax=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.set_xticks(np.linspace(0, 224, 8))\n    ax.set_yticks(np.linspace(0, 224, 8))\n    ax.grid()\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    return ax\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()])\n\ndef draw_rect(ax, b, color='white'):\n    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], \n                         fill=False, edgecolor=color, lw=2))\n    draw_outline(patch, 4)\n\ndef draw_text(ax, xy, txt, sz=14, color='white'):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color=color, fontsize=sz, \n        weight='bold')\n    draw_outline(text, 1)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":25,"end":29},{"type":1,"start":34,"end":38},{"type":1,"start":45,"end":47},{"type":1,"start":48,"end":51},{"type":1,"start":269,"end":275},{"type":1,"start":280,"end":283},{"type":1,"start":416,"end":419},{"type":1,"start":543,"end":548},{"type":1,"start":602,"end":605}]},{"name":"9675","type":8,"text":"def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1],a[2]-a[0]])\n\ndef draw_im(im, ann):\n    ax = show_img(im, figsize=(16,8))\n    for b,c in ann:\n        b = bb_hw(b)\n        draw_rect(ax, b)\n        draw_text(ax, b[:2], cats[c], sz=16)\n\ndef draw_idx(i):\n    im_a = trn_anno[i]\n    im = open_image(IMG_PATH/trn_fns[i])\n    draw_im(im, im_a)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":14,"end":20},{"type":1,"start":64,"end":67},{"type":1,"start":128,"end":131},{"type":1,"start":136,"end":138},{"type":1,"start":236,"end":239}]},{"name":"153a","type":13,"text":"Multi class [26:12]","markups":[{"type":3,"start":13,"end":18,"href":"https://youtu.be/0frKXR-2PBY?t=26m12s","title":"","rel":"","anchorType":0}]},{"name":"189d","type":8,"text":"MC_CSV = PATH/'tmp/mc.csv'","markups":[]},{"name":"9a35","type":8,"text":"trn_anno[12]","markups":[]},{"name":"a704","type":8,"text":"[(array([ 96, 155, 269, 350]), 7)]","markups":[{"type":2,"start":0,"end":34}]},{"name":"2ebe","type":8,"text":"mc = [set([cats[p[1]] for p in trn_anno[o]]) for o in trn_ids]\nmcs = [' '.join(str(p) for p in o) for o in mc]","markups":[{"type":1,"start":22,"end":25},{"type":1,"start":28,"end":30},{"type":1,"start":45,"end":48},{"type":1,"start":51,"end":53},{"type":1,"start":86,"end":89},{"type":1,"start":92,"end":94},{"type":1,"start":98,"end":101},{"type":1,"start":104,"end":106}]},{"name":"8c39","type":8,"text":"df = pd.DataFrame({'fn': [trn_fns[o] for o in trn_ids], \n                   'clas': mcs}, columns=['fn','clas'])\ndf.to_csv(MC_CSV, index=False)","markups":[{"type":1,"start":37,"end":40},{"type":1,"start":43,"end":45},{"type":1,"start":137,"end":142}]},{"name":"512b","type":1,"text":"One of the students pointed out that by using Pandas, we can do things much simpler than using collections.defaultdict and shared this gist. The more you get to know Pandas, the more often you realize it is a good way to solve lots of different problems.","markups":[{"type":10,"start":95,"end":118},{"type":3,"start":130,"end":139,"href":"https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed","title":"","rel":"","anchorType":0}]},{"name":"2dc8","type":1,"text":"Question: When you are incrementally building on top of smaller models, do you reuse them as pre-trained weights? or do you toss it away then retrain from scratch [27:11]? When Jeremy is figuring stuff out as he goes like this, he would generally lean towards tossing away because reusing pre-trained weights introduces unnecessary complexities. However, if he is trying to get to a point where he can train on really big images, he will generally start on much smaller and often re-use these weights.","markups":[{"type":3,"start":164,"end":169,"href":"https://youtu.be/0frKXR-2PBY?t=27m11s","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":8}]},{"name":"5b68","type":8,"text":"f_model=resnet34\nsz=224\nbs=64","markups":[]},{"name":"a183","type":8,"text":"tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, MC_CSV, tfms=tfms)","markups":[]},{"name":"99ad","type":8,"text":"learn = ConvLearner.pretrained(f_model, md)\nlearn.opt_fn = optim.Adam","markups":[]},{"name":"cc2f","type":8,"text":"lr = 2e-2","markups":[]},{"name":"17d6","type":8,"text":"learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))","markups":[]},{"name":"560a","type":8,"text":"epoch      trn_loss   val_loss   \x3clambda\x3e                  \n    0      0.104836   0.085015   0.972356  \n    1      0.088193   0.079739   0.972461                   \n    2      0.072346   0.077259   0.974114","markups":[{"type":2,"start":0,"end":206}]},{"name":"60c4","type":8,"text":"[0.077258907, 0.9741135761141777]","markups":[{"type":2,"start":0,"end":33}]},{"name":"a327","type":8,"text":"lrs = np.array([lr/100, lr/10, lr])","markups":[]},{"name":"5faa","type":8,"text":"learn.freeze_to(-2)","markups":[]},{"name":"d590","type":8,"text":"learn.fit(lrs/10, 1, cycle_len=5, use_clr=(32,5))","markups":[]},{"name":"fd47","type":8,"text":"epoch      trn_loss   val_loss   \x3clambda\x3e                   \n    0      0.063236   0.088847   0.970681  \n    1      0.049675   0.079885   0.973723                   \n    2      0.03693    0.076906   0.975601                   \n    3      0.026645   0.075304   0.976187                   \n    4      0.018805   0.074934   0.975165","markups":[{"type":2,"start":0,"end":329}]},{"name":"5ca7","type":8,"text":"[0.074934497, 0.97516526281833649]","markups":[{"type":2,"start":0,"end":34}]},{"name":"e973","type":8,"text":"learn.save('mclas')","markups":[]},{"name":"2834","type":8,"text":"learn.load('mclas')","markups":[]},{"name":"8c3b","type":8,"text":"y = learn.predict()\nx,_ = next(iter(md.val_dl))\nx = to_np(x)","markups":[]},{"name":"5b1f","type":8,"text":"fig, axes = plt.subplots(3, 4, figsize=(12, 8))\nfor i,ax in enumerate(axes.flat):\n    ima=md.val_ds.denorm(x)[i]\n    ya = np.nonzero(y[i]\x3e0.4)[0]\n    b = '\\n'.join(md.classes[o] for o in ya)\n    ax = show_img(ima, ax=ax)\n    draw_text(ax, (0,0), b)\nplt.tight_layout()","markups":[{"type":1,"start":48,"end":51},{"type":1,"start":57,"end":59},{"type":1,"start":155,"end":157},{"type":1,"start":178,"end":181},{"type":1,"start":184,"end":186}]},{"name":"76a2","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2m1Qoq3NhsqdYBd4hUTR6A.png","originalWidth":2038,"originalHeight":1404}},{"name":"4bec","type":1,"text":"Multi-class classification is pretty straight forward [28:28]. One minor tweak is the use of set in this line so that each object type appear once.:","markups":[{"type":10,"start":93,"end":96},{"type":3,"start":55,"end":60,"href":"https://youtu.be/0frKXR-2PBY?t=28m28s","title":"","rel":"","anchorType":0}]},{"name":"de81","type":8,"text":"mc = [set([cats[p[1]] for p in trn_anno[o]]) for o in trn_ids]","markups":[{"type":1,"start":6,"end":9},{"type":1,"start":22,"end":25},{"type":1,"start":28,"end":30},{"type":1,"start":45,"end":48},{"type":1,"start":51,"end":53}]},{"name":"f2f7","type":13,"text":"SSD and YOLO [29:10]","markups":[{"type":3,"start":14,"end":19,"href":"https://youtu.be/0frKXR-2PBY?t=29m10s","title":"","rel":"","anchorType":0}]},{"name":"4997","type":1,"text":"We have an input image that goes through a conv net which outputs a vector of size 4+c where c=len(cats) . This gives us an object detector for a single largest object. Let’s now create one that finds 16 objects. The obvious way to do this would be to take the last linear layer and rather than having 4+c outputs, we could have 16x(4+c) outputs. This gives us 16 sets of class probabilities and 16 sets of bounding box coordinates. Then we would just need a loss function that will check whether those 16 sets of bounding boxes correctly represented the up to 16 objects in the image (we will go into the loss function later).","markups":[{"type":10,"start":83,"end":86},{"type":10,"start":93,"end":104},{"type":10,"start":302,"end":305},{"type":10,"start":329,"end":337}]},{"name":"09c8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fPHmCosDHcrHmtKvWFK9Mg.png","originalWidth":500,"originalHeight":423}},{"name":"8cbc","type":1,"text":"The second way to do this is rather than using nn.linear, what if instead, we took from our ResNet convolutional backbone and added an nn.Conv2d with stride 2 [31:32]? This will give us a 4x4x[# of filters] tensor — here let’s make it 4x4x(4+c) so that we get a tensor where the number of elements is exactly equal to the number of elements we wanted. Now if we created a loss function that took a 4x4x(4+c) tensor and and mapped it to 16 objects in the image and checked whether each one was correctly represented by these 4+c activations, this would work as well. It turns out, both of these approaches are actually used [33:48]. The approach where the output is one big long vector from a fully connected linear layer is used by a class of models known as YOLO (You Only Look Once), where else, the approach of the convolutional activations is used by models which started with something called SSD (Single Shot Detector). Since these things came out very similar times in late 2015, things are very much moved towards SSD. So the point where this morning, YOLO version 3 came out and is now doing SSD, so that’s what we are going to do. We will also learn about why this makes more sense as well.","markups":[{"type":10,"start":47,"end":56},{"type":10,"start":135,"end":144},{"type":10,"start":188,"end":206},{"type":10,"start":235,"end":244},{"type":10,"start":398,"end":407},{"type":10,"start":524,"end":527},{"type":3,"start":160,"end":165,"href":"https://youtu.be/0frKXR-2PBY?t=31m32s","title":"","rel":"","anchorType":0},{"type":3,"start":624,"end":629,"href":"https://youtu.be/0frKXR-2PBY?t=33m48s","title":"","rel":"","anchorType":0},{"type":3,"start":759,"end":784,"href":"https://arxiv.org/abs/1506.02640","title":"","rel":"noopener","anchorType":0},{"type":3,"start":898,"end":924,"href":"https://arxiv.org/abs/1512.02325","title":"","rel":"noopener","anchorType":0},{"type":3,"start":1060,"end":1074,"href":"https://pjreddie.com/media/files/papers/YOLOv3.pdf","title":"","rel":"noopener","anchorType":0}]},{"name":"20e9","type":13,"text":"Anchor boxes [35:04]","markups":[{"type":3,"start":14,"end":19,"href":"https://youtu.be/0frKXR-2PBY?t=35m04s","title":"","rel":"","anchorType":0}]},{"name":"42e6","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*8kpDP3FZFxW99IUQE0C8Xw.png","originalWidth":500,"originalHeight":428}},{"name":"4bfa","type":1,"text":"Let’s imagine that we had another Conv2d(stride=2) then we would have 2x2x(4+c) tensor. Basically, it is creating a grid that looks something like this:","markups":[{"type":10,"start":34,"end":50},{"type":10,"start":70,"end":79}]},{"name":"37fd","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*uA-oJok4-Rng6mnHOOPyNQ.png","originalWidth":965,"originalHeight":545}},{"name":"c44d","type":1,"text":"This is how the geometry of the activations of the second extra convolutional stride 2 layer are. Remember, stride 2 convolution does the same thing to the geometry of the activations as a stride 1 convolution followed by maxpooling assuming the padding is ok.","markups":[]},{"name":"fd03","type":1,"text":"Let’s talk about what we might do here [36:09]. We want each of these grid cell to be responsible for finding the largest object in that part of the image.","markups":[{"type":3,"start":40,"end":45,"href":"https://youtu.be/0frKXR-2PBY?t=36m9s","title":"","rel":"","anchorType":0}]},{"name":"3bae","type":13,"text":"Receptive Field [37:20]","markups":[{"type":3,"start":17,"end":22,"href":"https://youtu.be/0frKXR-2PBY?t=37m20s","title":"","rel":"","anchorType":0}]},{"name":"daf5","type":1,"text":"Why do we care about the idea that we would like each convolutional grid cell to be responsible for finding things that are in the corresponding part of the image? The reason is because of something called the receptive field of that convolutional grid cell. The basic idea is that throughout your convolutional layers, every piece of those tensors has a receptive field which means which part of the input image was responsible for calculating that cell. Like all things in life, the easiest way to see this is with Excel [38:01].","markups":[{"type":3,"start":524,"end":529,"href":"https://youtu.be/0frKXR-2PBY?t=38m1s","title":"","rel":"","anchorType":0}]},{"name":"4875","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*IgL2CMSit3Hh9N2Fq2Zlgg.png","originalWidth":1779,"originalHeight":1009}},{"name":"d0cd","type":1,"text":"Take a single activation (in this case in the maxpool layer)and let’s see where it came from [38:45]. In excel you can do Formulas → Trace Precedents. Tracing all the way back to the input layer, you can see that it came from this 6 x 6 portion of the image (as well as filters). What is more, the middle portion has lots of weights coming out of where else cells in the outside only have one weight coming out. So we call this 6 x 6 cells the receptive field of the one activation we picked.","markups":[{"type":3,"start":94,"end":99,"href":"https://youtu.be/0frKXR-2PBY?t=38m45s","title":"","rel":"","anchorType":0}]},{"name":"719f","type":4,"text":"3x3 convolution with opacity 15% — clearly the center of the box has more dependencies","markups":[],"layout":1,"metadata":{"id":"1*cCBVbJ2WjiPMlqX4nA2bwA.png","originalWidth":236,"originalHeight":233}},{"name":"10e4","type":1,"text":"Note that the receptive field is not just saying it’s this box but also that the center of the box has more dependencies [40:27] This is a critically important concept when it comes to understanding architectures and understanding why conv nets work the way they do.","markups":[{"type":3,"start":122,"end":127,"href":"https://youtu.be/0frKXR-2PBY?t=40m27s","title":"","rel":"","anchorType":0}]},{"name":"f66c","type":13,"text":"Architecture [41:18]","markups":[{"type":3,"start":14,"end":19,"href":"https://youtu.be/0frKXR-2PBY?t=41m18s","title":"","rel":"","anchorType":0}]},{"name":"19c4","type":1,"text":"The architecture is, we will have a ResNet backbone followed by one or more 2D convolutions (one for now) which is going to give us a 4x4 grid.","markups":[{"type":10,"start":134,"end":137}]},{"name":"ddb9","type":8,"text":"class StdConv(nn.Module):\n    def __init__(self, nin, nout, stride=2, drop=0.1):\n        super().__init__()\n        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, \n                              padding=1)\n        self.bn = nn.BatchNorm2d(nout)\n        self.drop = nn.Dropout(drop)\n        \n    def forward(self, x): \n        return self.drop(self.bn(F.relu(self.conv(x))))\n        \ndef flatten_conv(x,k):\n    bs,nf,gx,gy = x.size()\n    x = x.permute(0,2,3,1).contiguous()\n    return x.view(bs,-1,nf//k)","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":13},{"type":1,"start":30,"end":33},{"type":1,"start":298,"end":301},{"type":1,"start":329,"end":335},{"type":1,"start":386,"end":389},{"type":1,"start":480,"end":486}]},{"name":"fa2e","type":8,"text":"class OutConv(nn.Module):\n    def __init__(self, k, nin, bias):\n        super().__init__()\n        self.k = k\n        self.oconv1 = nn.Conv2d(nin, (len(id2cat)+1)*k, 3, \n                                padding=1)\n        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)\n        self.oconv1.bias.data.zero_().add_(bias)\n        \n    def forward(self, x):\n        return [flatten_conv(self.oconv1(x), self.k),\n                flatten_conv(self.oconv2(x), self.k)]","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":13},{"type":1,"start":30,"end":33},{"type":1,"start":331,"end":334},{"type":1,"start":361,"end":367}]},{"name":"d08c","type":8,"text":"class SSD_Head(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(0.25)\n        self.sconv0 = StdConv(512,256, stride=1)\n        self.sconv2 = StdConv(256,256)\n        self.out = OutConv(k, 256, bias)\n        \n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv2(x)\n        return self.out(x)\n\nhead_reg4 = SSD_Head(k, -3.)\nmodels = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)\nlearn = ConvLearner(md, models)\nlearn.opt_fn = optim.Adam","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":14},{"type":1,"start":31,"end":34},{"type":1,"start":266,"end":269},{"type":1,"start":383,"end":389}]},{"name":"2589","type":1,"text":"SSD_Head","markups":[{"type":1,"start":0,"end":8}]},{"name":"62b3","type":10,"text":"We start with ReLU and dropout","markups":[]},{"name":"dbc8","type":10,"text":"Then stride 1 convolution. The reason we start with a stride 1 convolution is because that does not change the geometry at all — it just lets us add an extra layer of calculation. It lets us create not just a linear layer but now we have a little mini neural network in our custom head. StdConv is defined above — it does convolution, ReLU, BatchNorm, and dropout. Most research code you see won’t define a class like this, instead they write the entire thing again and again. Don’t be like that. Duplicate code leads to errors and poor understanding.","markups":[{"type":10,"start":287,"end":294}]},{"name":"74ad","type":10,"text":"Stride 2 convolution [44:56]","markups":[{"type":3,"start":22,"end":27,"href":"https://youtu.be/0frKXR-2PBY?t=44m56s","title":"","rel":"","anchorType":0}]},{"name":"b4de","type":10,"text":"At the end, the output of step 3 is 4x4 which gets passed to OutConv. OutConv has two separate convolutional layers each of which is stride 1 so it is not changing the geometry of the input. One of them is of length of the number of classes (ignore k for now and +1 is for “background” — i.e. no object was detected), the other’s length is 4. Rather than having a single conv layer that outputs 4+c, let’s have two conv layers and return their outputs in a list. This allows these layers to specialize just a little bit. We talked about this idea that when you have multiple tasks, they can share layers, but they do not have to share all the layers. In this case, our two tasks of creating a classifier and creating and creating bounding box regression share every single layers except the very last one.","markups":[{"type":10,"start":36,"end":39},{"type":10,"start":61,"end":68},{"type":10,"start":70,"end":77},{"type":10,"start":249,"end":250},{"type":10,"start":263,"end":265},{"type":10,"start":395,"end":398}]},{"name":"7bcd","type":10,"text":"At the end, we flatten out the convolution because Jeremy wrote the loss function to expect flattened out tensor, but we could totally rewrite it to not do that.","markups":[]},{"name":"d6b4","type":13,"text":"Fastai Coding Style [42:58]","markups":[{"type":3,"start":0,"end":19,"href":"https://github.com/fastai/fastai/blob/master/docs/style.md","title":"","rel":"noopener","anchorType":0},{"type":3,"start":21,"end":26,"href":"https://youtu.be/0frKXR-2PBY?t=42m58s","title":"","rel":"","anchorType":0}]},{"name":"6d61","type":1,"text":"The first draft was released this week. It is very heavily orient towards the idea of expository programming which is the idea that programming code should be something that you can use to explain an idea, ideally as readily as mathematical notation, to somebody that understands your coding method. The idea goes back a very long way, but it was best described in the Turing Award lecture of 1979 by probably Jeremy’s greatest computer science hero Ken Iverson. He had been working on it since well before 1964 but 1964 was the first example of this approach of programming he released which is called APL and 25 years later, he won the Turing Award. He then passed on the baton to his son Eric Iverson. Fastai style guide is an attempt at taking some of these ideas.","markups":[]},{"name":"2d44","type":13,"text":"Loss Function [47:44]","markups":[{"type":3,"start":15,"end":20,"href":"https://youtu.be/0frKXR-2PBY?t=47m44s","title":"","rel":"","anchorType":0}]},{"name":"aad3","type":1,"text":"The loss function needs to look at each of these 16 sets of activations, each of which has four bounding box coordinates and c+1 class probabilities and decide if those activations are close or far away from the object which is the closest to this grid cell in the image. If nothing is there, then whether it is predicting background correctly. That turns out to be very hard to do.","markups":[{"type":10,"start":125,"end":128}]},{"name":"339d","type":13,"text":"Matching Problem [48:43]","markups":[{"type":3,"start":18,"end":23,"href":"https://youtu.be/0frKXR-2PBY?t=48m43s","title":"","rel":"","anchorType":0}]},{"name":"b566","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*2dqj3hivcOF6ThoL-nhMyA.png","originalWidth":540,"originalHeight":541}},{"name":"6b0b","type":1,"text":"The loss function needs to take each of the objects in the image and match them to one of these convolutional grid cells to say “this grid cell is responsible for this particular object” so then it can go ahead and say “okay, how close are the 4 coordinates and how close are the class probabilities.","markups":[]},{"name":"8e9f","type":1,"text":"Here is our goal [49:56]:","markups":[{"type":3,"start":18,"end":23,"href":"https://youtu.be/0frKXR-2PBY?t=49m56s","title":"","rel":"","anchorType":0}]},{"name":"f181","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*8M9x-WgHNasmuLSJNbKoaQ.png","originalWidth":1168,"originalHeight":335}},{"name":"3bda","type":1,"text":"Our dependent variable looks like the one on the left, and our final convolutional layer is going to be 4x4x(c+1) in this case c=20. We then flatten that out into a vector. Our goal is to come up with a function which takes in a dependent variable and also some particular set of activations that ended up coming out of the model and returns a higher number if these activations are not a good reflection of the ground truth bounding boxes; or a lower number if it is a good reflection.","markups":[{"type":10,"start":104,"end":113},{"type":10,"start":127,"end":131}]},{"name":"308c","type":13,"text":"Testing [51:58]","markups":[{"type":3,"start":9,"end":14,"href":"https://youtu.be/0frKXR-2PBY?t=51m58s","title":"","rel":"","anchorType":0}]},{"name":"986e","type":8,"text":"x,y = next(iter(md.val_dl))\nx,y = V(x),V(y)\nlearn.model.eval()\nbatch = learn.model(x)\nb_clas,b_bb = batch\nb_clas.size(),b_bb.size()","markups":[]},{"name":"209c","type":8,"text":"(torch.Size([64, 16, 21]), torch.Size([64, 16, 4]))","markups":[{"type":2,"start":0,"end":51}]},{"name":"992c","type":1,"text":"Make sure these shapes make sense. Let’s now look at the ground truth y [53:24]:","markups":[{"type":10,"start":70,"end":71},{"type":3,"start":73,"end":78,"href":"https://youtu.be/0frKXR-2PBY?t=53m24s","title":"","rel":"","anchorType":0}]},{"name":"5514","type":8,"text":"idx=7\nb_clasi = b_clas[idx]\nb_bboxi = b_bb[idx]\nima=md.val_ds.ds.denorm(to_np(x))[idx]\nbbox,clas = get_y(y[0][idx], y[1][idx])\nbbox,clas","markups":[]},{"name":"b746","type":8,"text":"(Variable containing:\n  0.6786  0.4866  0.9911  0.6250\n  0.7098  0.0848  0.9911  0.5491\n  0.5134  0.8304  0.6696  0.9063\n [torch.cuda.FloatTensor of size 3x4 (GPU 0)], Variable containing:\n   8\n  10\n  17\n [torch.cuda.LongTensor of size 3 (GPU 0)])","markups":[{"type":2,"start":0,"end":247}]},{"name":"a6a5","type":1,"text":"Note that bounding box coordinates have been scaled to between 0 and 1 — basically we are treating the image as being 1x1, so they are relative to the size of the image.","markups":[]},{"name":"b788","type":1,"text":"We already have show_ground_truth function. This torch_gt (gt: ground truth) function simply converts tensors into numpy array.","markups":[{"type":10,"start":16,"end":33},{"type":10,"start":49,"end":57}]},{"name":"d582","type":8,"text":"def torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n    return show_ground_truth(ax, ima, to_np((bbox*224).long()),\n         to_np(clas), \n         to_np(prs) if prs is not None else None, thresh)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":38,"end":42},{"type":1,"start":61,"end":67},{"type":1,"start":164,"end":166},{"type":1,"start":171,"end":173},{"type":1,"start":174,"end":177},{"type":1,"start":178,"end":182},{"type":1,"start":183,"end":187},{"type":1,"start":188,"end":192}]},{"name":"4392","type":8,"text":"fig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, bbox, clas)","markups":[]},{"name":"d862","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*Q3ZtSRtk-a2OwKfE1wa5zw.png","originalWidth":401,"originalHeight":401}},{"name":"68a9","type":1,"text":"The above is a ground truth. Here is our 4x4 grid cells from our final convolutional layer [54:44]:","markups":[{"type":10,"start":41,"end":44},{"type":3,"start":92,"end":97,"href":"https://youtu.be/0frKXR-2PBY?t=54m44s","title":"","rel":"","anchorType":0}]},{"name":"1592","type":8,"text":"fig, ax = plt.subplots(figsize=(7,7))\ntorch_gt(ax, ima, anchor_cnr, b_clasi.max(1)[1])","markups":[]},{"name":"9b0b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*xjKmShqdLnD_JX4Aj7U80g.png","originalWidth":401,"originalHeight":401}},{"name":"930c","type":1,"text":"Each of these square boxes, different papers call them different things. The three terms you’ll hear are: anchor boxes, prior boxes, or default boxes. We will stick with the term anchor boxes.","markups":[]},{"name":"e77b","type":1,"text":"What we are going to do for this loss function is we are going to go through a matching problem where we are going to take every one of these 16 boxes and see which one of these three ground truth objects has the highest amount of overlap with a given square [55:21]. To do this, we have to have some way of measuring amount of overlap and a standard function for this is called Jaccard index (IoU).","markups":[{"type":3,"start":260,"end":265,"href":"https://youtu.be/0frKXR-2PBY?t=55m21s","title":"","rel":"","anchorType":0},{"type":3,"start":379,"end":392,"href":"https://en.wikipedia.org/wiki/Jaccard_index","title":"","rel":"noopener","anchorType":0}]},{"name":"7378","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*10ORjq4HuOc0umcnojiDPA.png","originalWidth":300,"originalHeight":234}},{"name":"3347","type":1,"text":"We are going to go through and find the Jaccard overlap for each one of the three objects versus each of the 16 anchor boxes [57:11]. That is going to give us a 3x16 matrix.","markups":[{"type":10,"start":161,"end":165},{"type":3,"start":126,"end":131,"href":"https://youtu.be/0frKXR-2PBY?t=57m11s","title":"","rel":"","anchorType":0}]},{"name":"3634","type":1,"text":"Here are the coordinates of all of our anchor boxes (centers, height, width):","markups":[{"type":2,"start":13,"end":24}]},{"name":"4953","type":8,"text":"anchors","markups":[]},{"name":"2863","type":8,"text":"Variable containing:\n 0.1250  0.1250  0.2500  0.2500\n 0.1250  0.3750  0.2500  0.2500\n 0.1250  0.6250  0.2500  0.2500\n 0.1250  0.8750  0.2500  0.2500\n 0.3750  0.1250  0.2500  0.2500\n 0.3750  0.3750  0.2500  0.2500\n 0.3750  0.6250  0.2500  0.2500\n 0.3750  0.8750  0.2500  0.2500\n 0.6250  0.1250  0.2500  0.2500\n 0.6250  0.3750  0.2500  0.2500\n 0.6250  0.6250  0.2500  0.2500\n 0.6250  0.8750  0.2500  0.2500\n 0.8750  0.1250  0.2500  0.2500\n 0.8750  0.3750  0.2500  0.2500\n 0.8750  0.6250  0.2500  0.2500\n 0.8750  0.8750  0.2500  0.2500\n[torch.cuda.FloatTensor of size 16x4 (GPU 0)]","markups":[{"type":2,"start":0,"end":578}]},{"name":"9f3a","type":1,"text":"Here are the amount of overlap between 3 ground truth objects and 16 anchor boxes:","markups":[]},{"name":"f523","type":8,"text":"overlaps = jaccard(bbox.data, anchor_cnr.data)\noverlaps","markups":[]},{"name":"09bc","type":8,"text":"Columns 0 to 7   \n0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000      ","markups":[]},{"name":"888b","type":8,"text":"Columns 8 to 15   \n0.0000  0.0091 0.0922  0.0000  0.0000  0.0315  0.3985  0.0000  0.0356  0.0549 0.0103  0.0000  0.2598  0.4538  0.0653  0.0000  0.0000  0.0000 0.0000  0.1897  0.0000  0.0000  0.0000  0.0000 [torch.cuda.FloatTensor of size 3x16 (GPU 0)]","markups":[]},{"name":"8142","type":1,"text":"What we could do now is we could take the max of dimension 1 (row-wise) which will tell us for each ground truth object, what the maximum amount that overlaps with some grid cell as well as the index:","markups":[]},{"name":"197d","type":8,"text":"overlaps.max(1)","markups":[]},{"name":"5e49","type":8,"text":"(\n  0.3985\n  0.4538\n  0.1897\n [torch.cuda.FloatTensor of size 3 (GPU 0)], \n  14\n  13\n  11\n [torch.cuda.LongTensor of size 3 (GPU 0)])","markups":[{"type":2,"start":0,"end":133}]},{"name":"fdce","type":1,"text":"We will also going to look at max over a dimension 0 (column-wise) which will tell us what is the maximum amount of overlap for each grid cell across all of the ground truth objects [59:08]:","markups":[{"type":3,"start":183,"end":188,"href":"https://youtu.be/0frKXR-2PBY?t=59m8s","title":"","rel":"","anchorType":0}]},{"name":"df2e","type":8,"text":"overlaps.max(0)","markups":[]},{"name":"757d","type":8,"text":"(\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0356\n  0.0549\n  0.0922\n  0.1897\n  0.2598\n  0.4538\n  0.3985\n  0.0000\n [torch.cuda.FloatTensor of size 16 (GPU 0)], \n  0\n  0\n  0\n  0\n  0\n  0\n  0\n  0\n  1\n  1\n  0\n  2\n  1\n  1\n  0\n  0\n [torch.cuda.LongTensor of size 16 (GPU 0)])","markups":[{"type":2,"start":0,"end":301}]},{"name":"5f19","type":1,"text":"What is particularly interesting here is that it tells us for every grid cell what is the index of the ground truth object which overlaps with it the most. Zero is a bit overloaded here — zero could either mean the amount of overlap was zero or its largest overlap is with object index zero. It is going to turn out not to matter but just FYI.","markups":[]},{"name":"d4fd","type":1,"text":"There is a function called map_to_ground_truth which we will not worry about for now [59:57]. It is super simple code but it is slightly awkward to think about. Basically what it does is it combines these two sets of overlaps in a way described in the SSD paper to assign every anchor box to a ground truth object. The way it assign that is each of the three (row-wise max) gets assigned as is. For the rest of the anchor boxes, they get assigned to anything which they have an overlap of at least 0.5 with (column-wise). If neither applies, it is considered to be a cell which contains background.","markups":[{"type":10,"start":27,"end":46},{"type":3,"start":86,"end":91,"href":"https://youtu.be/0frKXR-2PBY?t=59m57s","title":"","rel":"","anchorType":0}]},{"name":"367d","type":8,"text":"gt_overlap,gt_idx = map_to_ground_truth(overlaps)\ngt_overlap,gt_idx","markups":[]},{"name":"31ef","type":8,"text":"(\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0000\n  0.0356\n  0.0549\n  0.0922\n  1.9900\n  0.2598\n  1.9900\n  1.9900\n  0.0000\n [torch.cuda.FloatTensor of size 16 (GPU 0)], \n  0\n  0\n  0\n  0\n  0\n  0\n  0\n  0\n  1\n  1\n  0\n  2\n  1\n  1\n  0\n  0\n [torch.cuda.LongTensor of size 16 (GPU 0)])","markups":[{"type":2,"start":0,"end":301}]},{"name":"66c7","type":1,"text":"Now you can see a list of all the assignments [1:01:05]. Anywhere that has gt_overlap \x3c 0.5 gets assigned background. The three row-wise max anchor box has high number to force the assignments. Now we can combine these values to classes:","markups":[{"type":10,"start":75,"end":91},{"type":3,"start":47,"end":54,"href":"https://youtu.be/0frKXR-2PBY?t=1h1m5s","title":"","rel":"","anchorType":0}]},{"name":"313f","type":8,"text":"gt_clas = clas[gt_idx]; gt_clas","markups":[]},{"name":"cfab","type":8,"text":"Variable containing:\n  8\n  8\n  8\n  8\n  8\n  8\n  8\n  8\n 10\n 10\n  8\n 17\n 10\n 10\n  8\n  8\n[torch.cuda.LongTensor of size 16 (GPU 0)]","markups":[{"type":2,"start":0,"end":127}]},{"name":"73df","type":1,"text":"Then add a threshold and finally comes up with the three classes that are being predicted:","markups":[]},{"name":"9817","type":8,"text":"thresh = 0.5\npos = gt_overlap \x3e thresh\npos_idx = torch.nonzero(pos)[:,0]\nneg_idx = torch.nonzero(1-pos)[:,0]\npos_idx","markups":[]},{"name":"0b8b","type":8,"text":" 11\n 13\n 14\n[torch.cuda.LongTensor of size 3 (GPU 0)]","markups":[{"type":2,"start":0,"end":53}]},{"name":"3aa2","type":1,"text":"And here are what each of these anchor boxes is meant to be predicting:","markups":[]},{"name":"3a09","type":8,"text":"gt_clas[1-pos] = len(id2cat)\n[id2cat[o] if o\x3clen(id2cat) else 'bg' for o in gt_clas.data]","markups":[{"type":1,"start":40,"end":42},{"type":1,"start":57,"end":61},{"type":1,"start":67,"end":70},{"type":1,"start":73,"end":75}]},{"name":"6a65","type":8,"text":"['bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'bg',\n 'sofa',\n 'bg',\n 'diningtable',\n 'chair',\n 'bg']","markups":[{"type":2,"start":0,"end":125}]},{"name":"c233","type":1,"text":"So that was the matching stage [1:02:29]. For L1 loss, we can:","markups":[{"type":3,"start":32,"end":39,"href":"https://youtu.be/0frKXR-2PBY?t=1h2m29s","title":"","rel":"","anchorType":0}]},{"name":"1dc4","type":10,"text":"take the activations which matched (pos_idx = [11, 13, 14])","markups":[{"type":10,"start":36,"end":58}]},{"name":"9131","type":10,"text":"subtract from those the ground truth bounding boxes","markups":[]},{"name":"9713","type":10,"text":"take the absolute value of the difference","markups":[]},{"name":"7f64","type":10,"text":"take the mean of that.","markups":[]},{"name":"483c","type":1,"text":"For classifications, we can just do a cross entropy","markups":[]},{"name":"4c9b","type":8,"text":"gt_bbox = bbox[gt_idx]\nloc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\nclas_loss  = F.cross_entropy(b_clasi, gt_clas)\nloc_loss,clas_loss","markups":[]},{"name":"0d14","type":8,"text":"(Variable containing:\n 1.00000e-02 *\n   6.5887\n [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:\n  1.0331\n [torch.cuda.FloatTensor of size 1 (GPU 0)])","markups":[{"type":2,"start":0,"end":166}]},{"name":"8150","type":1,"text":"We will end up with 16 predicted bounding boxes, most of them will be background. If you are wondering what it predicts in terms of bounding box of background, the answer is it totally ignores it.","markups":[]},{"name":"7536","type":8,"text":"fig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima=md.val_ds.ds.denorm(to_np(x))[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    ima=md.val_ds.ds.denorm(to_np(x))[idx]\n    bbox,clas = get_y(bbox,clas); bbox,clas\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], \n             b_clas[idx].max(1)[0].sigmoid(), 0.01)\nplt.tight_layout()","markups":[{"type":1,"start":49,"end":52},{"type":1,"start":60,"end":62}]},{"name":"bcf4","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*8azTUd1Ujf3FQSMBwIXgAw.png","originalWidth":1181,"originalHeight":866}},{"name":"558e","type":13,"text":"Tweak 1. How do we interpret the activations [1:04:16]?","markups":[{"type":3,"start":46,"end":53,"href":"https://youtu.be/0frKXR-2PBY?t=1h4m16s","title":"","rel":"","anchorType":0}]},{"name":"cc2a","type":1,"text":"The way we interpret the activation is defined here:","markups":[]},{"name":"b02b","type":8,"text":"def actn_to_bb(actn, anchors):\n    actn_bbs = torch.tanh(actn)\n    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]\n    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]\n    return hw2corners(actn_centers, actn_hw)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":185,"end":191}]},{"name":"2a9b","type":1,"text":"We grab the activations, we stick them through tanh (remember tanh is the same shape as sigmoid except it is scaled to be between -1 and 1) which forces it to be within that range. We then grab the actual position of the anchor boxes, and we will move them around according to the value of the activations divided by two (actn_bbs[:,:2]/2). In other words, each predicted bounding box can be moved by up to 50% of a grid size from where its default position is. Ditto for its height and width — it can be up to twice as big or half as big as its default size.","markups":[{"type":10,"start":47,"end":51},{"type":10,"start":62,"end":66},{"type":10,"start":322,"end":338}]},{"name":"d59f","type":13,"text":"Tweak 2. We actually use binary cross entropy loss instead of cross entropy [1:05:36]","markups":[{"type":3,"start":77,"end":84,"href":"https://youtu.be/0frKXR-2PBY?t=1h5m36s","title":"","rel":"","anchorType":0}]},{"name":"960e","type":8,"text":"class BCE_Loss(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, pred, targ):\n        t = one_hot_embedding(targ, self.num_classes+1)\n        t = V(t[:,:-1].contiguous())#.cpu()\n        x = pred[:,:-1]\n        w = self.get_weight(x,t)\n        return F.binary_cross_entropy_with_logits(x, t, w, \n                            size_average=False)/self.num_classes\n    \n    def get_weight(self,x,t): return None","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":14},{"type":1,"start":31,"end":34},{"type":1,"start":135,"end":138},{"type":1,"start":331,"end":337},{"type":1,"start":424,"end":429},{"type":1,"start":457,"end":460},{"type":1,"start":483,"end":489},{"type":1,"start":490,"end":494},{"type":2,"start":258,"end":265}]},{"name":"820e","type":1,"text":"Binary cross entropy is what we normally use for multi-label classification. Like in the planet satellite competition, each satellite image could have multiple things. If it has multiple things in it, you cannot use softmax because softmax really encourages just one thing to have the high number. In our case, each anchor box can only have one object associated with it, so it is not for that reason that we are avoiding softmax. It is something else — which is it is possible for an anchor box to have nothing associated with it. There are two ways to handle this idea of “background”; one would be to say background is just a class, so let’s use softmax and just treat background as one of the classes that the softmax could predict. A lot of people have done it this way. But that is a really hard thing to ask neural network to do [1:06:52] — it is basically asking whether this grid cell does not have any of the 20 objects that I am interested with Jaccard overlap of more than 0.5. It is a really hard to thing to put into a single computation. On the other hand, what if we just asked for each class; “is it a motorbike?” “is it a bus?”, “ is it a person?” etc and if all the answer is no, consider that background. That is the way we do it here. It is not that we can have multiple true labels, but we can have zero.","markups":[{"type":3,"start":837,"end":844,"href":"https://youtu.be/0frKXR-2PBY?t=1h5m52s","title":"","rel":"","anchorType":0}]},{"name":"29b4","type":1,"text":"In forward :","markups":[{"type":10,"start":3,"end":10}]},{"name":"482a","type":10,"text":"First we take the one hot embedding of the target (at this stage, we do have the idea of background)","markups":[]},{"name":"0e7c","type":10,"text":"Then we remove the background column (the last one) which results in a vector either of all zeros or one one.","markups":[]},{"name":"7ba1","type":10,"text":"Use binary cross-entropy predictions.","markups":[]},{"name":"6a88","type":1,"text":"This is a minor tweak, but it is the kind of minor tweak that Jeremy wants you to think about and understand because it makes a really big difference to your training and when there is some increment over a previous paper, it would be something like this [1:08:25]. It is important to understand what this is doing and more importantly why.","markups":[{"type":3,"start":256,"end":263,"href":"https://youtu.be/0frKXR-2PBY?t=1h8m25s","title":"","rel":"","anchorType":0}]},{"name":"9ad6","type":1,"text":"So now we have [1:09:39]:","markups":[{"type":3,"start":16,"end":23,"href":"https://youtu.be/0frKXR-2PBY?t=1h9m39s","title":"","rel":"","anchorType":0}]},{"name":"3ec1","type":9,"text":"A custom loss function","markups":[]},{"name":"d4bf","type":9,"text":"A way to calculate Jaccard index","markups":[]},{"name":"882e","type":9,"text":"A way to convert activations to bounding box","markups":[]},{"name":"71d6","type":9,"text":"A way to map anchor boxes to ground truth","markups":[]},{"name":"9d33","type":1,"text":"Now all it’s left is SSD loss function.","markups":[]},{"name":"5fe0","type":13,"text":"SSD Loss Function [1:09:55]","markups":[{"type":3,"start":19,"end":26,"href":"https://youtu.be/0frKXR-2PBY?t=1h9m55s","title":"","rel":"","anchorType":0}]},{"name":"96a6","type":8,"text":"def ssd_1_loss(b_c,b_bb,bbox,clas,print_it=False):\n    bbox,clas = get_y(bbox,clas)\n    a_ic = actn_to_bb(b_bb, anchors)\n    overlaps = jaccard(bbox.data, anchor_cnr.data)\n    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)\n    gt_clas = clas[gt_idx]\n    pos = gt_overlap \x3e 0.4\n    pos_idx = torch.nonzero(pos)[:,0]\n    gt_clas[1-pos] = len(id2cat)\n    gt_bbox = bbox[gt_idx]\n    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n    clas_loss  = loss_f(b_c, gt_clas)\n    return loc_loss, clas_loss\n\ndef ssd_loss(pred,targ,print_it=False):\n    lcs,lls = 0.,0.\n    for b_c,b_bb,bbox,clas in zip(*pred,*targ):\n        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,print_it)\n        lls += loc_loss\n        lcs += clas_loss\n    if print_it: print(f'loc: {lls.data[0]}, clas: {lcs.data[0]}')\n    return lls+lcs","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":43,"end":48},{"type":1,"start":494,"end":500},{"type":1,"start":522,"end":525},{"type":1,"start":554,"end":559},{"type":1,"start":586,"end":589},{"type":1,"start":609,"end":611},{"type":1,"start":752,"end":754},{"type":1,"start":778,"end":791},{"type":1,"start":799,"end":812},{"type":1,"start":819,"end":825}]},{"name":"8c47","type":1,"text":"The ssd_loss function which is what we set as the criteria, it loops through each image in the mini-batch and call ssd_1_loss function (i.e. SSD loss for one image).","markups":[{"type":10,"start":4,"end":12},{"type":10,"start":115,"end":125}]},{"name":"98d0","type":1,"text":"ssd_1_loss is where it is all happening. It begins by de-structuring bbox and clas. Let’s take a closer look at get_y [1:10:38]:","markups":[{"type":10,"start":0,"end":10},{"type":10,"start":69,"end":73},{"type":10,"start":78,"end":82},{"type":10,"start":112,"end":117},{"type":3,"start":119,"end":126,"href":"https://youtu.be/0frKXR-2PBY?t=1h10m38s","title":"","rel":"","anchorType":0}]},{"name":"b8d2","type":8,"text":"def get_y(bbox,clas):\n    bbox = bbox.view(-1,4)/sz\n    bb_keep = ((bbox[:,2]-bbox[:,0])\x3e0).nonzero()[:,0]\n    return bbox[bb_keep],clas[bb_keep]","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":111,"end":117}]},{"name":"d4b5","type":1,"text":"A lot of code you find on the internet does not work with mini-batches. It only does one thing at a time which we don’t want. In this case, all these functions (get_y, actn_to_bb, map_to_ground_truth) is working on, not exactly a mini-batch at a time, but a whole bunch of ground truth objects at a time. The data loader is being fed a mini-batch at a time to do the convolutional layers. Because we can have different numbers of ground truth objects in each image but a tensor has to be the strict rectangular shape, fastai automatically pads it with zeros (any target values that are shorter) [1:11:08]. This was something that was added recently and super handy, but that does mean that you then have to make sure that you get rid of those zeros. So get_y gets rid of any of the bounding boxes that are just padding.","markups":[{"type":10,"start":161,"end":166},{"type":10,"start":168,"end":178},{"type":10,"start":180,"end":199},{"type":10,"start":753,"end":758},{"type":3,"start":596,"end":603,"href":"https://youtu.be/0frKXR-2PBY?t=1h11m8s","title":"","rel":"","anchorType":0},{"type":2,"start":409,"end":464}]},{"name":"f229","type":10,"text":"Get rid of the padding","markups":[]},{"name":"0e2d","type":10,"text":"Turn the activations to bounding boxes","markups":[]},{"name":"953a","type":10,"text":"Do the Jaccard","markups":[]},{"name":"fa65","type":10,"text":"Do map_to_ground_truth","markups":[]},{"name":"26b9","type":10,"text":"Check that there is an overlap greater than something around 0.4~0.5 (different papers use different values for this)","markups":[]},{"name":"aa5a","type":10,"text":"Find the indices of things that matched","markups":[]},{"name":"bad6","type":10,"text":"Assign background class for the ones that did not match","markups":[]},{"name":"a55d","type":10,"text":"Then finally get L1 loss for the localization, binary cross entropy loss for the classification, and return them which gets added in ssd_loss","markups":[{"type":10,"start":133,"end":141}]},{"name":"b35e","type":13,"text":"Training [1:12:47]","markups":[{"type":3,"start":10,"end":17,"href":"https://youtu.be/0frKXR-2PBY?t=1h12m47s","title":"","rel":"","anchorType":0}]},{"name":"0f37","type":8,"text":"learn.crit = ssd_loss\nlr = 3e-3\nlrs = np.array([lr/100,lr/10,lr])","markups":[]},{"name":"8eb3","type":8,"text":"learn.lr_find(lrs/1000,1.)\nlearn.sched.plot(1)","markups":[]},{"name":"5ef8","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      44.232681  21476.816406","markups":[{"type":2,"start":0,"end":93}]},{"name":"9862","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*V8J7FkreIVG7tKxGQQRV2Q.png","originalWidth":402,"originalHeight":270}},{"name":"c8ad","type":8,"text":"learn.lr_find(lrs/1000,1.)\nlearn.sched.plot(1)","markups":[]},{"name":"6db9","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      86.852668  32587.789062","markups":[{"type":2,"start":0,"end":93}]},{"name":"939e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*-q583mkIy-e3k6dz5HmkYw.png","originalWidth":396,"originalHeight":270}},{"name":"b198","type":8,"text":"learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))","markups":[]},{"name":"e101","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      45.570843  37.099854 \n    1      37.165911  32.165031                           \n    2      33.27844   30.990122                           \n    3      31.12054   29.804482                           \n    4      29.305789  28.943184","markups":[{"type":2,"start":0,"end":300}]},{"name":"3fb3","type":8,"text":"[28.943184]","markups":[{"type":2,"start":0,"end":11}]},{"name":"f2a9","type":8,"text":"learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))","markups":[]},{"name":"e4ee","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      43.726979  33.803085 \n    1      34.771754  29.012939                           \n    2      30.591864  27.132868                           \n    3      27.896905  26.151638                           \n    4      25.907382  25.739273","markups":[{"type":2,"start":0,"end":300}]},{"name":"e44e","type":8,"text":"[25.739273]","markups":[{"type":2,"start":0,"end":11}]},{"name":"b505","type":8,"text":"learn.save('0')","markups":[]},{"name":"e878","type":8,"text":"learn.load('0')","markups":[]},{"name":"6949","type":13,"text":"Result [1:13:16]","markups":[{"type":3,"start":8,"end":15,"href":"https://youtu.be/0frKXR-2PBY?t=1h13m16s","title":"","rel":"","anchorType":0}]},{"name":"3657","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*8azTUd1Ujf3FQSMBwIXgAw.png","originalWidth":1181,"originalHeight":866}},{"name":"efec","type":1,"text":"In practice, we want to remove the background and also add some threshold for probabilities, but it is on the right track. The potted plant image, the result is not surprising as all of our anchor boxes were small (4x4 grid). To go from here to something that is going to be more accurate, all we are going to do is to create way more anchor boxes.","markups":[]},{"name":"c496","type":1,"text":"Question: For the multi-label classification, why aren’t we multiplying the categorical loss by a constant like we did before [1:15:20]? Great question. It is because later on it will turn out we do not need to.","markups":[{"type":3,"start":127,"end":134,"href":"https://youtu.be/0frKXR-2PBY?t=1h15m20s","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":8}]},{"name":"5ece","type":13,"text":"More anchors! [1:14:47]","markups":[{"type":3,"start":15,"end":22,"href":"https://youtu.be/0frKXR-2PBY?t=1h14m47s","title":"","rel":"","anchorType":0}]},{"name":"3906","type":1,"text":"There are 3 ways to do this:","markups":[]},{"name":"9e13","type":10,"text":"Create anchor boxes of different sizes (zoom):","markups":[]},{"name":"ce4b","type":4,"text":"","markups":[],"layout":6,"metadata":{"id":"1*OtrTSJqBXyjeypKehik1CQ.png","originalWidth":457,"originalHeight":454}},{"name":"b8d9","type":4,"text":"","markups":[],"layout":7,"metadata":{"id":"1*YG5bCP3O-jVhaQX_wuiSSg.png","originalWidth":428,"originalHeight":432}},{"name":"02ff","type":4,"text":"From left (1x1, 2x2, 4x4 grids of anchor boxes). Notice that some of the anchor box is bigger than the original image.","markups":[],"layout":7,"metadata":{"id":"1*QCo0wOgJKXDBYNlmE7zUmA.png","originalWidth":414,"originalHeight":417}},{"name":"225d","type":1,"text":"2. Create anchor boxes of different aspect ratios:","markups":[]},{"name":"fdae","type":4,"text":"","markups":[],"layout":6,"metadata":{"id":"1*ko8vZK4RD8H2l4u1hXCQZQ.png","originalWidth":401,"originalHeight":401}},{"name":"0b2d","type":4,"text":"","markups":[],"layout":7,"metadata":{"id":"1*3rvuvY6Fu2S6eoN3nK1QWg.png","originalWidth":401,"originalHeight":404}},{"name":"508b","type":4,"text":"","markups":[],"layout":7,"metadata":{"id":"1*bWZwFqf2Bv-ZbW-KedNO0Q.png","originalWidth":404,"originalHeight":401}},{"name":"8881","type":1,"text":"3. Use more convolutional layers as sources of anchor boxes (the boxes are randomly jittered so that we can see ones that are overlapping [1:16:28]):","markups":[{"type":3,"start":139,"end":146,"href":"https://youtu.be/0frKXR-2PBY?t=1h16m28s","title":"","rel":"","anchorType":0}]},{"name":"f455","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*LwFOFtmawmpqp6VDc56RmA.png","originalWidth":402,"originalHeight":403}},{"name":"305a","type":1,"text":"Combining these approaches, you can create lots of anchor boxes (Jeremy said he wouldn’t print it, but here it is):","markups":[]},{"name":"fd17","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ymt8L0CCKMd9SG82SemdIA.png","originalWidth":449,"originalHeight":459}},{"name":"14d2","type":8,"text":"anc_grids = [4, 2, 1]\nanc_zooms = [0.75, 1., 1.3]\nanc_ratios = [(1., 1.), (1., 0.5), (0.5, 1.)]\n\nanchor_scales = [(anz*i,anz*j) for anz in anc_zooms \n                                    for (i,j) in anc_ratios]\nk = len(anchor_scales)\nanc_offsets = [1/(o*2) for o in anc_grids]","markups":[{"type":1,"start":128,"end":131},{"type":1,"start":136,"end":138},{"type":1,"start":186,"end":189},{"type":1,"start":196,"end":198},{"type":1,"start":257,"end":260},{"type":1,"start":263,"end":265}]},{"name":"2e8f","type":8,"text":"anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n                        for ao,ag in zip(anc_offsets,anc_grids)])\nanc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)","markups":[{"type":1,"start":89,"end":92},{"type":1,"start":99,"end":101},{"type":1,"start":218,"end":221},{"type":1,"start":228,"end":230}]},{"name":"908b","type":8,"text":"anc_sizes = np.concatenate([np.array([[o/ag,p/ag] \n              for i in range(ag*ag) for o,p in anchor_scales])\n                 for ag in anc_grids])\ngrid_sizes = V(np.concatenate([np.array([ 1/ag \n              for i in range(ag*ag) for o,p in anchor_scales])\n                  for ag in anc_grids]), \n                      requires_grad=False).unsqueeze(1)\nanchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), \n              requires_grad=False).float()\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])","markups":[{"type":1,"start":65,"end":68},{"type":1,"start":71,"end":73},{"type":1,"start":87,"end":90},{"type":1,"start":95,"end":97},{"type":1,"start":131,"end":134},{"type":1,"start":138,"end":140},{"type":1,"start":215,"end":218},{"type":1,"start":221,"end":223},{"type":1,"start":237,"end":240},{"type":1,"start":245,"end":247},{"type":1,"start":282,"end":285},{"type":1,"start":289,"end":291},{"type":1,"start":342,"end":347},{"type":1,"start":450,"end":455}]},{"name":"43cc","type":1,"text":"anchors : middle and height, width","markups":[{"type":10,"start":0,"end":7}]},{"name":"1b94","type":1,"text":"anchor_cnr : top left and bottom right corners","markups":[{"type":10,"start":0,"end":10}]},{"name":"ce3e","type":13,"text":"Review of key concept [1:18:00]","markups":[{"type":3,"start":23,"end":30,"href":"https://youtu.be/0frKXR-2PBY?t=1h18m","title":"","rel":"","anchorType":0}]},{"name":"a3db","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*C67J9RhTAiz9MCD-ebpp_w.png","originalWidth":1746,"originalHeight":509}},{"name":"cdc9","type":9,"text":"We have a vector of ground truth (sets of 4 bounding box coordinates and a class)","markups":[]},{"name":"eb81","type":9,"text":"We have a neural net that takes some input and spits out some output activations","markups":[]},{"name":"1fb5","type":9,"text":"Compare the activations and the ground truth, calculate a loss, find the derivative of that, and adjust weights according to the derivative times a learning rate.","markups":[]},{"name":"90af","type":9,"text":"We need a loss function that can take ground truth and activation and spit out a number that says how good these activations are. To do this, we need to take each one of m ground truth objects and decide which set of (4+c) activations is responsible for that object [1:21:58] — which one we should be comparing to decide whether the class is correct and bounding box is close or not (matching problem).","markups":[{"type":10,"start":170,"end":171},{"type":10,"start":217,"end":222},{"type":3,"start":267,"end":274,"href":"https://youtu.be/0frKXR-2PBY?t=1h21m58s","title":"","rel":"","anchorType":0}]},{"name":"48c8","type":9,"text":"Since we are using SSD approach, so it is not arbitrary which ones we match up [1:23:18]. We want to match up the set of activations whose receptive field has the maximum density from where the real object is.","markups":[{"type":3,"start":80,"end":87,"href":"https://youtu.be/0frKXR-2PBY?t=1h23m18s","title":"","rel":"","anchorType":0}]},{"name":"c1a1","type":9,"text":"The loss function needs to be some consistent task. If in the first image, the top left object corresponds with the first 4+c activations, and in the second image, we threw things around and suddenly it’s now going with the last 4+c activations, the neural net doesn’t know what to learn.","markups":[]},{"name":"3723","type":9,"text":"Once matching problem is resolved, the rest is just the same as the single object detection.","markups":[]},{"name":"7654","type":1,"text":"Architectures:","markups":[]},{"name":"2c45","type":9,"text":"YOLO — the last layer is fully connected (no concept of geometry)","markups":[]},{"name":"639f","type":9,"text":"SSD — the last layer is convolutional","markups":[]},{"name":"db54","type":13,"text":"k (zooms x ratios)[1:29:39]","markups":[{"type":3,"start":19,"end":26,"href":"https://youtu.be/0frKXR-2PBY?t=1h29m39s","title":"","rel":"","anchorType":0}]},{"name":"c68a","type":1,"text":"For every grid cell which can be different sizes, we can have different orientations and zooms representing different anchor boxes which are just like conceptual ideas that every one of anchor boxes is associated with one set of 4+c activations in our model. So however many anchor boxes we have, we need to have that times (4+c) activations. That does not mean that each convolutional layer needs that many activations. Because 4x4 convolutional layer already has 16 sets of activations, the 2x2 layer has 4 sets of activations, and finally 1x1 has one set. So we basically get 1 + 4 + 16 for free. So we only needs to know k where k is the number of zooms by the number of aspect ratios. Where else, the grids, we will get for free through our architecture.","markups":[{"type":10,"start":229,"end":232},{"type":10,"start":324,"end":329},{"type":10,"start":625,"end":626},{"type":10,"start":633,"end":634}]},{"name":"ca16","type":13,"text":"Model Architecture [1:31:10]","markups":[{"type":3,"start":20,"end":27,"href":"https://youtu.be/0frKXR-2PBY?t=1h31m10s","title":"","rel":"","anchorType":0}]},{"name":"4a50","type":8,"text":"drop=0.4\n\nclass SSD_MultiHead(nn.Module):\n    def __init__(self, k, bias):\n        super().__init__()\n        self.drop = nn.Dropout(drop)\n        self.sconv0 = StdConv(512,256, stride=1, drop=drop)\n        self.sconv1 = StdConv(256,256, drop=drop)\n        self.sconv2 = StdConv(256,256, drop=drop)\n        self.sconv3 = StdConv(256,256, drop=drop)\n        self.out1 = OutConv(k, 256, bias)\n        self.out2 = OutConv(k, 256, bias)\n        self.out3 = OutConv(k, 256, bias)\n\n    def forward(self, x):\n        x = self.drop(F.relu(x))\n        x = self.sconv0(x)\n        x = self.sconv1(x)\n        o1c,o1l = self.out1(x)\n        x = self.sconv2(x)\n        o2c,o2l = self.out2(x)\n        x = self.sconv3(x)\n        o3c,o3l = self.out3(x)\n        return [torch.cat([o1c,o2c,o3c], dim=1),\n                torch.cat([o1l,o2l,o3l], dim=1)]\n\nhead_reg4 = SSD_MultiHead(k, -4.)\nmodels = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)\nlearn = ConvLearner(md, models)\nlearn.opt_fn = optim.Adam","markups":[{"type":1,"start":10,"end":15},{"type":1,"start":16,"end":29},{"type":1,"start":46,"end":49},{"type":1,"start":480,"end":483},{"type":1,"start":744,"end":750}]},{"name":"5fd9","type":1,"text":"The model is nearly identical to what we had before. But we have a number of stride 2 convolutions which is going to take us through to 4x4, 2x2, and 1x1 (each stride 2 convolution halves our grid size in both directions).","markups":[]},{"name":"5dc6","type":9,"text":"After we do our first convolution to get to 4x4, we will grab a set of outputs from that because we want to save away the 4x4 anchors.","markups":[]},{"name":"16d7","type":9,"text":"Once we get to 2x2, we grab another set of now 2x2 anchors","markups":[]},{"name":"f368","type":9,"text":"Then finally we get to 1x1","markups":[]},{"name":"afc1","type":9,"text":"We then concatenate them all together, which gives us the correct number of activations (one activation for every anchor box).","markups":[]},{"name":"5984","type":13,"text":"Training [1:32:50]","markups":[{"type":3,"start":10,"end":17,"href":"https://youtu.be/0frKXR-2PBY?t=1h32m50s","title":"","rel":"","anchorType":0}]},{"name":"676d","type":8,"text":"learn.crit = ssd_loss\nlr = 1e-2\nlrs = np.array([lr/100,lr/10,lr])","markups":[]},{"name":"1dca","type":8,"text":"learn.lr_find(lrs/1000,1.)\nlearn.sched.plot(n_skip_end=2)","markups":[]},{"name":"1150","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*jB_OxbaTmMXHbkeXE4G0SQ.png","originalWidth":402,"originalHeight":270}},{"name":"a580","type":8,"text":"learn.fit(lrs, 1, cycle_len=4, use_clr=(20,8))","markups":[]},{"name":"0d46","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      15.124349  15.015433 \n    1      13.091956  10.39855                            \n    2      11.643629  9.4289                              \n    3      10.532467  8.822998","markups":[{"type":2,"start":0,"end":240}]},{"name":"296f","type":8,"text":"[8.822998]","markups":[{"type":2,"start":0,"end":10}]},{"name":"7b9f","type":8,"text":"learn.save('tmp')","markups":[]},{"name":"7493","type":8,"text":"learn.freeze_to(-2)\nlearn.fit(lrs/2, 1, cycle_len=4, use_clr=(20,8))","markups":[]},{"name":"98c2","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      9.821056   10.335152 \n    1      9.419633   11.834093                           \n    2      8.78818    7.907762                            \n    3      8.219976   7.456364","markups":[{"type":2,"start":0,"end":240}]},{"name":"727f","type":8,"text":"[7.4563637]","markups":[{"type":2,"start":0,"end":11}]},{"name":"2c78","type":8,"text":"x,y = next(iter(md.val_dl))\ny = V(y)\nbatch = learn.model(V(x))\nb_clas,b_bb = batch\nx = to_np(x)\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\nfor idx,ax in enumerate(axes.flat):\n    ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], \n             b_clas[idx].max(1)[0].sigmoid(), 0.2)\nplt.tight_layout()","markups":[{"type":1,"start":146,"end":149},{"type":1,"start":157,"end":159},{"type":1,"start":402,"end":405}]},{"name":"bac0","type":1,"text":"Here, we printed out those detections with at least probability of 0.2 . Some of them look pretty hopeful but others not so much.","markups":[{"type":10,"start":67,"end":70}]},{"name":"bc4f","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*l168j5d3fWBZLST3XLPD6A.png","originalWidth":1178,"originalHeight":858}},{"name":"acc2","type":3,"text":"History of object detection [1:33:43]","markups":[{"type":3,"start":29,"end":36,"href":"https://youtu.be/0frKXR-2PBY?t=1h33m43s","title":"","rel":"","anchorType":0}]},{"name":"16b5","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*bQPvoI0soxtlBt1cEZlzcQ.png","originalWidth":1706,"originalHeight":904}},{"name":"8ece","type":1,"text":"Scalable Object Detection using Deep Neural Networks","markups":[{"type":3,"start":0,"end":52,"href":"https://arxiv.org/abs/1312.2249","title":"","rel":"","anchorType":0}]},{"name":"c5f6","type":9,"text":"When people refer to the multi-box method, they are talking about this paper.","markups":[]},{"name":"1caf","type":9,"text":"This was the paper that came up with the idea that we can have a loss function that has this matching process and then you can use that to do object detection. So everything since that time has been trying to figure out how to make this better.","markups":[]},{"name":"29bb","type":1,"text":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks","markups":[{"type":3,"start":0,"end":78,"href":"https://arxiv.org/abs/1506.01497","title":"","rel":"","anchorType":0}]},{"name":"3858","type":9,"text":"In parallel, Ross Girshick was going down a totally different direction. He had these two-stage process where the first stage used the classical computer vision approaches to find edges and changes of gradients to guess which parts of the image may represent distinct objects. Then fit each of those into a convolutional neural network which was basically designed to figure out if that is the kind of object we are interested in.","markups":[]},{"name":"9e50","type":9,"text":"R-CNN and Fast R-CNN are hybrid of traditional computer vision and deep learning.","markups":[]},{"name":"3b77","type":9,"text":"What Ross and his team then did was they took the multibox idea and replaced the traditional non-deep learning computer vision part of their two stage process with the conv net. So now they have two conv nets: one for region proposals (all of the things that might be objects) and the second part was the same as his earlier work.","markups":[]},{"name":"9a15","type":1,"text":"You Only Look Once: Unified, Real-Time Object Detection","markups":[{"type":3,"start":0,"end":55,"href":"https://arxiv.org/abs/1506.02640","title":"","rel":"","anchorType":0}]},{"name":"cb78","type":1,"text":"SSD: Single Shot MultiBox Detector","markups":[{"type":3,"start":0,"end":34,"href":"https://arxiv.org/abs/1512.02325","title":"","rel":"","anchorType":0}]},{"name":"9e7d","type":9,"text":"At similar time these paper came out. Both of these did something pretty cool which is they achieved similar performance as the Faster R-CNN but with 1 stage.","markups":[]},{"name":"ea38","type":9,"text":"They took the multibox idea and they tried to figure out how to deal with messy outputs. The basic ideas were to use, for example, hard negative mining where they would go through and find all of the matches that did not look that good and throw them away, use very tricky and complex data augmentation methods, and all kind of hackery. But they got them to work pretty well.","markups":[]},{"name":"a3f4","type":1,"text":"Focal Loss for Dense Object Detection (RetinaNet)","markups":[{"type":3,"start":0,"end":37,"href":"https://arxiv.org/abs/1708.02002","title":"","rel":"","anchorType":0}]},{"name":"e6d1","type":9,"text":"Then something really cool happened late last year which is this thing called focal loss.","markups":[]},{"name":"883d","type":9,"text":"They actually realized why this messy thing wasn’t working. When we look at an image, there are 3 different granularities of convolutional grid (4x4, 2x2, 1x1) [1:37:28]. The 1x1 is quite likely to have a reasonable overlap with some object because most photos have some kind of main subject. On the other hand, in the 4x4 grid cells, the most of 16 anchor boxes are not going to have a much of an overlap with anything. So if somebody was to say to you “$20 bet, what do you reckon this little clip is?” and you are not sure, you will say “background” because most of the time, it is the background.","markups":[{"type":3,"start":161,"end":168,"href":"https://youtu.be/0frKXR-2PBY?t=1h37m28s","title":"","rel":"","anchorType":0}]},{"name":"05da","type":1,"text":"Question: I understand why we have a 4x4 grid of receptive fields with 1 anchor box each to coarsely localize objects in the image. But what I think I’m missing is why we need multiple receptive fields at different sizes. The first version already included 16 receptive fields, each with a single anchor box associated. With the additions, there are now many more anchor boxes to consider. Is this because you constrained how much a receptive field could move or scale from its original size? Or is there another reason? [1:38:47] It is kind of backwards. The reason Jeremy did the constraining was because he knew he was going to be adding more boxes later. But really, the reason is that the Jaccard overlap between one of those 4x4 grid cells and a picture where a single object that takes up most of the image is never going to be 0.5. The intersection is much smaller than the union because the object is too big. So for this general idea to work where we are saying you are responsible for something that you have better than 50% overlap with, we need anchor boxes which will on a regular basis have a 50% or higher overlap which means we need to have a variety of sizes, shapes, and scales. This all happens in the loss function. The vast majority of the interesting stuff in all of the object detection is the loss function.","markups":[{"type":3,"start":522,"end":529,"href":"https://youtu.be/0frKXR-2PBY?t=1h38m47s","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":8}]},{"name":"bfb0","type":13,"text":"Focal Loss [1:40:38]","markups":[{"type":3,"start":12,"end":19,"href":"https://youtu.be/0frKXR-2PBY?t=1h40m38s","title":"","rel":"","anchorType":0}]},{"name":"aa98","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6Bood7G6dUuhigy9cxkZ-Q.png","originalWidth":598,"originalHeight":341}},{"name":"7caf","type":1,"text":"The key thing is this very first picture. The blue line is the binary cross entropy loss. If the answer is not a motorbike [1:41:46], and I said “I think it’s not a motorbike and I am 60% sure” with the blue line, the loss is still about 0.5 which is pretty bad. So if we want to get our loss down, then for all these things which are actually back ground, we have to be saying “I am sure that is background”, “I am sure it’s not a motorbike, or a bus, or a person” — because if I don’t say we are sure it is not any of these things, then we still get loss.","markups":[{"type":3,"start":124,"end":131,"href":"https://youtu.be/0frKXR-2PBY?t=1h41m46s","title":"","rel":"","anchorType":0}]},{"name":"75ba","type":1,"text":"That is why the motorbike example did not work [1:42:39]. Because even when it gets to lower right corner and it wants to say “I think it’s a motorbike”, there is no payoff for it to say so. If it is wrong, it gets killed. And the vast majority of the time, it is background. Even if it is not background, it is not enough just to say “it’s not background” — you have to say which of the 20 things it is.","markups":[{"type":3,"start":48,"end":55,"href":"https://youtu.be/0frKXR-2PBY?t=1h42m39s","title":"","rel":"","anchorType":0}]},{"name":"0405","type":1,"text":"So the trick is to trying to find a different loss function [1:44:00] that looks more like the purple line. Focal loss is literally just a scaled cross entropy loss. Now if we say “I’m .6 sure it’s not a motorbike” then the loss function will say “good for you! no worries” [1:44:42].","markups":[{"type":3,"start":61,"end":68,"href":"https://youtu.be/0frKXR-2PBY?t=1h44m","title":"","rel":"","anchorType":0},{"type":3,"start":275,"end":282,"href":"https://youtu.be/0frKXR-2PBY?t=1h44m42s","title":"","rel":"","anchorType":0}]},{"name":"e20f","type":1,"text":"The actual contribution of this paper is to add (1 − pt)^γ to the start of the equation [1:45:06] which sounds like nothing but actually people have been trying to figure out this problem for years. When you come across a paper like this which is game-changing, you shouldn’t assume you are going to have to write thousands of lines of code. Very often it is one line of code, or the change of a single constant, or adding log to a single place.","markups":[{"type":10,"start":48,"end":58},{"type":3,"start":89,"end":96,"href":"https://youtu.be/0frKXR-2PBY?t=1h45m6s","title":"","rel":"","anchorType":0}]},{"name":"3d32","type":1,"text":"A couple of terrific things about this paper [1:46:08]:","markups":[{"type":3,"start":46,"end":53,"href":"https://youtu.be/0frKXR-2PBY?t=1h46m8s","title":"","rel":"","anchorType":0}]},{"name":"4dce","type":9,"text":"Equations are written in a simple manner","markups":[]},{"name":"b4f3","type":9,"text":"They “refactor”","markups":[]},{"name":"021e","type":13,"text":"Implementing Focal Loss [1:49:27]:","markups":[{"type":3,"start":25,"end":32,"href":"https://youtu.be/0frKXR-2PBY?t=1h49m27s","title":"","rel":"","anchorType":0}]},{"name":"a6b9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*wIp0HYEWPnkiuxLeCfEiAg.png","originalWidth":429,"originalHeight":36}},{"name":"0a90","type":1,"text":"Remember, -log(pt) is the cross entropy loss and focal loss is just a scaled version. When we defined the binomial cross entropy loss, you may have noticed that there was a weight which by default was none:","markups":[]},{"name":"2c64","type":8,"text":"class BCE_Loss(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, pred, targ):\n        t = one_hot_embedding(targ, self.num_classes+1)\n        t = V(t[:,:-1].contiguous())#.cpu()\n        x = pred[:,:-1]\n        w = self.get_weight(x,t)\n        return F.binary_cross_entropy_with_logits(x, t, w, \n                          size_average=False)/self.num_classes\n    \n    def get_weight(self,x,t): return None","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":14},{"type":1,"start":31,"end":34},{"type":1,"start":135,"end":138},{"type":1,"start":331,"end":337},{"type":1,"start":422,"end":427},{"type":1,"start":455,"end":458},{"type":1,"start":481,"end":487},{"type":1,"start":488,"end":492},{"type":2,"start":258,"end":265}]},{"name":"0e89","type":1,"text":"When you call F.binary_cross_entropy_with_logits, you can pass in the weight. Since we just wanted to multiply a cross entropy by something, we can just define get_weight. Here is the entirety of focal loss [1:50:23]:","markups":[{"type":10,"start":14,"end":48},{"type":10,"start":160,"end":170},{"type":3,"start":208,"end":215,"href":"https://youtu.be/0frKXR-2PBY?t=1h50m23s","title":"","rel":"","anchorType":0}]},{"name":"1cbf","type":8,"text":"class FocalLoss(BCE_Loss):\n    def get_weight(self,x,t):\n        alpha,gamma = 0.25,2.\n        p = x.sigmoid()\n        pt = p*t + (1-p)*(1-t)\n        w = alpha*t + (1-alpha)*(1-t)\n        return w * (1-pt).pow(gamma)","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":15},{"type":1,"start":31,"end":34},{"type":1,"start":188,"end":194}]},{"name":"9651","type":1,"text":"If you were wondering why alpha and gamma are 0.25 and 2, here is another excellent thing about this paper, because they tried lots of different values and found that these work well:","markups":[]},{"name":"9bed","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*qFPRvFHQMQplSJGp3QLiNA.png","originalWidth":319,"originalHeight":232}},{"name":"6ed2","type":13,"text":"Training [1:51:25]","markups":[{"type":3,"start":10,"end":17,"href":"https://youtu.be/0frKXR-2PBY?t=1h51m25s","title":"","rel":"","anchorType":0}]},{"name":"ebda","type":8,"text":"learn.lr_find(lrs/1000,1.)\nlearn.sched.plot(n_skip_end=2)","markups":[]},{"name":"5e64","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*lQPSR3V2IXbxOpcgNE-U-Q.png","originalWidth":396,"originalHeight":270}},{"name":"cf7e","type":8,"text":"learn.fit(lrs, 1, cycle_len=10, use_clr=(20,10))","markups":[]},{"name":"a909","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      24.263046  28.975235 \n    1      20.459562  16.362392                           \n    2      17.880827  14.884829                           \n    3      15.956896  13.676485                           \n    4      14.521345  13.134197                           \n    5      13.460941  12.594139                           \n    6      12.651842  12.069849                           \n    7      11.944972  11.956457                           \n    8      11.385798  11.561226                           \n    9      10.988802  11.362164","markups":[{"type":2,"start":0,"end":595}]},{"name":"f3db","type":8,"text":"[11.362164]","markups":[{"type":2,"start":0,"end":11}]},{"name":"08b0","type":8,"text":"learn.save('fl0')\nlearn.load('fl0')","markups":[]},{"name":"5836","type":8,"text":"learn.freeze_to(-2)\nlearn.fit(lrs/4, 1, cycle_len=10, use_clr=(20,10))","markups":[]},{"name":"911e","type":8,"text":"epoch      trn_loss   val_loss                            \n    0      10.871668  11.615532 \n    1      10.908461  11.604334                           \n    2      10.549796  11.486127                           \n    3      10.130961  11.088478                           \n    4      9.70691    10.72144                            \n    5      9.319202   10.600481                           \n    6      8.916653   10.358334                           \n    7      8.579452   10.624706                           \n    8      8.274838   10.163422                           \n    9      7.994316   10.108068","markups":[{"type":2,"start":0,"end":595}]},{"name":"44c8","type":8,"text":"[10.108068]","markups":[{"type":2,"start":0,"end":11}]},{"name":"268f","type":8,"text":"learn.save('drop4')\nlearn.load('drop4')","markups":[]},{"name":"eed6","type":8,"text":"plot_results(0.75)","markups":[]},{"name":"3481","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*G4HCc1mpkvHFqbhrb5Uwpw.png","originalWidth":1143,"originalHeight":856}},{"name":"bc8c","type":1,"text":"This time things are looking quite a bit better. So our last step, for now, is to basically figure out how to pull out just the interesting ones.","markups":[]},{"name":"dc8a","type":13,"text":"Non Maximum Suppression [1:52:15]","markups":[{"type":3,"start":25,"end":32,"href":"https://youtu.be/0frKXR-2PBY?t=1h52m15s","title":"","rel":"","anchorType":0}]},{"name":"f1db","type":1,"text":"All we are going to do is we are going to go through every pair of these bounding boxes and if they overlap by more than some amount, say 0.5, using Jaccard and they are both predicting the same class, we are going to assume they are the same thing and we are going to pick the one with higher p value.","markups":[{"type":10,"start":294,"end":295}]},{"name":"6313","type":1,"text":"It is really boring code, Jeremy didn’t write it himself and copied somebody else’s. No reason particularly to go through it.","markups":[]},{"name":"1f12","type":8,"text":"def nms(boxes, scores, overlap=0.5, top_k=100):\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0: return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    count = 0\n    while idx.numel() \x3e 0:\n        i = idx[-1]  # index of current largest val\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1: break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  \n        # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU \x3c= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":105,"end":107},{"type":1,"start":128,"end":134},{"type":1,"start":525,"end":530},{"type":1,"start":651,"end":653},{"type":1,"start":672,"end":677},{"type":1,"start":1759,"end":1765},{"type":2,"start":292,"end":317},{"type":2,"start":342,"end":377},{"type":2,"start":569,"end":599},{"type":2,"start":702,"end":733},{"type":2,"start":742,"end":776},{"type":2,"start":977,"end":1025},{"type":2,"start":1298,"end":1349},{"type":2,"start":1450,"end":1485},{"type":2,"start":1549,"end":1572},{"type":2,"start":1646,"end":1667},{"type":2,"start":1676,"end":1719}]},{"name":"6e66","type":8,"text":"def show_nmf(idx):\n    ima=md.val_ds.ds.denorm(x)[idx]\n    bbox,clas = get_y(y[0][idx], y[1][idx])\n    a_ic = actn_to_bb(b_bb[idx], anchors)\n    clas_pr, clas_ids = b_clas[idx].max(1)\n    clas_pr = clas_pr.sigmoid()\n\n    conf_scores = b_clas[idx].sigmoid().t().data\n\n    out1,out2,cc = [],[],[]\n    for cl in range(0, len(conf_scores)-1):\n        c_mask = conf_scores[cl] \x3e 0.25\n        if c_mask.sum() == 0: continue\n        scores = conf_scores[cl][c_mask]\n        l_mask = c_mask.unsqueeze(1).expand_as(a_ic)\n        boxes = a_ic[l_mask].view(-1, 4)\n        ids, count = nms(boxes.data, scores, 0.4, 50)\n        ids = ids[:count]\n        out1.append(scores[ids])\n        out2.append(boxes.data[ids])\n        cc.append([cl]*count)\n    cc = T(np.concatenate(cc))\n    out1 = torch.cat(out1)\n    out2 = torch.cat(out2)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    torch_gt(ax, ima, out2, cc, out1, 0.1)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":299,"end":302},{"type":1,"start":306,"end":308},{"type":1,"start":387,"end":389},{"type":1,"start":409,"end":417}]},{"name":"26af","type":8,"text":"for i in range(12): show_nmf(i)","markups":[{"type":1,"start":0,"end":3},{"type":1,"start":6,"end":8}]},{"name":"fc05","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*MXk2chJJEcjOz8hMn1ZsOQ.png","originalWidth":456,"originalHeight":456}},{"name":"d53d","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*Fj9fK3G6iXBsGI_XJrxXyg.png","originalWidth":456,"originalHeight":456,"isFeatured":true}},{"name":"2958","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*6p3dm-i-YxC9QkxouHJdoA.png","originalWidth":456,"originalHeight":456}},{"name":"75fa","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*nkEpAd2_H4lG1vQfnCJn4Q.png","originalWidth":456,"originalHeight":456}},{"name":"cbef","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*THGq5C21NaP92vw5E_QNdA.png","originalWidth":456,"originalHeight":456}},{"name":"b2b2","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*0wckbiUSax2JpBlgJxJ05g.png","originalWidth":456,"originalHeight":456}},{"name":"a1f1","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*EWbNGEQFvYMgC4PSaLe8Ww.png","originalWidth":456,"originalHeight":456}},{"name":"5cba","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*vTRCVjln4vkma1R6eBeSwA.png","originalWidth":456,"originalHeight":456}},{"name":"4351","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*3Q01FZuzfptkYrekJiGm1g.png","originalWidth":456,"originalHeight":456}},{"name":"b40b","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*-cD3LQIG9FnyJbt0cnpbNg.png","originalWidth":456,"originalHeight":456}},{"name":"ead3","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*Hkgs1u9PFH9ZrTKL8YBW2Q.png","originalWidth":456,"originalHeight":456}},{"name":"bb83","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*uyTNlp61jcyaW9knbnNSEw.png","originalWidth":456,"originalHeight":456}},{"name":"55b5","type":1,"text":"There are some things still to fix here [1:53:43]. The trick will be to use something called feature pyramid. That is what we are going to do in lesson 14.","markups":[{"type":3,"start":41,"end":48,"href":"https://youtu.be/0frKXR-2PBY?t=1h53m43s","title":"","rel":"","anchorType":0}]},{"name":"5eed","type":13,"text":"Talking a little more about SSD paper [1:54:03]","markups":[{"type":3,"start":39,"end":46,"href":"https://youtu.be/0frKXR-2PBY?t=1h54m3s","title":"","rel":"","anchorType":0}]},{"name":"b19e","type":1,"text":"When this paper came out, Jeremy was excited because this and YOLO were the first kind of single-pass good quality object detection method that come along. There has been this continuous repetition of history in the deep learning world which is things that involve multiple passes of multiple different pieces, over time, particularly where they involve some non-deep learning pieces (like R-CNN did), over time, they always get turned into a single end-to-end deep learning model. So I tend to ignore them until that happens because that’s the point where people have figured out how to show this as a deep learning model, as soon as they do that they generally end up something much faster and much more accurate. So SSD and YOLO were really important.","markups":[]},{"name":"0629","type":1,"text":"The model is 4 paragraphs. Papers are really concise which means you need to read them pretty carefully. Partly, though, you need to know which bits to read carefully. The bits where they say “here we are going to prove the error bounds on this model,” you could ignore that because you don’t care about proving error bounds. But the bit which says here is what the model is, you need to read real carefully.","markups":[]},{"name":"18c6","type":1,"text":"Jeremy reads a section 2.1 Model [1:56:37]","markups":[{"type":3,"start":34,"end":41,"href":"https://youtu.be/0frKXR-2PBY?t=1h56m37s","title":"","rel":"","anchorType":0},{"type":1,"start":23,"end":32}]},{"name":"1d2d","type":1,"text":"If you jump straight in and read a paper like this, these 4 paragraphs would probably make no sense. But now that we’ve gone through it, you read those and hopefully thinking “oh that’s just what Jeremy said, only they sad it better than Jeremy and less words [2:00:37]. If you start to read a paper and go “what the heck”, the trick is to then start reading back over the citations.","markups":[{"type":3,"start":261,"end":268,"href":"https://youtu.be/0frKXR-2PBY?t=2h37s","title":"","rel":"","anchorType":0}]},{"name":"9414","type":1,"text":"Jeremy reads Matching strategy and Training objective (a.k.a. Loss function)[2:01:44]","markups":[{"type":3,"start":77,"end":84,"href":"https://youtu.be/0frKXR-2PBY?t=2h1m44s","title":"","rel":"","anchorType":0},{"type":1,"start":13,"end":30},{"type":1,"start":35,"end":53}]},{"name":"11d8","type":13,"text":"Some paper tips [2:02:34]","markups":[{"type":3,"start":17,"end":24,"href":"https://youtu.be/0frKXR-2PBY?t=2h2m34s","title":"","rel":"","anchorType":0}]},{"name":"6731","type":1,"text":"Scalable Object Detection using Deep Neural Networks","markups":[{"type":3,"start":0,"end":52,"href":"https://arxiv.org/pdf/1312.2249.pdf","title":"","rel":"","anchorType":0}]},{"name":"20f5","type":9,"text":"“Training objective” is loss function","markups":[]},{"name":"1d70","type":9,"text":"Double bars and two 2’s like this means Mean Squared Error","markups":[]},{"name":"4909","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*LubBtX9ODFMBgI34bFHtdw.png","originalWidth":446,"originalHeight":73}},{"name":"72d3","type":9,"text":"log(c) and log(1-c), and x and (1-x) they are all the pieces for binary cross entropy:","markups":[]},{"name":"ba87","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*3Xq3HB72jsVKI7uHOHzRDQ.png","originalWidth":562,"originalHeight":86}},{"name":"2118","type":1,"text":"This week, go through the code and go through the paper and see what is going on. Remember what Jeremy did to make it easier for you was he took that loss function, he copied it into a cell and split it up so that each bit was in a separate cell. Then after every sell, he printed or plotted that value. Hopefully this is a good starting point.","markups":[]},{"name":"27d5","type":1,"text":"Lessons: 1 ・ 2 ・ 3 ・ 4 ・ 5 ・ 6 ・ 7 ・ 8 ・ 9 ・ 10 ・ 11 ・ 12 ・ 13 ・ 14","markups":[{"type":3,"start":9,"end":10,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197","title":"","rel":"","anchorType":0},{"type":3,"start":13,"end":14,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4","title":"","rel":"","anchorType":0},{"type":3,"start":17,"end":18,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56","title":"","rel":"","anchorType":0},{"type":3,"start":21,"end":22,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa","title":"","rel":"","anchorType":0},{"type":3,"start":25,"end":26,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8","title":"","rel":"","anchorType":0},{"type":3,"start":29,"end":30,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c","title":"","rel":"","anchorType":0},{"type":3,"start":33,"end":34,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c","title":"","rel":"","anchorType":0},{"type":3,"start":37,"end":38,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493","title":"","rel":"","anchorType":0},{"type":3,"start":41,"end":42,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","title":"","rel":"","anchorType":0},{"type":3,"start":45,"end":47,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c","title":"","rel":"","anchorType":0},{"type":3,"start":50,"end":52,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34","title":"","rel":"","anchorType":0},{"type":3,"start":55,"end":57,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94","title":"","rel":"","anchorType":0},{"type":3,"start":60,"end":62,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0","title":"","rel":"","anchorType":0},{"type":3,"start":65,"end":67,"href":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add","title":"","rel":"","anchorType":0},{"type":1,"start":41,"end":42}]}],"sections":[{"name":"5f04","startIndex":0},{"name":"d0af","startIndex":3},{"name":"8519","startIndex":429}]},"postDisplay":{"coverless":true}},"virtuals":{"allowNotes":true,"previewImage":{"imageId":"1*Fj9fK3G6iXBsGI_XJrxXyg.png","filter":"","backgroundSize":"","originalWidth":456,"originalHeight":456,"strategy":"resample","height":0,"width":0},"wordCount":10008,"imageCount":56,"readingTime":41.31603773584905,"subtitle":"My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really”…","userPostRelation":{"userId":"b43c07dd10f6","postId":"5f0cf9e4bb5b","readAt":0,"readLaterAddedAt":1536422104592,"votedAt":1536461904616,"collaboratorAddedAt":0,"notesAddedAt":0,"subscribedAt":0,"lastReadSectionName":"d0af","lastReadVersionId":"de36f820714b","lastReadAt":1539961186645,"lastReadParagraphName":"ddb9","lastReadPercentage":0.38,"viewedAt":1539873031603,"presentedCountInResponseManagement":0,"clapCount":25,"seriesUpdateNotifsOptedInAt":0,"queuedAt":1536422104592,"seriesFirstViewedAt":0,"presentedCountInStream":9,"seriesLastViewedAt":0,"audioProgressSec":0},"usersBySocialRecommends":[],"noIndex":false,"recommends":55,"socialRecommends":[],"isBookmarked":true,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":48590,"metadata":{"postCount":48590,"coverImage":{"id":"1*jnZT4gFAzScOJ_VnYsni0g.png","originalWidth":700,"originalHeight":450,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":11621,"metadata":{"postCount":11621,"coverImage":{"id":"1*fiYPinouczFoVeB6KYqs9Q.gif","originalWidth":1152,"originalHeight":864,"isFeatured":true}},"type":"Tag"},{"slug":"ai","name":"AI","postCount":23325,"metadata":{"postCount":23325,"coverImage":{"id":"0*K8eg3bUVu4AG-4FB","isFeatured":true}},"type":"Tag"},{"slug":"neural-networks","name":"Neural Networks","postCount":3676,"metadata":{"postCount":3676,"coverImage":{"id":"1*fiYPinouczFoVeB6KYqs9Q.gif","originalWidth":1152,"originalHeight":864,"isFeatured":true}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":6,"links":{"entries":[{"url":"https://en.wikipedia.org/wiki/Jaccard_index","alts":[],"httpStatus":200},{"url":"http://www.fast.ai/","alts":[],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c","alts":[{"type":2,"url":"medium://p/de70d626976c"},{"type":3,"url":"medium://p/de70d626976c"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8","alts":[{"type":3,"url":"medium://p/dd904506bee8"},{"type":2,"url":"medium://p/dd904506bee8"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa","alts":[{"type":2,"url":"medium://p/2048a26d58aa"},{"type":3,"url":"medium://p/2048a26d58aa"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c","alts":[{"type":2,"url":"medium://p/1b9503aff0c"},{"type":3,"url":"medium://p/1b9503aff0c"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56","alts":[{"type":2,"url":"medium://p/74b0ef79e56"},{"type":3,"url":"medium://p/74b0ef79e56"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c","alts":[{"type":2,"url":"medium://p/422d87c3340c"},{"type":3,"url":"medium://p/422d87c3340c"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0","alts":[{"type":2,"url":"medium://p/43454b21a5d0"},{"type":3,"url":"medium://p/43454b21a5d0"}],"httpStatus":200},{"url":"https://github.com/fastai/fastai/blob/master/docs/style.md","alts":[],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197","alts":[{"type":2,"url":"medium://p/602f73869197"},{"type":3,"url":"medium://p/602f73869197"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493","alts":[{"type":2,"url":"medium://p/5ae195c49493"},{"type":3,"url":"medium://p/5ae195c49493"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","alts":[{"type":2,"url":"medium://p/5f0cf9e4bb5b"},{"type":3,"url":"medium://p/5f0cf9e4bb5b"}],"httpStatus":200},{"url":"https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb","alts":[],"httpStatus":200},{"url":"https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed","alts":[],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94","alts":[{"type":2,"url":"medium://p/215dfbf04a94"},{"type":3,"url":"medium://p/215dfbf04a94"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4","alts":[{"type":2,"url":"medium://p/eeae2edd2be4"},{"type":3,"url":"medium://p/eeae2edd2be4"}],"httpStatus":200},{"url":"https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb","alts":[],"httpStatus":200},{"url":"http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1","alts":[],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=24m34s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=37m20s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=9m34s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h38m47s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=26m12s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=2h2m34s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=57m11s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h23m18s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h9m39s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h18m","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://twitter.com/jeremyphoward","alts":[{"type":2,"url":"twitter://user?screen_name=jeremyphoward"},{"type":3,"url":"twitter://user?screen_name=jeremyphoward"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h53m43s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h2m29s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=59m57s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h9m55s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h50m23s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=54m44s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=38m45s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=36m9s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h44m42s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=40m27s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=38m1s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=55m21s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=33m48s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h49m27s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h4m16s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=20m39s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h10m38s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=9m14s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=59m8s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h56m37s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=15m46s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h52m15s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=41m18s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=18m2s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h16m28s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=25m29s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=31m32s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=13m54s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h31m10s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=51m58s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h29m39s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h46m8s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h41m46s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=2h37s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=19m12s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h8m25s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=29m10s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h15m20s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=4m40s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=48m43s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h11m8s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=22m46s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h13m16s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h12m47s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=53m24s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=27m11s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h5m36s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=35m04s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h51m25s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=6m17s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=7m10s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=2m58s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h44m","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h1m5s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=44m56s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h45m6s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=42m58s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h14m47s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h32m50s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h54m3s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h21m58s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=28m28s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h33m43s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h40m38s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=47m44s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=10m35s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=49m56s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h37m28s","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h42m39s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=1h5m52s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://arxiv.org/abs/1506.01497","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1312.2249","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1708.02002","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1512.02325","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1506.02640","alts":[],"httpStatus":200},{"url":"https://twitter.com/math_rachel","alts":[{"type":2,"url":"twitter://user?screen_name=math_rachel"},{"type":3,"url":"twitter://user?screen_name=math_rachel"}],"httpStatus":200},{"url":"https://youtu.be/0frKXR-2PBY?t=2h1m44s","alts":[{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"},{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=0frKXR-2PBY&feature=applinks"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add","alts":[{"type":2,"url":"medium://p/e0d23c7a0add"},{"type":3,"url":"medium://p/e0d23c7a0add"}],"httpStatus":200},{"url":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34","alts":[{"type":2,"url":"medium://p/61477d24dc34"},{"type":3,"url":"medium://p/61477d24dc34"}],"httpStatus":200},{"url":"https://arxiv.org/pdf/1312.2249.pdf","alts":[],"httpStatus":200},{"url":"https://pjreddie.com/media/files/papers/YOLOv3.pdf","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1533588307581},"isLockedPreviewOnly":false,"takeoverId":"","metaDescription":"","totalClapCount":321,"sectionCount":3,"readingList":1,"topics":[]},"coverless":true,"slug":"deep-learning-2-part-2-lesson-9","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"1*Fj9fK3G6iXBsGI_XJrxXyg.png","originalWidth":456,"originalHeight":456,"isFeatured":true}},{"name":"2be5","type":3,"text":"Deep Learning 2: Part 2 Lesson 9","markups":[],"alignment":1},{"name":"2560","type":1,"text":"My personal notes from fast.ai course. These notes will continue to be updated and improved as I…","markups":[{"type":3,"start":23,"end":37,"href":"http://www.fast.ai/","title":"","rel":"noopener nofollow nofollow noopener noopener noopener nofollow noopener","anchorType":0},{"type":2,"start":0,"end":96}],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to “really”…"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","approvedHomeCollectionId":"","newsletterId":"","webCanonicalUrl":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","mediumUrl":"https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"collectionUserRelations":[],"mode":null,"references":{"User":{"eab3a535185e":{"userId":"eab3a535185e","name":"Hiromi Suenaga","username":"hiromi_suenaga","createdAt":1505308665801,"imageId":"1*OUE6UCNR-GpaCAVQno08Wg.jpeg","backgroundImageId":"","bio":"","twitterScreenName":"hiromi_suenaga","socialStats":{"userId":"eab3a535185e","usersFollowedCount":10,"usersFollowedByCount":1348,"type":"SocialStats"},"social":{"userId":"b43c07dd10f6","targetUserId":"eab3a535185e","type":"Social"},"facebookAccountId":"","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"type":"User"}},"Social":{"eab3a535185e":{"userId":"b43c07dd10f6","targetUserId":"eab3a535185e","type":"Social"}},"SocialStats":{"eab3a535185e":{"userId":"eab3a535185e","usersFollowedCount":10,"usersFollowedByCount":1348,"type":"SocialStats"}}}})
// ]]></script><script id="parsely-cfg" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/p.js"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true}, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/main-common-async.js"></script><script charset="UTF-8" src="Deep%20Learning%202%20%20Part%202%20Lesson%209%20%E2%80%93%20Hiromi%20Suenaga%20%E2%80%93%20Medium_files/main-notes.js"></script></body></html>